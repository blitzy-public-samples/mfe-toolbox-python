{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Real-World Applications of the MFE Toolbox\n",
    "\n",
    "This notebook demonstrates integrated applications of the MFE Toolbox for solving real-world financial econometrics problems. We'll combine multiple techniques into complete analytical workflows for practical financial applications.\n",
    "\n",
    "## Case Studies\n",
    "\n",
    "1. **Portfolio Risk Management**: Combining multivariate volatility modeling with Value-at-Risk estimation\n",
    "2. **Market Regime Analysis**: Using bootstrap methods to identify and analyze different market regimes\n",
    "3. **Volatility Forecasting for Options Trading**: Integrating GARCH models with realized volatility measures\n",
    "4. **Stress Testing and Scenario Analysis**: Simulating extreme market conditions using multivariate models\n",
    "5. **Factor Model Construction**: Building and validating statistical factor models\n",
    "\n",
    "These case studies demonstrate how to combine multiple components of the MFE Toolbox to solve complex financial problems using modern Python-based workflows."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup and Imports\n",
    "\n",
    "First, let's import the necessary modules and set up our environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standard imports\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from typing import Dict, List, Tuple, Optional, Union, Any\n",
    "import asyncio\n",
    "import datetime\n",
    "import warnings\n",
    "from scipy import stats\n",
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "# MFE Toolbox imports\n",
    "import mfe\n",
    "# Univariate volatility models\n",
    "from mfe.models.univariate import GARCH, EGARCH, TARCH, AGARCH, APARCH, FIGARCH, HEAVY, IGARCH\n",
    "# Multivariate volatility models\n",
    "from mfe.models.multivariate import DCC, BEKK, CCC, OGARCH, RARCH, RCC, RiskMetrics\n",
    "# Bootstrap methods\n",
    "from mfe.models.bootstrap import BlockBootstrap, StationaryBootstrap, ModelConfidenceSet, BSDS\n",
    "# Time series models\n",
    "from mfe.models.time_series import ARMA\n",
    "# Realized volatility models\n",
    "from mfe.models.realized import RealizedVariance, BiPowerVariation, RealizedKernel\n",
    "# Statistical distributions\n",
    "from mfe.models.distributions import Normal, StudentT, GED, SkewedT\n",
    "# Utility functions\n",
    "from mfe.utils.data_transformations import returns_from_prices\n",
    "from mfe.utils.matrix_ops import cov2corr\n",
    "\n",
    "# Configure plotting\n",
    "plt.style.use('seaborn-v0_8-whitegrid')\n",
    "sns.set_context(\"notebook\", font_scale=1.2)\n",
    "plt.rcParams['figure.figsize'] = (12, 6)\n",
    "plt.rcParams['figure.dpi'] = 100\n",
    "\n",
    "# Suppress warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Display version information\n",
    "print(f\"MFE Toolbox version: {mfe.__version__}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Loading and Preparation\n",
    "\n",
    "We'll create a utility function to load financial market data from various sources. This function will handle both external data sources and synthetic data generation for demonstration purposes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_market_data(tickers: List[str], \n",
    "                     start_date: str = \"2015-01-01\", \n",
    "                     end_date: Optional[str] = None,\n",
    "                     data_source: str = \"yahoo\",\n",
    "                     use_synthetic: bool = False) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Load market data for multiple tickers from various sources.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    tickers : List[str]\n",
    "        List of ticker symbols to load\n",
    "    start_date : str\n",
    "        Start date in YYYY-MM-DD format\n",
    "    end_date : str, optional\n",
    "        End date in YYYY-MM-DD format, defaults to current date\n",
    "    data_source : str\n",
    "        Source of data: 'yahoo', 'fred', 'csv', etc.\n",
    "    use_synthetic : bool\n",
    "        If True, generate synthetic data instead of loading from source\n",
    "        \n",
    "    Returns\n",
    "    -------\n",
    "    pd.DataFrame\n",
    "        DataFrame containing adjusted close prices for all tickers\n",
    "    \"\"\"\n",
    "    # If synthetic data is requested, generate it\n",
    "    if use_synthetic:\n",
    "        return create_synthetic_market_data(tickers, start_date, end_date)\n",
    "    \n",
    "    try:\n",
    "        import pandas_datareader as pdr\n",
    "        \n",
    "        # Set end date to today if not provided\n",
    "        if end_date is None:\n",
    "            end_date = datetime.datetime.now().strftime(\"%Y-%m-%d\")\n",
    "            \n",
    "        # Initialize DataFrame to store results\n",
    "        all_data = pd.DataFrame()\n",
    "        \n",
    "        # Download data for each ticker\n",
    "        for ticker in tickers:\n",
    "            try:\n",
    "                if data_source.lower() == 'yahoo':\n",
    "                    data = pdr.get_data_yahoo(ticker, start=start_date, end=end_date)\n",
    "                    all_data[ticker] = data['Adj Close']\n",
    "                elif data_source.lower() == 'fred':\n",
    "                    data = pdr.get_data_fred(ticker, start=start_date, end=end_date)\n",
    "                    all_data[ticker] = data\n",
    "                else:\n",
    "                    raise ValueError(f\"Unsupported data source: {data_source}\")\n",
    "                    \n",
    "                print(f\"Downloaded data for {ticker}: {len(data)} days\")\n",
    "            except Exception as e:\n",
    "                print(f\"Error downloading data for {ticker}: {e}\")\n",
    "        \n",
    "        # Check if we got any data\n",
    "        if all_data.empty:\n",
    "            print(\"No data downloaded. Using synthetic data instead.\")\n",
    "            return create_synthetic_market_data(tickers, start_date, end_date)\n",
    "        \n",
    "        return all_data\n",
    "    except ImportError:\n",
    "        print(\"pandas-datareader not installed. Please install with: pip install pandas-datareader\")\n",
    "        print(\"Using synthetic data instead.\")\n",
    "        return create_synthetic_market_data(tickers, start_date, end_date)\n",
    "    except Exception as e:\n",
    "        print(f\"Error loading data: {e}\")\n",
    "        print(\"Using synthetic data instead.\")\n",
    "        return create_synthetic_market_data(tickers, start_date, end_date)\n",
    "\n",
    "def create_synthetic_market_data(tickers: List[str], \n",
    "                                start_date: str = \"2015-01-01\", \n",
    "                                end_date: Optional[str] = None,\n",
    "                                include_regimes: bool = True) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Create synthetic market data for multiple assets with realistic properties.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    tickers : List[str]\n",
    "        List of ticker symbols to simulate\n",
    "    start_date : str\n",
    "        Start date in YYYY-MM-DD format\n",
    "    end_date : str, optional\n",
    "        End date in YYYY-MM-DD format, defaults to current date\n",
    "    include_regimes : bool\n",
    "        If True, include distinct market regimes in the data\n",
    "        \n",
    "    Returns\n",
    "    -------\n",
    "    pd.DataFrame\n",
    "        DataFrame containing synthetic price data for all tickers\n",
    "    \"\"\"\n",
    "    # Parse dates\n",
    "    start = pd.to_datetime(start_date)\n",
    "    if end_date is None:\n",
    "        end = pd.to_datetime(datetime.datetime.now().strftime(\"%Y-%m-%d\"))\n",
    "    else:\n",
    "        end = pd.to_datetime(end_date)\n",
    "    \n",
    "    # Create date range (business days only)\n",
    "    dates = pd.date_range(start=start, end=end, freq='B')\n",
    "    \n",
    "    # Set random seed for reproducibility\n",
    "    np.random.seed(42)\n",
    "    \n",
    "    # Number of assets and days\n",
    "    n_assets = len(tickers)\n",
    "    n_days = len(dates)\n",
    "    \n",
    "    # Create correlation matrix with realistic correlations\n",
    "    # We'll use a factor model approach to ensure positive definiteness\n",
    "    n_factors = 3  # Market, size, value factors for example\n",
    "    factor_loadings = np.random.uniform(0.3, 0.9, size=(n_assets, n_factors))\n",
    "    \n",
    "    # Create correlation matrix from factor loadings\n",
    "    corr_matrix = factor_loadings @ factor_loadings.T\n",
    "    # Add idiosyncratic variance to ensure diagonal of 1s\n",
    "    for i in range(n_assets):\n",
    "        corr_matrix[i, i] = 1.0\n",
    "    \n",
    "    # Ensure it's a valid correlation matrix\n",
    "    # Make it symmetric\n",
    "    corr_matrix = (corr_matrix + corr_matrix.T) / 2\n",
    "    # Normalize to ensure diagonal is 1\n",
    "    d = np.sqrt(np.diag(corr_matrix))\n",
    "    corr_matrix = corr_matrix / np.outer(d, d)\n",
    "    \n",
    "    # Create volatilities for each asset (annualized)\n",
    "    asset_vols = np.random.uniform(0.15, 0.35, size=n_assets)  # 15% to 35% annual vol\n",
    "    \n",
    "    # Convert to daily volatility\n",
    "    daily_vols = asset_vols / np.sqrt(252)\n",
    "    \n",
    "    # Create covariance matrix\n",
    "    cov_matrix = np.diag(daily_vols) @ corr_matrix @ np.diag(daily_vols)\n",
    "    \n",
    "    # Generate correlated returns\n",
    "    # We'll use Cholesky decomposition\n",
    "    chol = np.linalg.cholesky(cov_matrix)\n",
    "    \n",
    "    # Generate random normal returns\n",
    "    random_returns = np.random.normal(0, 1, size=(n_days, n_assets))\n",
    "    \n",
    "    # Transform to correlated returns\n",
    "    correlated_returns = random_returns @ chol.T\n",
    "    \n",
    "    # Add drift (expected return)\n",
    "    expected_returns = np.random.uniform(0.05, 0.15, size=n_assets) / 252  # 5% to 15% annual return\n",
    "    correlated_returns += expected_returns\n",
    "    \n",
    "    # If including regimes, modify the returns to create distinct market regimes\n",
    "    if include_regimes:\n",
    "        # Define regimes\n",
    "        # 1. Normal regime (already generated)\n",
    "        # 2. High volatility regime (e.g., crisis)\n",
    "        # 3. Low correlation regime (e.g., sector rotation)\n",
    "        # 4. Bull market regime (high returns, low vol)\n",
    "        \n",
    "        # Divide the time period into segments\n",
    "        segment_size = n_days // 4\n",
    "        \n",
    "        # High volatility regime (2x volatility, negative drift)\n",
    "        high_vol_start = segment_size\n",
    "        high_vol_end = segment_size * 2\n",
    "        vol_multiplier = 2.5\n",
    "        correlated_returns[high_vol_start:high_vol_end] *= vol_multiplier\n",
    "        correlated_returns[high_vol_start:high_vol_end] -= 0.001  # Negative drift\n",
    "        \n",
    "        # Low correlation regime (reduce correlations)\n",
    "        low_corr_start = segment_size * 2\n",
    "        low_corr_end = segment_size * 3\n",
    "        # Add more idiosyncratic movement\n",
    "        idiosyncratic = np.random.normal(0, 0.01, size=(low_corr_end - low_corr_start, n_assets))\n",
    "        correlated_returns[low_corr_start:low_corr_end] += idiosyncratic\n",
    "        \n",
    "        # Bull market regime (positive drift, lower vol)\n",
    "        bull_start = segment_size * 3\n",
    "        bull_end = n_days\n",
    "        correlated_returns[bull_start:bull_end] *= 0.7  # Lower vol\n",
    "        correlated_returns[bull_start:bull_end] += 0.001  # Positive drift\n",
    "    \n",
    "    # Add volatility clustering\n",
    "    # We'll use a simple AR(1) process for volatility\n",
    "    vol_persistence = 0.95\n",
    "    vol_scale = np.ones((n_days, n_assets))\n",
    "    vol_innovation = np.random.normal(0, 0.1, size=(n_days, n_assets))\n",
    "    \n",
    "    for t in range(1, n_days):\n",
    "        vol_scale[t] = np.sqrt(0.05 + vol_persistence * vol_scale[t-1]**2 + 0.05 * vol_innovation[t]**2)\n",
    "    \n",
    "    # Apply time-varying volatility\n",
    "    correlated_returns = correlated_returns * vol_scale\n",
    "    \n",
    "    # Add occasional jumps\n",
    "    n_jumps = int(n_days * 0.01)  # 1% of days have jumps\n",
    "    jump_days = np.random.choice(range(n_days), size=n_jumps, replace=False)\n",
    "    jump_assets = np.random.choice(range(n_assets), size=n_jumps)\n",
    "    jump_signs = np.random.choice([-1, 1], size=n_jumps)\n",
    "    jump_sizes = np.random.uniform(0.02, 0.05, size=n_jumps)\n",
    "    \n",
    "    for day, asset, sign, size in zip(jump_days, jump_assets, jump_signs, jump_sizes):\n",
    "        correlated_returns[day, asset] += sign * size\n",
    "    \n",
    "    # Convert returns to prices\n",
    "    # Start with price of 100 for each asset\n",
    "    prices = np.zeros((n_days, n_assets))\n",
    "    prices[0] = 100.0\n",
    "    \n",
    "    for t in range(1, n_days):\n",
    "        prices[t] = prices[t-1] * (1 + correlated_returns[t])\n",
    "    \n",
    "    # Create DataFrame\n",
    "    price_df = pd.DataFrame(prices, index=dates, columns=tickers)\n",
    "    \n",
    "    print(f\"Created synthetic data for {n_assets} assets over {n_days} days\")\n",
    "    return price_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Case Study 1: Portfolio Risk Management\n",
    "\n",
    "In this case study, we'll demonstrate a comprehensive portfolio risk management workflow that combines:\n",
    "\n",
    "1. Multivariate volatility modeling with DCC-GARCH\n",
    "2. Value-at-Risk (VaR) and Expected Shortfall (ES) estimation\n",
    "3. Stress testing and scenario analysis\n",
    "4. Risk decomposition and attribution\n",
    "\n",
    "This workflow is typical of what might be used in a risk management department of an investment firm."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define portfolio assets\n",
    "# We'll use major US indices and sector ETFs\n",
    "portfolio_assets = ['SPY', 'QQQ', 'IWM', 'XLF', 'XLE', 'XLK', 'XLV', 'XLI']\n",
    "\n",
    "# Load price data\n",
    "try:\n",
    "    price_data = load_market_data(portfolio_assets, \"2018-01-01\")\n",
    "except Exception as e:\n",
    "    print(f\"Error loading data: {e}\")\n",
    "    print(\"Using synthetic data instead\")\n",
    "    price_data = create_synthetic_market_data(portfolio_assets, \"2018-01-01\")\n",
    "\n",
    "# Calculate returns\n",
    "returns_df = pd.DataFrame()\n",
    "for ticker in price_data.columns:\n",
    "    returns_df[ticker] = returns_from_prices(price_data[ticker], log=True) * 100  # Convert to percentage\n",
    "\n",
    "# Handle missing values if any\n",
    "if returns_df.isna().any().any():\n",
    "    returns_df = returns_df.fillna(method='ffill').fillna(method='bfill')\n",
    "\n",
    "# Display summary statistics\n",
    "returns_summary = returns_df.describe()\n",
    "print(\"Returns Summary Statistics:\")\n",
    "returns_summary\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize the data\n",
    "# Plot normalized prices\n",
    "plt.figure(figsize=(14, 8))\n",
    "normalized_prices = price_data.div(price_data.iloc[0]) * 100\n",
    "for ticker in normalized_prices.columns:\n",
    "    plt.plot(normalized_prices.index, normalized_prices[ticker], label=ticker)\n",
    "plt.title('Normalized Price Series (Base = 100)')\n",
    "plt.ylabel('Normalized Price')\n",
    "plt.xlabel('Date')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()\n",
    "\n",
    "# Plot correlation matrix\n",
    "plt.figure(figsize=(12, 10))\n",
    "corr_matrix = returns_df.corr()\n",
    "sns.heatmap(corr_matrix, annot=True, cmap='coolwarm', vmin=-1, vmax=1, center=0,\n",
    "            square=True, linewidths=0.5, cbar_kws={\"shrink\": 0.8})\n",
    "plt.title('Asset Correlation Matrix')\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define portfolio weights\n",
    "# For this example, we'll create a portfolio with the following allocation:\n",
    "# - 40% in broad market ETFs (SPY, QQQ, IWM)\n",
    "# - 60% in sector ETFs (XLF, XLE, XLK, XLV, XLI)\n",
    "\n",
    "portfolio_weights = {\n",
    "    'SPY': 0.20,  # S&P 500\n",
    "    'QQQ': 0.15,  # NASDAQ 100\n",
    "    'IWM': 0.05,  # Russell 2000\n",
    "    'XLF': 0.10,  # Financials\n",
    "    'XLE': 0.10,  # Energy\n",
    "    'XLK': 0.20,  # Technology\n",
    "    'XLV': 0.10,  # Healthcare\n",
    "    'XLI': 0.10   # Industrials\n",
    "}\n",
    "\n",
    "# Create a DataFrame with weights\n",
    "weights_df = pd.DataFrame(list(portfolio_weights.items()), columns=['Asset', 'Weight'])\n",
    "print(\"Portfolio Weights:\")\n",
    "weights_df\n",
    "\n",
    "# Create weight vector in the same order as returns_df\n",
    "weight_vector = np.array([portfolio_weights[ticker] for ticker in returns_df.columns])\n",
    "\n",
    "# Calculate portfolio returns\n",
    "portfolio_returns = returns_df.dot(weight_vector)\n",
    "\n",
    "# Plot portfolio returns\n",
    "plt.figure(figsize=(14, 6))\n",
    "plt.plot(portfolio_returns.index, portfolio_returns, color='blue')\n",
    "plt.title('Portfolio Daily Returns')\n",
    "plt.ylabel('Returns (%)')\n",
    "plt.xlabel('Date')\n",
    "plt.grid(True)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: Estimate a DCC-GARCH model for the portfolio assets\n",
    "# Convert returns DataFrame to NumPy array\n",
    "returns_array = returns_df.values\n",
    "\n",
    "# Create and estimate a DCC-GARCH model with Student's t distribution\n",
    "# Student's t is often more appropriate for financial returns due to fat tails\n",
    "print(\"Estimating DCC-GARCH model with Student's t distribution...\")\n",
    "dcc_model = DCC(returns_df.shape[1],  # Number of assets\n",
    "                p=1, q=1,             # DCC orders\n",
    "                univariate_model=GARCH,  # Univariate model for each series\n",
    "                univariate_params={'p': 1, 'q': 1},  # GARCH(1,1) for each series\n",
    "                distribution=StudentT())  # Student's t distribution\n",
    "\n",
    "# Fit the model\n",
    "dcc_results = dcc_model.fit(returns_array)\n",
    "print(\"DCC-GARCH estimation complete.\")\n",
    "\n",
    "# Display DCC parameters\n",
    "print(\"\nDCC Parameters:\")\n",
    "dcc_params = pd.DataFrame({\n",
    "    'Parameter': dcc_results.parameter_names[-2:],  # Last two parameters are DCC parameters\n",
    "    'Estimate': dcc_results.parameters[-2:],\n",
    "    'Std. Error': dcc_results.std_errors[-2:],\n",
    "    't-statistic': dcc_results.t_stats[-2:],\n",
    "    'p-value': dcc_results.p_values[-2:]\n",
    "})\n",
    "print(dcc_params)\n",
    "\n",
    "# Calculate persistence\n",
    "alpha = dcc_results.parameters[-2]  # DCC alpha parameter\n",
    "beta = dcc_results.parameters[-1]   # DCC beta parameter\n",
    "persistence = alpha + beta\n",
    "print(f\"\nDCC Persistence (α + β): {persistence:.4f}\")\n",
    "print(f\"Half-life: {np.log(0.5) / np.log(persistence):.2f} days\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 2: Calculate portfolio volatility using the DCC-GARCH model\n",
    "# Function to calculate portfolio variance from covariance matrix\n",
    "def portfolio_variance(cov_matrix, weights):\n",
    "    return weights.T @ cov_matrix @ weights\n",
    "\n",
    "# Calculate portfolio volatility\n",
    "dcc_portfolio_var = np.array([portfolio_variance(cov_matrix, weight_vector) \n",
    "                             for cov_matrix in dcc_results.conditional_covariance])\n",
    "dcc_portfolio_vol = np.sqrt(dcc_portfolio_var)  # Daily volatility\n",
    "dcc_portfolio_vol_annual = dcc_portfolio_vol * np.sqrt(252)  # Annualized\n",
    "\n",
    "# Calculate rolling portfolio volatility for comparison\n",
    "window_size = 60  # 60-day rolling window (approximately 3 months)\n",
    "rolling_portfolio_vol = portfolio_returns.rolling(window=window_size).std() * np.sqrt(252)  # Annualized\n",
    "\n",
    "# Plot portfolio volatility\n",
    "plt.figure(figsize=(14, 6))\n",
    "plt.plot(returns_df.index, dcc_portfolio_vol_annual, label='DCC-GARCH', color='blue')\n",
    "plt.plot(rolling_portfolio_vol.index, rolling_portfolio_vol, label=f'{window_size}-Day Rolling', \n",
    "         color='red', alpha=0.7)\n",
    "plt.title('Portfolio Volatility (Annualized)')\n",
    "plt.ylabel('Volatility (%)')\n",
    "plt.xlabel('Date')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 3: Calculate Value-at-Risk (VaR) and Expected Shortfall (ES)\n",
    "# We'll calculate these risk measures using the DCC-GARCH model\n",
    "\n",
    "# Define confidence levels\n",
    "confidence_levels = [0.95, 0.99]\n",
    "\n",
    "# Function to calculate VaR and ES\n",
    "def calculate_var_es(returns, volatility, confidence_level, distribution='normal', df=None):\n",
    "    \"\"\"\n",
    "    Calculate Value-at-Risk and Expected Shortfall.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    returns : np.ndarray or pd.Series\n",
    "        Historical returns\n",
    "    volatility : np.ndarray or pd.Series\n",
    "        Volatility estimates\n",
    "    confidence_level : float\n",
    "        Confidence level (e.g., 0.95 for 95% VaR)\n",
    "    distribution : str\n",
    "        Distribution assumption: 'normal', 'empirical', or 't'\n",
    "    df : float, optional\n",
    "        Degrees of freedom for t-distribution\n",
    "        \n",
    "    Returns\n",
    "    -------\n",
    "    tuple\n",
    "        (VaR, ES) values\n",
    "    \"\"\"\n",
    "    if distribution == 'normal':\n",
    "        # Calculate VaR assuming normal distribution\n",
    "        z_score = stats.norm.ppf(1 - confidence_level)\n",
    "        var = -volatility * z_score\n",
    "        \n",
    "        # Calculate ES for normal distribution\n",
    "        # ES = -μ - σ * φ(z_α) / (1 - α)\n",
    "        # where φ is the standard normal PDF\n",
    "        pdf_value = stats.norm.pdf(z_score)\n",
    "        es = -volatility * pdf_value / (1 - confidence_level)\n",
    "        \n",
    "    elif distribution == 't' and df is not None:\n",
    "        # Calculate VaR assuming t-distribution\n",
    "        t_score = stats.t.ppf(1 - confidence_level, df)\n",
    "        var = -volatility * t_score * np.sqrt((df - 2) / df)\n",
    "        \n",
    "        # Calculate ES for t-distribution\n",
    "        # ES = -μ - σ * (df / (df - 1)) * (ft(t_α) / (1 - α)) * ((df + t_α^2) / (df - 1))\n",
    "        # where ft is the t-distribution PDF\n",
    "        pdf_value = stats.t.pdf(t_score, df)\n",
    "        es = -volatility * (df / (df - 1)) * (pdf_value / (1 - confidence_level)) * ((df + t_score**2) / (df - 1))\n",
    "        \n",
    "    elif distribution == 'empirical':\n",
    "        # Calculate standardized residuals\n",
    "        std_residuals = returns / volatility\n",
    "        \n",
    "        # Calculate VaR using empirical distribution\n",
    "        var_quantile = np.percentile(std_residuals, 100 * (1 - confidence_level))\n",
    "        var = volatility * var_quantile\n",
    "        \n",
    "        # Calculate ES using empirical distribution\n",
    "        es_threshold = var_quantile\n",
    "        beyond_var = std_residuals[std_residuals <= es_threshold]\n",
    "        es = volatility * np.mean(beyond_var) if len(beyond_var) > 0 else var\n",
    "    else:\n",
    "        raise ValueError(\"Invalid distribution or missing degrees of freedom\")\n",
    "    \n",
    "    return var, es\n",
    "\n",
    "# Calculate VaR and ES for each day using DCC-GARCH volatility\n",
    "var_es_results = {}\n",
    "\n",
    "# Get degrees of freedom from Student's t distribution\n",
    "# It's typically the last univariate parameter for each asset\n",
    "# We'll use the average across assets for simplicity\n",
    "df_params = []\n",
    "for i in range(returns_df.shape[1]):\n",
    "    # Find the degrees of freedom parameter for this asset\n",
    "    # It's typically the last parameter for each univariate model\n",
    "    univariate_params_count = 3  # omega, alpha, beta for GARCH(1,1)\n",
    "    df_param_idx = i * (univariate_params_count + 1) + univariate_params_count\n",
    "    if df_param_idx < len(dcc_results.parameters) - 2:  # -2 for DCC parameters\n",
    "        df_params.append(dcc_results.parameters[df_param_idx])\n",
    "\n",
    "# Use average degrees of freedom if available, otherwise default to 5\n",
    "avg_df = np.mean(df_params) if df_params else 5.0\n",
    "print(f\"Average degrees of freedom for Student's t distribution: {avg_df:.2f}\")\n",
    "\n",
    "# Calculate VaR and ES for each confidence level\n",
    "for confidence in confidence_levels:\n",
    "    # Normal distribution\n",
    "    var_normal, es_normal = calculate_var_es(\n",
    "        portfolio_returns.values, dcc_portfolio_vol, confidence, 'normal')\n",
    "    \n",
    "    # t-distribution\n",
    "    var_t, es_t = calculate_var_es(\n",
    "        portfolio_returns.values, dcc_portfolio_vol, confidence, 't', avg_df)\n",
    "    \n",
    "    # Store results\n",
    "    var_es_results[f'VaR {confidence*100:.0f}% Normal'] = var_normal\n",
    "    var_es_results[f'ES {confidence*100:.0f}% Normal'] = es_normal\n",
    "    var_es_results[f'VaR {confidence*100:.0f}% t'] = var_t\n",
    "    var_es_results[f'ES {confidence*100:.0f}% t'] = es_t\n",
    "\n",
    "# Convert to DataFrame\n",
    "var_es_df = pd.DataFrame(var_es_results, index=returns_df.index)\n",
    "\n",
    "# Plot VaR and ES\n",
    "plt.figure(figsize=(14, 8))\n",
    "\n",
    "# Plot portfolio returns\n",
    "plt.plot(portfolio_returns.index, portfolio_returns, color='blue', alpha=0.5, label='Portfolio Returns')\n",
    "\n",
    "# Plot VaR and ES for 99% confidence level\n",
    "plt.plot(var_es_df.index, var_es_df['VaR 99% t'], color='red', label='99% VaR (t-dist)')\n",
    "plt.plot(var_es_df.index, var_es_df['ES 99% t'], color='darkred', linestyle='--', label='99% ES (t-dist)')\n",
    "\n",
    "# Plot VaR and ES for 95% confidence level\n",
    "plt.plot(var_es_df.index, var_es_df['VaR 95% t'], color='orange', label='95% VaR (t-dist)')\n",
    "plt.plot(var_es_df.index, var_es_df['ES 95% t'], color='darkorange', linestyle='--', label='95% ES (t-dist)')\n",
    "\n",
    "plt.title('Portfolio Returns with Value-at-Risk and Expected Shortfall')\n",
    "plt.ylabel('Returns (%)')\n",
    "plt.xlabel('Date')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 4: Backtesting VaR\n",
    "# Count VaR violations (when actual returns are worse than VaR)\n",
    "violations = {}\n",
    "for confidence in confidence_levels:\n",
    "    for dist in ['Normal', 't']:\n",
    "        var_col = f'VaR {confidence*100:.0f}% {dist.lower()}'\n",
    "        violations[var_col] = (portfolio_returns < var_es_df[var_col]).sum()\n",
    "\n",
    "# Calculate expected number of violations\n",
    "n_days = len(portfolio_returns)\n",
    "expected_violations = {}\n",
    "for confidence in confidence_levels:\n",
    "    expected_violations[f'{confidence*100:.0f}%'] = n_days * (1 - confidence)\n",
    "\n",
    "# Create violation summary\n",
    "violation_summary = pd.DataFrame({\n",
    "    'VaR Level': list(violations.keys()),\n",
    "    'Violations': list(violations.values()),\n",
    "    'Expected': [expected_violations[level.split('%')[0] + '%'] for level in violations.keys()],\n",
    "    'Ratio': [violations[level] / expected_violations[level.split('%')[0] + '%'] for level in violations.keys()]\n",
    "})\n",
    "\n",
    "print(\"VaR Backtest Results:\")\n",
    "violation_summary\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 5: Risk Decomposition and Attribution\n",
    "# Calculate marginal contribution to risk (MCR) and percentage contribution to risk (PCR)\n",
    "\n",
    "# Function to calculate risk contributions\n",
    "def calculate_risk_contributions(cov_matrix, weights):\n",
    "    \"\"\"\n",
    "    Calculate marginal and percentage contributions to portfolio risk.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    cov_matrix : np.ndarray\n",
    "        Covariance matrix\n",
    "    weights : np.ndarray\n",
    "        Portfolio weights\n",
    "        \n",
    "    Returns\n",
    "    -------\n",
    "    tuple\n",
    "        (marginal_contributions, percentage_contributions)\n",
    "    \"\"\"\n",
    "    # Calculate portfolio variance and volatility\n",
    "    port_var = portfolio_variance(cov_matrix, weights)\n",
    "    port_vol = np.sqrt(port_var)\n",
    "    \n",
    "    # Calculate marginal contribution to risk (MCR)\n",
    "    # MCR_i = w_i * (Σw)_i / σ_p\n",
    "    # where (Σw)_i is the i-th element of the vector Σw\n",
    "    mcr = weights * (cov_matrix @ weights) / port_vol\n",
    "    \n",
    "    # Calculate percentage contribution to risk (PCR)\n",
    "    # PCR_i = MCR_i / σ_p\n",
    "    pcr = mcr / port_vol\n",
    "    \n",
    "    return mcr, pcr\n",
    "\n",
    "# Calculate risk contributions using the latest covariance matrix\n",
    "latest_cov = dcc_results.conditional_covariance[-1]\n",
    "mcr, pcr = calculate_risk_contributions(latest_cov, weight_vector)\n",
    "\n",
    "# Create risk contribution summary\n",
    "risk_contrib = pd.DataFrame({\n",
    "    'Asset': returns_df.columns,\n",
    "    'Weight': weight_vector,\n",
    "    'Marginal Contribution': mcr,\n",
    "    'Percentage Contribution': pcr * 100  # Convert to percentage\n",
    "})\n",
    "\n",
    "print(\"Risk Contribution Analysis:\")\n",
    "risk_contrib\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize risk contributions\n",
    "plt.figure(figsize=(14, 7))\n",
    "\n",
    "# Sort by percentage contribution\n",
    "risk_contrib_sorted = risk_contrib.sort_values('Percentage Contribution', ascending=False)\n",
    "\n",
    "# Create bar chart\n",
    "bars = plt.bar(risk_contrib_sorted['Asset'], risk_contrib_sorted['Percentage Contribution'])\n",
    "\n",
    "# Add weight information\n",
    "for i, bar in enumerate(bars):\n",
    "    weight = risk_contrib_sorted.iloc[i]['Weight'] * 100\n",
    "    plt.text(bar.get_x() + bar.get_width()/2, 0.5, f'Weight: {weight:.1f}%', \n",
    "             ha='center', va='bottom', rotation=90, color='white', fontweight='bold')\n",
    "\n",
    "plt.title('Percentage Contribution to Portfolio Risk')\n",
    "plt.ylabel('Contribution (%)')\n",
    "plt.grid(axis='y')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Compare weight allocation vs risk contribution\n",
    "plt.figure(figsize=(14, 7))\n",
    "\n",
    "# Create a DataFrame for comparison\n",
    "comparison = pd.DataFrame({\n",
    "    'Asset': risk_contrib['Asset'],\n",
    "    'Weight Allocation (%)': risk_contrib['Weight'] * 100,\n",
    "    'Risk Contribution (%)': risk_contrib['Percentage Contribution']\n",
    "})\n",
    "\n",
    "# Sort by risk contribution\n",
    "comparison = comparison.sort_values('Risk Contribution (%)', ascending=False)\n",
    "\n",
    "# Plot\n",
    "comparison.set_index('Asset')[['Weight Allocation (%)', 'Risk Contribution (%)']].plot(kind='bar', figsize=(14, 7))\n",
    "plt.title('Weight Allocation vs Risk Contribution')\n",
    "plt.ylabel('Percentage (%)')\n",
    "plt.grid(axis='y')\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 6: Stress Testing and Scenario Analysis\n",
    "# We'll simulate different market scenarios and analyze their impact on the portfolio\n",
    "\n",
    "# Define scenarios\n",
    "scenarios = {\n",
    "    'Base Case': {\n",
    "        'description': 'Current market conditions',\n",
    "        'volatility_multiplier': 1.0,\n",
    "        'correlation_adjustment': 0.0,\n",
    "        'return_shift': 0.0\n",
    "    },\n",
    "    'Market Crash': {\n",
    "        'description': 'Severe market downturn with high volatility and correlations',\n",
    "        'volatility_multiplier': 2.5,\n",
    "        'correlation_adjustment': 0.2,  # Increase correlations\n",
    "        'return_shift': -2.0  # Negative daily returns\n",
    "    },\n",
    "    'Sector Rotation': {\n",
    "        'description': 'Decreased correlations with moderate volatility',\n",
    "        'volatility_multiplier': 1.3,\n",
    "        'correlation_adjustment': -0.3,  # Decrease correlations\n",
    "        'return_shift': 0.0\n",
    "    },\n",
    "    'Tech Crash': {\n",
    "        'description': 'Technology sector crash with spillover effects',\n",
    "        'volatility_multiplier': 1.8,\n",
    "        'correlation_adjustment': 0.1,\n",
    "        'return_shift': -1.5,\n",
    "        'sector_specific': {'XLK': -3.0}  # Additional shock to tech sector\n",
    "    },\n",
    "    'Energy Crisis': {\n",
    "        'description': 'Energy sector crisis with market-wide impact',\n",
    "        'volatility_multiplier': 1.5,\n",
    "        'correlation_adjustment': 0.1,\n",
    "        'return_shift': -1.0,\n",
    "        'sector_specific': {'XLE': -4.0}  # Additional shock to energy sector\n",
    "    }\n",
    "}\n",
    "\n",
    "# Function to adjust correlation matrix\n",
    "def adjust_correlation_matrix(corr_matrix, adjustment):\n",
    "    \"\"\"\n",
    "    Adjust correlation matrix by adding a constant to off-diagonal elements,\n",
    "    then rescaling to ensure it remains a valid correlation matrix.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    corr_matrix : np.ndarray\n",
    "        Original correlation matrix\n",
    "    adjustment : float\n",
    "        Adjustment to apply to off-diagonal elements\n",
    "        \n",
    "    Returns\n",
    "    -------\n",
    "    np.ndarray\n",
    "        Adjusted correlation matrix\n",
    "    \"\"\"\n",
    "    # Create a copy of the correlation matrix\n",
    "    adjusted_corr = corr_matrix.copy()\n",
    "    \n",
    "    # Apply adjustment to off-diagonal elements\n",
    "    n = adjusted_corr.shape[0]\n",
    "    for i in range(n):\n",
    "        for j in range(n):\n",
    "            if i != j:\n",
    "                adjusted_corr[i, j] += adjustment\n",
    "                # Ensure correlations are between -1 and 1\n",
    "                adjusted_corr[i, j] = max(-0.99, min(0.99, adjusted_corr[i, j]))\n",
    "    \n",
    "    # Ensure the matrix is positive definite\n",
    "    # We'll use the nearest positive definite matrix if needed\n",
    "    try:\n",
    "        # Check if the matrix is positive definite\n",
    "        np.linalg.cholesky(adjusted_corr)\n",
    "    except np.linalg.LinAlgError:\n",
    "        # If not, find the nearest positive definite matrix\n",
    "        # This is a simplified approach - in practice, you might use more sophisticated methods\n",
    "        eigvals, eigvecs = np.linalg.eigh(adjusted_corr)\n",
    "        eigvals = np.maximum(eigvals, 1e-6)  # Ensure positive eigenvalues\n",
    "        adjusted_corr = eigvecs @ np.diag(eigvals) @ eigvecs.T\n",
    "        \n",
    "        # Rescale to ensure diagonal elements are 1\n",
    "        d = np.sqrt(np.diag(adjusted_corr))\n",
    "        adjusted_corr = adjusted_corr / np.outer(d, d)\n",
    "    \n",
    "    return adjusted_corr\n",
    "\n",
    "# Function to run scenario analysis\n",
    "def run_scenario_analysis(scenarios, latest_cov, weight_vector, asset_names, confidence_level=0.99):\n",
    "    \"\"\"\n",
    "    Run scenario analysis for different market conditions.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    scenarios : dict\n",
    "        Dictionary of scenarios\n",
    "    latest_cov : np.ndarray\n",
    "        Latest covariance matrix\n",
    "    weight_vector : np.ndarray\n",
    "        Portfolio weights\n",
    "    asset_names : list\n",
    "        List of asset names\n",
    "    confidence_level : float\n",
    "        Confidence level for VaR and ES calculations\n",
    "        \n",
    "    Returns\n",
    "    -------\n",
    "    pd.DataFrame\n",
    "        Scenario analysis results\n",
    "    \"\"\"\n",
    "    results = []\n",
    "    \n",
    "    # Get latest volatilities (diagonal of covariance matrix)\n",
    "    latest_vols = np.sqrt(np.diag(latest_cov))\n",
    "    \n",
    "    # Get latest correlation matrix\n",
    "    latest_corr = cov2corr(latest_cov)\n",
    "    \n",
    "    for scenario_name, scenario in scenarios.items():\n",
    "        # Apply volatility multiplier\n",
    "        adjusted_vols = latest_vols * scenario['volatility_multiplier']\n",
    "        \n",
    "        # Apply correlation adjustment\n",
    "        adjusted_corr = adjust_correlation_matrix(latest_corr, scenario['correlation_adjustment'])\n",
    "        \n",
    "        # Reconstruct covariance matrix\n",
    "        adjusted_cov = np.diag(adjusted_vols) @ adjusted_corr @ np.diag(adjusted_vols)\n",
    "        \n",
    "        # Calculate portfolio volatility\n",
    "        port_var = portfolio_variance(adjusted_cov, weight_vector)\n",
    "        port_vol = np.sqrt(port_var)\n",
    "        \n",
    "        # Calculate VaR and ES assuming t-distribution\n",
    "        var, es = calculate_var_es(None, port_vol, confidence_level, 't', avg_df)\n",
    "        \n",
    "        # Calculate expected return under this scenario\n",
    "        base_return = scenario['return_shift']\n",
    "        \n",
    "        # Apply sector-specific shocks if any\n",
    "        if 'sector_specific' in scenario:\n",
    "            for asset, shock in scenario['sector_specific'].items():\n",
    "                if asset in asset_names:\n",
    "                    asset_idx = asset_names.index(asset)\n",
    "                    base_return += weight_vector[asset_idx] * shock\n",
    "        \n",
    "        # Store results\n",
    "        results.append({\n",
    "            'Scenario': scenario_name,\n",
    "            'Description': scenario['description'],\n",
    "            'Expected Return (%)': base_return,\n",
    "            'Volatility (%)': port_vol * 100,  # Convert to percentage\n",
    "            f'VaR {confidence_level*100:.0f}% (%)': -var * 100,  # Convert to positive percentage\n",
    "            f'ES {confidence_level*100:.0f}% (%)': -es * 100,  # Convert to positive percentage\n",
    "        })\n",
    "    \n",
    "    return pd.DataFrame(results)\n",
    "\n",
    "# Run scenario analysis\n",
    "scenario_results = run_scenario_analysis(\n",
    "    scenarios, latest_cov, weight_vector, list(returns_df.columns), 0.99)\n",
    "\n",
    "print(\"Scenario Analysis Results:\")\n",
    "scenario_results\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize scenario analysis results\n",
    "plt.figure(figsize=(14, 8))\n",
    "\n",
    "# Create bar chart for VaR and ES\n",
    "x = np.arange(len(scenario_results))\n",
    "width = 0.35\n",
    "\n",
    "plt.bar(x - width/2, scenario_results['VaR 99.0% (%)'], width, label='99% VaR')\n",
    "plt.bar(x + width/2, scenario_results['ES 99.0% (%)'], width, label='99% ES')\n",
    "\n",
    "plt.xlabel('Scenario')\n",
    "plt.ylabel('Risk Measure (%)')\n",
    "plt.title('VaR and ES Under Different Scenarios')\n",
    "plt.xticks(x, scenario_results['Scenario'])\n",
    "plt.legend()\n",
    "plt.grid(axis='y')\n",
    "\n",
    "# Add expected return as text\n",
    "for i, v in enumerate(scenario_results['Expected Return (%)']):\n",
    "    plt.text(i, 0.5, f'Return: {v:.1f}%', ha='center', va='bottom', rotation=90, color='black')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Create a risk-return plot\n",
    "plt.figure(figsize=(12, 8))\n",
    "plt.scatter(scenario_results['Volatility (%)'], scenario_results['Expected Return (%)'], \n",
    "            s=100, alpha=0.7)\n",
    "\n",
    "# Add labels for each scenario\n",
    "for i, scenario in enumerate(scenario_results['Scenario']):\n",
    "    plt.annotate(scenario, \n",
    "                 (scenario_results['Volatility (%)'][i], scenario_results['Expected Return (%)'][i]),\n",
    "                 xytext=(10, 5), textcoords='offset points')\n",
    "\n",
    "plt.axhline(y=0, color='gray', linestyle='--')\n",
    "plt.title('Risk-Return Profile Under Different Scenarios')\n",
    "plt.xlabel('Volatility (%)')\n",
    "plt.ylabel('Expected Return (%)')\n",
    "plt.grid(True)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 7: Portfolio Optimization for Risk Reduction\n",
    "# We'll find the minimum variance portfolio as a risk-reduction alternative\n",
    "\n",
    "from scipy.optimize import minimize\n",
    "\n",
    "# Function to calculate portfolio variance (objective function)\n",
    "def portfolio_variance_objective(weights, cov_matrix):\n",
    "    return weights.T @ cov_matrix @ weights\n",
    "\n",
    "# Function to ensure weights sum to 1 (constraint)\n",
    "def sum_to_one(weights):\n",
    "    return np.sum(weights) - 1.0\n",
    "\n",
    "# Find minimum variance portfolio\n",
    "n_assets = len(weight_vector)\n",
    "initial_weights = np.ones(n_assets) / n_assets  # Equal weights\n",
    "\n",
    "# Define constraints\n",
    "constraints = ({'type': 'eq', 'fun': sum_to_one})\n",
    "\n",
    "# Define bounds (no short selling)\n",
    "bounds = tuple((0, 1) for _ in range(n_assets))\n",
    "\n",
    "# Run optimization\n",
    "result = minimize(portfolio_variance_objective, initial_weights, args=(latest_cov,),\n",
    "                 method='SLSQP', bounds=bounds, constraints=constraints)\n",
    "\n",
    "# Get optimized weights\n",
    "min_var_weights = result['x']\n",
    "\n",
    "# Calculate risk metrics for minimum variance portfolio\n",
    "min_var_var = portfolio_variance(latest_cov, min_var_weights)\n",
    "min_var_vol = np.sqrt(min_var_var)\n",
    "min_var_vol_annual = min_var_vol * np.sqrt(252)  # Annualized\n",
    "\n",
    "# Calculate VaR and ES for minimum variance portfolio\n",
    "min_var_var_99, min_var_es_99 = calculate_var_es(None, min_var_vol, 0.99, 't', avg_df)\n",
    "\n",
    "# Create comparison with current portfolio\n",
    "current_vol = np.sqrt(portfolio_variance(latest_cov, weight_vector))\n",
    "current_vol_annual = current_vol * np.sqrt(252)  # Annualized\n",
    "current_var_99, current_es_99 = calculate_var_es(None, current_vol, 0.99, 't', avg_df)\n",
    "\n",
    "# Create weight comparison\n",
    "weight_comparison = pd.DataFrame({\n",
    "    'Asset': returns_df.columns,\n",
    "    'Current Weight': weight_vector * 100,  # Convert to percentage\n",
    "    'Min Variance Weight': min_var_weights * 100  # Convert to percentage\n",
    "})\n",
    "\n",
    "print(\"Weight Comparison:\")\n",
    "weight_comparison\n",
    "\n",
    "# Create risk comparison\n",
    "risk_comparison = pd.DataFrame({\n",
    "    'Portfolio': ['Current', 'Minimum Variance'],\n",
    "    'Volatility (%)': [current_vol_annual * 100, min_var_vol_annual * 100],\n",
    "    'VaR 99% (%)': [-current_var_99 * 100, -min_var_var_99 * 100],\n",
    "    'ES 99% (%)': [-current_es_99 * 100, -min_var_es_99 * 100],\n",
    "    'Volatility Reduction (%)': [0, (1 - min_var_vol_annual / current_vol_annual) * 100],\n",
    "    'VaR Reduction (%)': [0, (1 - min_var_var_99 / current_var_99) * 100],\n",
    "    'ES Reduction (%)': [0, (1 - min_var_es_99 / current_es_99) * 100]\n",
    "})\n",
    "\n",
    "print(\"\nRisk Comparison:\")\n",
    "risk_comparison\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize weight comparison\n",
    "plt.figure(figsize=(14, 7))\n",
    "\n",
    "# Sort by current weight\n",
    "weight_comparison_sorted = weight_comparison.sort_values('Current Weight', ascending=False)\n",
    "\n",
    "# Create grouped bar chart\n",
    "x = np.arange(len(weight_comparison_sorted))\n",
    "width = 0.35\n",
    "\n",
    "plt.bar(x - width/2, weight_comparison_sorted['Current Weight'], width, label='Current Portfolio')\n",
    "plt.bar(x + width/2, weight_comparison_sorted['Min Variance Weight'], width, label='Minimum Variance Portfolio')\n",
    "\n",
    "plt.xlabel('Asset')\n",
    "plt.ylabel('Weight (%)')\n",
    "plt.title('Portfolio Weight Comparison')\n",
    "plt.xticks(x, weight_comparison_sorted['Asset'])\n",
    "plt.legend()\n",
    "plt.grid(axis='y')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Visualize risk comparison\n",
    "plt.figure(figsize=(14, 7))\n",
    "\n",
    "# Create grouped bar chart for risk metrics\n",
    "x = np.arange(3)  # Volatility, VaR, ES\n",
    "width = 0.35\n",
    "\n",
    "metrics = ['Volatility (%)', 'VaR 99% (%)', 'ES 99% (%)']\n",
    "current_values = risk_comparison.loc[0, metrics].values\n",
    "min_var_values = risk_comparison.loc[1, metrics].values\n",
    "\n",
    "plt.bar(x - width/2, current_values, width, label='Current Portfolio')\n",
    "plt.bar(x + width/2, min_var_values, width, label='Minimum Variance Portfolio')\n",
    "\n",
    "plt.xlabel('Risk Metric')\n",
    "plt.ylabel('Value (%)')\n",
    "plt.title('Risk Metric Comparison')\n",
    "plt.xticks(x, metrics)\n",
    "plt.legend()\n",
    "plt.grid(axis='y')\n",
    "\n",
    "# Add reduction percentages as text\n",
    "reductions = risk_comparison.loc[1, ['Volatility Reduction (%)', 'VaR Reduction (%)', 'ES Reduction (%)']].values\n",
    "for i, v in enumerate(reductions):\n",
    "    plt.text(i, min_var_values[i] + 0.5, f'↓ {v:.1f}%', ha='center', va='bottom', color='green')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Case Study 2: Market Regime Analysis\n",
    "\n",
    "In this case study, we'll demonstrate how to identify and analyze different market regimes using bootstrap methods and time series analysis. This approach is useful for understanding how market dynamics change over time and adapting investment strategies accordingly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We'll use the S&P 500 index for this analysis\n",
    "market_ticker = 'SPY'\n",
    "\n",
    "# Load data with a longer history\n",
    "try:\n",
    "    market_data = load_market_data([market_ticker], \"2010-01-01\")\n",
    "    market_prices = market_data[market_ticker]\n",
    "except Exception as e:\n",
    "    print(f\"Error loading data: {e}\")\n",
    "    print(\"Using synthetic data instead\")\n",
    "    market_data = create_synthetic_market_data([market_ticker], \"2010-01-01\", include_regimes=True)\n",
    "    market_prices = market_data[market_ticker]\n",
    "\n",
    "# Calculate returns\n",
    "market_returns = returns_from_prices(market_prices, log=True) * 100  # Convert to percentage\n",
    "\n",
    "# Plot price and returns\n",
    "fig, axes = plt.subplots(2, 1, figsize=(14, 10), sharex=True)\n",
    "\n",
    "# Plot price\n",
    "axes[0].plot(market_prices.index, market_prices, color='blue')\n",
    "axes[0].set_title(f'{market_ticker} Price')\n",
    "axes[0].set_ylabel('Price')\n",
    "axes[0].grid(True)\n",
    "\n",
    "# Plot returns\n",
    "axes[1].plot(market_returns.index, market_returns, color='red', alpha=0.7)\n",
    "axes[1].set_title(f'{market_ticker} Daily Returns (%)')\n",
    "axes[1].set_ylabel('Returns (%)')\n",
    "axes[1].set_xlabel('Date')\n",
    "axes[1].grid(True)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: Calculate rolling statistics to identify potential regime changes\n",
    "window_size = 60  # 60-day rolling window (approximately 3 months)\n",
    "\n",
    "# Calculate rolling statistics\n",
    "rolling_stats = pd.DataFrame(index=market_returns.index)\n",
    "rolling_stats['Returns'] = market_returns\n",
    "rolling_stats['Mean'] = market_returns.rolling(window=window_size).mean()\n",
    "rolling_stats['Volatility'] = market_returns.rolling(window=window_size).std() * np.sqrt(252)  # Annualized\n",
    "rolling_stats['Skewness'] = market_returns.rolling(window=window_size).skew()\n",
    "rolling_stats['Kurtosis'] = market_returns.rolling(window=window_size).kurt()\n",
    "\n",
    "# Calculate rolling Sharpe ratio (assuming risk-free rate of 0 for simplicity)\n",
    "rolling_stats['Sharpe'] = (rolling_stats['Mean'] * 252) / rolling_stats['Volatility']\n",
    "\n",
    "# Plot rolling statistics\n",
    "fig, axes = plt.subplots(3, 1, figsize=(14, 12), sharex=True)\n",
    "\n",
    "# Plot rolling mean and volatility\n",
    "axes[0].plot(rolling_stats.index, rolling_stats['Mean'], color='blue', label='Rolling Mean')\n",
    "axes[0].set_title('Rolling Mean Return (%)')\n",
    "axes[0].set_ylabel('Mean (%)')\n",
    "axes[0].axhline(y=0, color='gray', linestyle='--')\n",
    "axes[0].grid(True)\n",
    "\n",
    "axes[1].plot(rolling_stats.index, rolling_stats['Volatility'], color='red', label='Rolling Volatility')\n",
    "axes[1].set_title('Rolling Volatility (Annualized %)')\n",
    "axes[1].set_ylabel('Volatility (%)')\n",
    "axes[1].grid(True)\n",
    "\n",
    "axes[2].plot(rolling_stats.index, rolling_stats['Sharpe'], color='green', label='Rolling Sharpe')\n",
    "axes[2].set_title('Rolling Sharpe Ratio')\n",
    "axes[2].set_ylabel('Sharpe Ratio')\n",
    "axes[2].axhline(y=0, color='gray', linestyle='--')\n",
    "axes[2].grid(True)\n",
    "axes[2].set_xlabel('Date')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 2: Use change point detection to identify regime changes\n",
    "# We'll use a simple approach based on volatility changes\n",
    "\n",
    "def detect_regime_changes(time_series, window_size=60, z_threshold=1.5):\n",
    "    \"\"\"\n",
    "    Detect regime changes based on volatility shifts.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    time_series : pd.Series\n",
    "        Time series of returns\n",
    "    window_size : int\n",
    "        Rolling window size\n",
    "    z_threshold : float\n",
    "        Z-score threshold for regime change detection\n",
    "        \n",
    "    Returns\n",
    "    -------\n",
    "    pd.DataFrame\n",
    "        DataFrame with regime indicators\n",
    "    \"\"\"\n",
    "    # Calculate rolling volatility\n",
    "    rolling_vol = time_series.rolling(window=window_size).std()\n",
    "    \n",
    "    # Calculate z-score of volatility\n",
    "    vol_mean = rolling_vol.mean()\n",
    "    vol_std = rolling_vol.std()\n",
    "    vol_z = (rolling_vol - vol_mean) / vol_std\n",
    "    \n",
    "    # Identify regime changes\n",
    "    high_vol = vol_z > z_threshold\n",
    "    low_vol = vol_z < -z_threshold\n",
    "    normal_vol = (~high_vol) & (~low_vol)\n",
    "    \n",
    "    # Create regime indicators\n",
    "    regimes = pd.DataFrame(index=time_series.index)\n",
    "    regimes['Returns'] = time_series\n",
    "    regimes['Volatility'] = rolling_vol\n",
    "    regimes['Volatility_Z'] = vol_z\n",
    "    regimes['Regime'] = 0  # Default: normal regime\n",
    "    regimes.loc[high_vol, 'Regime'] = 1  # High volatility regime\n",
    "    regimes.loc[low_vol, 'Regime'] = -1  # Low volatility regime\n",
    "    \n",
    "    # Fill NaN values in Regime\n",
    "    regimes['Regime'] = regimes['Regime'].fillna(0)\n",
    "    \n",
    "    # Add regime names for clarity\n",
    "    regime_names = {-1: 'Low Volatility', 0: 'Normal', 1: 'High Volatility'}\n",
    "    regimes['Regime_Name'] = regimes['Regime'].map(regime_names)\n",
    "    \n",
    "    return regimes\n",
    "\n",
    "# Detect regime changes\n",
    "regimes = detect_regime_changes(market_returns, window_size=60, z_threshold=1.5)\n",
    "\n",
    "# Plot regimes\n",
    "fig, axes = plt.subplots(2, 1, figsize=(14, 10), sharex=True)\n",
    "\n",
    "# Plot returns with regime background\n",
    "for regime in [-1, 0, 1]:\n",
    "    regime_data = regimes[regimes['Regime'] == regime]\n",
    "    if not regime_data.empty:\n",
    "        color = 'green' if regime == -1 else 'gray' if regime == 0 else 'red'\n",
    "        label = regimes.loc[regimes['Regime'] == regime, 'Regime_Name'].iloc[0] if not regime_data.empty else ''\n",
    "        axes[0].scatter(regime_data.index, regime_data['Returns'], color=color, alpha=0.7, s=10, label=label)\n",
    "\n",
    "axes[0].set_title(f'{market_ticker} Returns with Regime Classification')\n",
    "axes[0].set_ylabel('Returns (%)')\n",
    "axes[0].grid(True)\n",
    "axes[0].legend()\n",
    "\n",
    "# Plot volatility z-score with thresholds\n",
    "axes[1].plot(regimes.index, regimes['Volatility_Z'], color='blue')\n",
    "axes[1].axhline(y=1.5, color='red', linestyle='--', label='High Vol Threshold')\n",
    "axes[1].axhline(y=-1.5, color='green', linestyle='--', label='Low Vol Threshold')\n",
    "axes[1].set_title('Volatility Z-Score')\n",
    "axes[1].set_ylabel('Z-Score')\n",
    "axes[1].set_xlabel('Date')\n",
    "axes[1].grid(True)\n",
    "axes[1].legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 3: Analyze regime statistics using bootstrap methods\n",
    "# We'll use bootstrap to estimate the distribution of key statistics in each regime\n",
    "\n",
    "# Function to bootstrap statistics for a given regime\n",
    "def bootstrap_regime_statistics(returns, regime_indicator, n_bootstrap=1000, block_size=10):\n",
    "    \"\"\"\n",
    "    Bootstrap statistics for a specific market regime.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    returns : pd.Series\n",
    "        Time series of returns\n",
    "    regime_indicator : pd.Series\n",
    "        Indicator series for the regime\n",
    "    n_bootstrap : int\n",
    "        Number of bootstrap samples\n",
    "    block_size : int\n",
    "        Block size for block bootstrap\n",
    "        \n",
    "    Returns\n",
    "    -------\n",
    "    dict\n",
    "        Dictionary of bootstrapped statistics\n",
    "    \"\"\"\n",
    "    # Filter returns for the specific regime\n",
    "    regime_returns = returns[regime_indicator]\n",
    "    \n",
    "    # Skip if not enough data\n",
    "    if len(regime_returns) < block_size * 2:\n",
    "        return None\n",
    "    \n",
    "    # Convert to numpy array for bootstrap\n",
    "    returns_array = regime_returns.values\n",
    "    \n",
    "    # Create block bootstrap\n",
    "    bootstrap = BlockBootstrap(block_size=block_size)\n",
    "    \n",
    "    # Function to calculate statistics\n",
    "    def calculate_stats(data):\n",
    "        return {\n",
    "            'Mean': np.mean(data),\n",
    "            'Volatility': np.std(data) * np.sqrt(252),  # Annualized\n",
    "            'Skewness': stats.skew(data),\n",
    "            'Kurtosis': stats.kurtosis(data),\n",
    "            'Sharpe': (np.mean(data) * 252) / (np.std(data) * np.sqrt(252)),\n",
    "            'VaR_95': -np.percentile(data, 5),\n",
    "            'VaR_99': -np.percentile(data, 1),\n",
    "            'Max_Drawdown': calculate_max_drawdown(data)\n",
    "        }\n",
    "    \n",
    "    # Function to calculate maximum drawdown\n",
    "    def calculate_max_drawdown(returns):\n",
    "        # Convert returns to prices (starting at 100)\n",
    "        prices = 100 * np.cumprod(1 + returns / 100)\n",
    "        # Calculate running maximum\n",
    "        running_max = np.maximum.accumulate(prices)\n",
    "        # Calculate drawdowns\n",
    "        drawdowns = (prices / running_max - 1) * 100\n",
    "        # Find maximum drawdown\n",
    "        return np.min(drawdowns)\n",
    "    \n",
    "    # Generate bootstrap samples and calculate statistics\n",
    "    bootstrap_stats = []\n",
    "    for _ in range(n_bootstrap):\n",
    "        # Generate bootstrap sample\n",
    "        bootstrap_sample = bootstrap.generate(returns_array)\n",
    "        # Calculate statistics\n",
    "        stats_dict = calculate_stats(bootstrap_sample)\n",
    "        bootstrap_stats.append(stats_dict)\n",
    "    \n",
    "    # Convert list of dictionaries to dictionary of lists\n",
    "    result = {}\n",
    "    for key in bootstrap_stats[0].keys():\n",
    "        result[key] = [stats_dict[key] for stats_dict in bootstrap_stats]\n",
    "    \n",
    "    return result\n",
    "\n",
    "# Bootstrap statistics for each regime\n",
    "regime_bootstrap_stats = {}\n",
    "for regime in [-1, 0, 1]:\n",
    "    regime_indicator = regimes['Regime'] == regime\n",
    "    if regime_indicator.sum() > 0:  # Only if we have data for this regime\n",
    "        regime_name = regimes.loc[regimes['Regime'] == regime, 'Regime_Name'].iloc[0]\n",
    "        print(f\"Bootstrapping statistics for {regime_name} regime...\")\n",
    "        bootstrap_stats = bootstrap_regime_statistics(regimes['Returns'], regime_indicator, n_bootstrap=1000, block_size=10)\n",
    "        if bootstrap_stats is not None:\n",
    "            regime_bootstrap_stats[regime] = bootstrap_stats\n",
    "            print(f\"  - {len(bootstrap_stats['Mean'])} bootstrap samples generated\")\n",
    "        else:\n",
    "            print(f\"  - Not enough data for bootstrapping\")\n",
    "\n",
    "# Create summary statistics for each regime\n",
    "regime_summary = []\n",
    "for regime, bootstrap_stats in regime_bootstrap_stats.items():\n",
    "    regime_name = regimes.loc[regimes['Regime'] == regime, 'Regime_Name'].iloc[0]\n",
    "    \n",
    "    # Calculate mean and confidence intervals for each statistic\n",
    "    summary = {'Regime': regime_name}\n",
    "    for stat, values in bootstrap_stats.items():\n",
    "        summary[f'{stat}_Mean'] = np.mean(values)\n",
    "        summary[f'{stat}_5th'] = np.percentile(values, 5)\n",
    "        summary[f'{stat}_95th'] = np.percentile(values, 95)\n",
    "    \n",
    "    regime_summary.append(summary)\n",
    "\n",
    "# Convert to DataFrame\n",
    "regime_summary_df = pd.DataFrame(regime_summary)\n",
    "\n",
    "# Display key statistics\n",
    "key_stats = ['Mean', 'Volatility', 'Sharpe', 'VaR_95', 'Max_Drawdown']\n",
    "display_cols = ['Regime'] + [f'{stat}_{suffix}' for stat in key_stats for suffix in ['Mean', '5th', '95th']]\n",
    "display_df = regime_summary_df[display_cols]\n",
    "\n",
    "print(\"\nRegime Statistics Summary:\")\n",
    "display_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize bootstrap distributions for key statistics\n",
    "key_stats = ['Mean', 'Volatility', 'Sharpe', 'VaR_95']\n",
    "fig, axes = plt.subplots(len(key_stats), 1, figsize=(14, 4 * len(key_stats)))\n",
    "\n",
    "for i, stat in enumerate(key_stats):\n",
    "    for regime, bootstrap_stats in regime_bootstrap_stats.items():\n",
    "        regime_name = regimes.loc[regimes['Regime'] == regime, 'Regime_Name'].iloc[0]\n",
    "        color = 'green' if regime == -1 else 'gray' if regime == 0 else 'red'\n",
    "        \n",
    "        # Plot kernel density estimate\n",
    "        sns.kdeplot(bootstrap_stats[stat], ax=axes[i], color=color, label=regime_name)\n",
    "    \n",
    "    axes[i].set_title(f'Bootstrap Distribution of {stat}')\n",
    "    axes[i].grid(True)\n",
    "    axes[i].legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Create a comparative bar chart for key statistics\n",
    "fig, axes = plt.subplots(len(key_stats), 1, figsize=(14, 4 * len(key_stats)))\n",
    "\n",
    "for i, stat in enumerate(key_stats):\n",
    "    # Extract data for this statistic\n",
    "    regimes = []\n",
    "    means = []\n",
    "    lower_bounds = []\n",
    "    upper_bounds = []\n",
    "    \n",
    "    for _, row in regime_summary_df.iterrows():\n",
    "        regimes.append(row['Regime'])\n",
    "        means.append(row[f'{stat}_Mean'])\n",
    "        lower_bounds.append(row[f'{stat}_5th'])\n",
    "        upper_bounds.append(row[f'{stat}_95th'])\n",
    "    \n",
    "    # Calculate error bars\n",
    "    yerr = np.array([np.array(means) - np.array(lower_bounds), np.array(upper_bounds) - np.array(means)])\n",
    "    \n",
    "    # Create bar chart\n",
    "    bars = axes[i].bar(regimes, means, yerr=yerr, capsize=10)\n",
    "    \n",
    "    # Color bars based on regime\n",
    "    for j, bar in enumerate(bars):\n",
    "        regime_name = regimes[j]\n",
    "        if 'Low' in regime_name:\n",
    "            bar.set_color('green')\n",
    "        elif 'Normal' in regime_name:\n",
    "            bar.set_color('gray')\n",
    "        else:  # High volatility\n",
    "            bar.set_color('red')\n",
    "    \n",
    "    axes[i].set_title(f'{stat} by Market Regime (with 90% Confidence Interval)')\n",
    "    axes[i].grid(True, axis='y')\n",
    "    \n",
    "    # Add value labels\n",
    "    for j, v in enumerate(means):\n",
    "        axes[i].text(j, v + 0.1, f'{v:.2f}', ha='center')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 4: Analyze regime transitions using Markov Chain analysis\n",
    "# We'll estimate the transition probabilities between regimes\n",
    "\n",
    "def calculate_transition_matrix(regime_series):\n",
    "    \"\"\"\n",
    "    Calculate transition matrix for regime changes.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    regime_series : pd.Series\n",
    "        Series of regime indicators\n",
    "        \n",
    "    Returns\n",
    "    -------\n",
    "    pd.DataFrame\n",
    "        Transition probability matrix\n",
    "    \"\"\"\n",
    "    # Get unique regimes\n",
    "    regimes = sorted(regime_series.unique())\n",
    "    n_regimes = len(regimes)\n",
    "    \n",
    "    # Initialize transition count matrix\n",
    "    transition_counts = np.zeros((n_regimes, n_regimes))\n",
    "    \n",
    "    # Count transitions\n",
    "    for i in range(len(regime_series) - 1):\n",
    "        from_regime = regime_series.iloc[i]\n",
    "        to_regime = regime_series.iloc[i + 1]\n",
    "        \n",
    "        # Get indices in the transition matrix\n",
    "        from_idx = regimes.index(from_regime)\n",
    "        to_idx = regimes.index(to_regime)\n",
    "        \n",
    "        # Increment count\n",
    "        transition_counts[from_idx, to_idx] += 1\n",
    "    \n",
    "    # Calculate transition probabilities\n",
    "    transition_probs = np.zeros_like(transition_counts)\n",
    "    for i in range(n_regimes):\n",
    "        row_sum = transition_counts[i].sum()\n",
    "        if row_sum > 0:\n",
    "            transition_probs[i] = transition_counts[i] / row_sum\n",
    "    \n",
    "    # Create DataFrame\n",
    "    regime_names = [regimes.loc[regimes['Regime'] == r, 'Regime_Name'].iloc[0] for r in regimes]\n",
    "    transition_df = pd.DataFrame(transition_probs, index=regime_names, columns=regime_names)\n",
    "    \n",
    "    return transition_df\n",
    "\n",
    "# Calculate transition matrix\n",
    "transition_matrix = calculate_transition_matrix(regimes['Regime'])\n",
    "\n",
    "print(\"Regime Transition Probability Matrix:\")\n",
    "transition_matrix\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize transition matrix\n",
    "plt.figure(figsize=(10, 8))\n",
    "sns.heatmap(transition_matrix, annot=True, cmap='coolwarm', vmin=0, vmax=1, \n",
    "            square=True, linewidths=0.5, cbar_kws={\"shrink\": 0.8})\n",
    "plt.title('Regime Transition Probability Matrix')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Calculate regime duration statistics\n",
    "def calculate_regime_durations(regime_series):\n",
    "    \"\"\"\n",
    "    Calculate statistics on regime durations.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    regime_series : pd.Series\n",
    "        Series of regime indicators\n",
    "        \n",
    "    Returns\n",
    "    -------\n",
    "    pd.DataFrame\n",
    "        DataFrame with regime duration statistics\n",
    "    \"\"\"\n",
    "    # Initialize variables\n",
    "    current_regime = regime_series.iloc[0]\n",
    "    current_duration = 1\n",
    "    durations = {}\n",
    "    \n",
    "    # Iterate through the series\n",
    "    for i in range(1, len(regime_series)):\n",
    "        if regime_series.iloc[i] == current_regime:\n",
    "            current_duration += 1\n",
    "        else:\n",
    "            # Record duration for the previous regime\n",
    "            if current_regime not in durations:\n",
    "                durations[current_regime] = []\n",
    "            durations[current_regime].append(current_duration)\n",
    "            \n",
    "            # Reset for new regime\n",
    "            current_regime = regime_series.iloc[i]\n",
    "            current_duration = 1\n",
    "    \n",
    "    # Record the last regime duration\n",
    "    if current_regime not in durations:\n",
    "        durations[current_regime] = []\n",
    "    durations[current_regime].append(current_duration)\n",
    "    \n",
    "    # Calculate statistics\n",
    "    stats = []\n",
    "    for regime, duration_list in durations.items():\n",
    "        regime_name = regimes.loc[regimes['Regime'] == regime, 'Regime_Name'].iloc[0]\n",
    "        stats.append({\n",
    "            'Regime': regime_name,\n",
    "            'Count': len(duration_list),\n",
    "            'Min Duration': min(duration_list),\n",
    "            'Max Duration': max(duration_list),\n",
    "            'Mean Duration': np.mean(duration_list),\n",
    "            'Median Duration': np.median(duration_list),\n",
    "            'Total Days': sum(duration_list),\n",
    "            'Percentage': sum(duration_list) / len(regime_series) * 100\n",
    "        })\n",
    "    \n",
    "    return pd.DataFrame(stats)\n",
    "\n",
    "# Calculate regime durations\n",
    "duration_stats = calculate_regime_durations(regimes['Regime'])\n",
    "\n",
    "print(\"\nRegime Duration Statistics:\")\n",
    "duration_stats\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize regime durations\n",
    "plt.figure(figsize=(14, 7))\n",
    "\n",
    "# Create bar chart for mean durations\n",
    "bars = plt.bar(duration_stats['Regime'], duration_stats['Mean Duration'])\n",
    "\n",
    "# Color bars based on regime\n",
    "for i, bar in enumerate(bars):\n",
    "    regime_name = duration_stats.iloc[i]['Regime']\n",
    "    if 'Low' in regime_name:\n",
    "        bar.set_color('green')\n",
    "    elif 'Normal' in regime_name:\n",
    "        bar.set_color('gray')\n",
    "    else:  # High volatility\n",
    "        bar.set_color('red')\n",
    "\n",
    "plt.title('Mean Duration of Market Regimes (Trading Days)')\n",
    "plt.ylabel('Mean Duration (Days)')\n",
    "plt.grid(axis='y')\n",
    "\n",
    "# Add value labels\n",
    "for i, v in enumerate(duration_stats['Mean Duration']):\n",
    "    plt.text(i, v + 0.5, f'{v:.1f}', ha='center')\n",
    "\n",
    "# Add percentage labels\n",
    "for i, v in enumerate(duration_stats['Percentage']):\n",
    "    plt.text(i, 5, f'{v:.1f}% of time', ha='center', color='white', fontweight='bold')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Create a pie chart for regime distribution\n",
    "plt.figure(figsize=(10, 8))\n",
    "colors = ['green', 'gray', 'red']\n",
    "plt.pie(duration_stats['Percentage'], labels=duration_stats['Regime'], autopct='%1.1f%%', \n",
    "        colors=colors, startangle=90)\n",
    "plt.title('Distribution of Market Regimes')\n",
    "plt.axis('equal')  # Equal aspect ratio ensures that pie is drawn as a circle\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 5: Predict future regime probabilities using the Markov model\n",
    "# We'll use the transition matrix to forecast regime probabilities\n",
    "\n",
    "def forecast_regime_probabilities(transition_matrix, current_regime, n_periods=30):\n",
    "    \"\"\"\n",
    "    Forecast future regime probabilities using a Markov model.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    transition_matrix : pd.DataFrame\n",
    "        Transition probability matrix\n",
    "    current_regime : str\n",
    "        Current regime name\n",
    "    n_periods : int\n",
    "        Number of periods to forecast\n",
    "        \n",
    "    Returns\n",
    "    -------\n",
    "    pd.DataFrame\n",
    "        DataFrame with forecasted probabilities\n",
    "    \"\"\"\n",
    "    # Get regime names\n",
    "    regimes = transition_matrix.index.tolist()\n",
    "    n_regimes = len(regimes)\n",
    "    \n",
    "    # Initialize probability vector\n",
    "    # Start with 100% probability in the current regime\n",
    "    current_idx = regimes.index(current_regime)\n",
    "    prob_vector = np.zeros(n_regimes)\n",
    "    prob_vector[current_idx] = 1.0\n",
    "    \n",
    "    # Convert transition matrix to numpy array\n",
    "    trans_matrix = transition_matrix.values\n",
    "    \n",
    "    # Initialize results\n",
    "    forecasts = []\n",
    "    forecasts.append({\n",
    "        'Period': 0,\n",
    "        **{regime: prob for regime, prob in zip(regimes, prob_vector)}\n",
    "    })\n",
    "    \n",
    "    # Forecast future probabilities\n",
    "    for t in range(1, n_periods + 1):\n",
    "        # Update probability vector\n",
    "        prob_vector = prob_vector @ trans_matrix\n",
    "        \n",
    "        # Store results\n",
    "        forecasts.append({\n",
    "            'Period': t,\n",
    "            **{regime: prob for regime, prob in zip(regimes, prob_vector)}\n",
    "        })\n",
    "    \n",
    "    return pd.DataFrame(forecasts)\n",
    "\n",
    "# Get current regime\n",
    "current_regime = regimes.loc[regimes.index[-1], 'Regime_Name']\n",
    "print(f\"Current regime: {current_regime}\")\n",
    "\n",
    "# Forecast regime probabilities\n",
    "forecast_horizon = 60  # 60 trading days (approximately 3 months)\n",
    "regime_forecasts = forecast_regime_probabilities(transition_matrix, current_regime, forecast_horizon)\n",
    "\n",
    "# Display forecast for selected periods\n",
    "selected_periods = [0, 1, 5, 10, 20, 30, 60]\n",
    "print(\"\nForecasted Regime Probabilities:\")\n",
    "regime_forecasts[regime_forecasts['Period'].isin(selected_periods)]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize regime probability forecasts\n",
    "plt.figure(figsize=(14, 7))\n",
    "\n",
    "# Plot probability evolution for each regime\n",
    "for regime in transition_matrix.index:\n",
    "    color = 'green' if 'Low' in regime else 'gray' if 'Normal' in regime else 'red'\n",
    "    plt.plot(regime_forecasts['Period'], regime_forecasts[regime], \n",
    "             label=regime, color=color, linewidth=2)\n",
    "\n",
    "plt.title('Forecasted Regime Probabilities')\n",
    "plt.xlabel('Trading Days Ahead')\n",
    "plt.ylabel('Probability')\n",
    "plt.grid(True)\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Create a stacked area chart for regime probabilities\n",
    "plt.figure(figsize=(14, 7))\n",
    "\n",
    "# Get regime names in the right order (low, normal, high)\n",
    "regime_order = sorted(transition_matrix.index, key=lambda x: 'Low' in x and 0 or 'Normal' in x and 1 or 2)\n",
    "\n",
    "# Create stacked area chart\n",
    "plt.stackplot(regime_forecasts['Period'], \n",
    "              [regime_forecasts[regime] for regime in regime_order],\n",
    "              labels=regime_order,\n",
    "              colors=['green', 'gray', 'red'],\n",
    "              alpha=0.8)\n",
    "\n",
    "plt.title('Regime Probability Evolution')\n",
    "plt.xlabel('Trading Days Ahead')\n",
    "plt.ylabel('Probability')\n",
    "plt.grid(True)\n",
    "plt.legend(loc='upper right')\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 6: Calculate expected portfolio performance based on regime forecasts\n",
    "# We'll combine regime probabilities with regime-specific performance metrics\n",
    "\n",
    "def calculate_expected_performance(regime_forecasts, regime_stats):\n",
    "    \"\"\"\n",
    "    Calculate expected portfolio performance based on regime forecasts.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    regime_forecasts : pd.DataFrame\n",
    "        Forecasted regime probabilities\n",
    "    regime_stats : pd.DataFrame\n",
    "        Regime-specific performance statistics\n",
    "        \n",
    "    Returns\n",
    "    -------\n",
    "    pd.DataFrame\n",
    "        DataFrame with expected performance metrics\n",
    "    \"\"\"\n",
    "    # Initialize results\n",
    "    expected_performance = pd.DataFrame()\n",
    "    expected_performance['Period'] = regime_forecasts['Period']\n",
    "    \n",
    "    # Calculate expected values for each statistic\n",
    "    for stat in ['Mean', 'Volatility', 'Sharpe', 'VaR_95']:\n",
    "        expected_performance[f'Expected_{stat}'] = 0.0\n",
    "        \n",
    "        # Sum weighted statistics across regimes\n",
    "        for _, row in regime_stats.iterrows():\n",
    "            regime = row['Regime']\n",
    "            stat_value = row[f'{stat}_Mean']\n",
    "            expected_performance[f'Expected_{stat}'] += regime_forecasts[regime] * stat_value\n",
    "    \n",
    "    return expected_performance\n",
    "\n",
    "# Calculate expected performance\n",
    "expected_performance = calculate_expected_performance(regime_forecasts, regime_summary_df)\n",
    "\n",
    "# Display expected performance for selected periods\n",
    "selected_periods = [0, 1, 5, 10, 20, 30, 60]\n",
    "print(\"Expected Performance Metrics:\")\n",
    "expected_performance[expected_performance['Period'].isin(selected_periods)]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize expected performance metrics\n",
    "fig, axes = plt.subplots(2, 2, figsize=(14, 10), sharex=True)\n",
    "axes = axes.flatten()\n",
    "\n",
    "# Plot expected metrics\n",
    "metrics = ['Expected_Mean', 'Expected_Volatility', 'Expected_Sharpe', 'Expected_VaR_95']\n",
    "titles = ['Expected Daily Return (%)', 'Expected Volatility (Annualized %)', \n",
    "          'Expected Sharpe Ratio', 'Expected 95% VaR (%)']\n",
    "\n",
    "for i, (metric, title) in enumerate(zip(metrics, titles)):\n",
    "    axes[i].plot(expected_performance['Period'], expected_performance[metric], color='blue', linewidth=2)\n",
    "    axes[i].set_title(title)\n",
    "    axes[i].grid(True)\n",
    "    \n",
    "    # Add horizontal line at current value\n",
    "    current_value = expected_performance.loc[0, metric]\n",
    "    axes[i].axhline(y=current_value, color='red', linestyle='--', \n",
    "                   label=f'Current: {current_value:.4f}')\n",
    "    axes[i].legend()\n",
    "\n",
    "# Set common x-axis label\n",
    "for ax in axes[2:]:  # Bottom row\n",
    "    ax.set_xlabel('Trading Days Ahead')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 7: Simulate future paths based on regime forecasts\n",
    "# We'll generate Monte Carlo simulations that incorporate regime dynamics\n",
    "\n",
    "def simulate_regime_paths(transition_matrix, regime_stats, current_regime, n_days=252, n_paths=1000):\n",
    "    \"\"\"\n",
    "    Simulate future paths incorporating regime dynamics.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    transition_matrix : pd.DataFrame\n",
    "        Transition probability matrix\n",
    "    regime_stats : pd.DataFrame\n",
    "        Regime-specific performance statistics\n",
    "    current_regime : str\n",
    "        Current regime name\n",
    "    n_days : int\n",
    "        Number of days to simulate\n",
    "    n_paths : int\n",
    "        Number of simulation paths\n",
    "        \n",
    "    Returns\n",
    "    -------\n",
    "    tuple\n",
    "        (simulated_returns, simulated_regimes, simulated_prices)\n",
    "    \"\"\"\n",
    "    # Get regime names and indices\n",
    "    regimes = transition_matrix.index.tolist()\n",
    "    current_idx = regimes.index(current_regime)\n",
    "    \n",
    "    # Convert transition matrix to numpy array\n",
    "    trans_matrix = transition_matrix.values\n",
    "    \n",
    "    # Create dictionaries for regime parameters\n",
    "    regime_params = {}\n",
    "    for _, row in regime_stats.iterrows():\n",
    "        regime = row['Regime']\n",
    "        regime_params[regime] = {\n",
    "            'mean': row['Mean_Mean'],\n",
    "            'vol': row['Volatility_Mean'] / np.sqrt(252),  # Convert to daily\n",
    "            'skew': row['Skewness_Mean'] if 'Skewness_Mean' in row else 0,\n",
    "            'kurt': row['Kurtosis_Mean'] if 'Kurtosis_Mean' in row else 3\n",
    "        }\n",
    "    \n",
    "    # Initialize arrays for simulations\n",
    "    simulated_returns = np.zeros((n_days, n_paths))\n",
    "    simulated_regimes = np.zeros((n_days, n_paths), dtype=int)\n",
    "    simulated_prices = np.zeros((n_days + 1, n_paths))\n",
    "    simulated_prices[0] = 100  # Start at 100\n",
    "    \n",
    "    # Simulate paths\n",
    "    for path in range(n_paths):\n",
    "        # Start in current regime\n",
    "        regime_idx = current_idx\n",
    "        current_regime_name = regimes[regime_idx]\n",
    "        \n",
    "        for day in range(n_days):\n",
    "            # Record current regime\n",
    "            simulated_regimes[day, path] = regime_idx\n",
    "            \n",
    "            # Get regime parameters\n",
    "            params = regime_params[current_regime_name]\n",
    "            \n",
    "            # Generate return for this day\n",
    "            # For simplicity, we'll use normal distribution\n",
    "            # In practice, you might want to use a more sophisticated distribution\n",
    "            daily_return = np.random.normal(params['mean'], params['vol'])\n",
    "            simulated_returns[day, path] = daily_return\n",
    "            \n",
    "            # Update price\n",
    "            simulated_prices[day + 1, path] = simulated_prices[day, path] * (1 + daily_return / 100)\n",
    "            \n",
    "            # Determine next regime\n",
    "            regime_probs = trans_matrix[regime_idx]\n",
    "            next_regime_idx = np.random.choice(len(regimes), p=regime_probs)\n",
    "            regime_idx = next_regime_idx\n",
    "            current_regime_name = regimes[regime_idx]\n",
    "    \n",
    "    return simulated_returns, simulated_regimes, simulated_prices\n",
    "\n",
    "# Simulate future paths\n",
    "n_days = 252  # 1 year\n",
    "n_paths = 1000\n",
    "print(f\"Simulating {n_paths} paths for {n_days} trading days...\")\n",
    "sim_returns, sim_regimes, sim_prices = simulate_regime_paths(\n",
    "    transition_matrix, regime_summary_df, current_regime, n_days, n_paths)\n",
    "print(\"Simulation complete.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize simulation results\n",
    "# Plot a subset of price paths\n",
    "plt.figure(figsize=(14, 7))\n",
    "\n",
    "# Create time index\n",
    "time_index = np.arange(n_days + 1)\n",
    "\n",
    "# Plot a subset of paths\n",
    "n_plot_paths = 50\n",
    "plot_indices = np.random.choice(n_paths, n_plot_paths, replace=False)\n",
    "for idx in plot_indices:\n",
    "    plt.plot(time_index, sim_prices[:, idx], color='blue', alpha=0.1)\n",
    "\n",
    "# Plot median path\n",
    "median_path = np.median(sim_prices, axis=1)\n",
    "plt.plot(time_index, median_path, color='red', linewidth=2, label='Median Path')\n",
    "\n",
    "# Plot confidence intervals\n",
    "lower_5 = np.percentile(sim_prices, 5, axis=1)\n",
    "upper_95 = np.percentile(sim_prices, 95, axis=1)\n",
    "plt.fill_between(time_index, lower_5, upper_95, color='red', alpha=0.2, label='90% Confidence Interval')\n",
    "\n",
    "plt.title('Simulated Price Paths with Regime Dynamics')\n",
    "plt.xlabel('Trading Days')\n",
    "plt.ylabel('Price (Base = 100)')\n",
    "plt.grid(True)\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Calculate terminal price distribution\n",
    "terminal_prices = sim_prices[-1]\n",
    "\n",
    "# Plot terminal price distribution\n",
    "plt.figure(figsize=(14, 7))\n",
    "sns.histplot(terminal_prices, kde=True, bins=50)\n",
    "plt.axvline(x=100, color='red', linestyle='--', label='Initial Price')\n",
    "plt.axvline(x=np.median(terminal_prices), color='green', linestyle='--', \n",
    "            label=f'Median: {np.median(terminal_prices):.2f}')\n",
    "plt.title('Terminal Price Distribution after 1 Year')\n",
    "plt.xlabel('Price (Base = 100)')\n",
    "plt.ylabel('Frequency')\n",
    "plt.grid(True)\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Calculate key statistics\n",
    "terminal_returns = (terminal_prices / 100 - 1) * 100  # Convert to percentage\n",
    "print(\"Terminal Return Statistics:\")\n",
    "print(f\"Median Return: {np.median(terminal_returns):.2f}%\")\n",
    "print(f\"Mean Return: {np.mean(terminal_returns):.2f}%\")\n",
    "print(f\"5th Percentile: {np.percentile(terminal_returns, 5):.2f}%\")\n",
    "print(f\"95th Percentile: {np.percentile(terminal_returns, 95):.2f}%\")\n",
    "print(f\"Probability of Positive Return: {(terminal_returns > 0).mean() * 100:.2f}%\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Case Study 3: Volatility Forecasting for Options Trading\n",
    "\n",
    "In this case study, we'll demonstrate how to combine GARCH models with realized volatility measures to improve volatility forecasting for options trading. This approach is particularly useful for options pricing and trading strategies that depend on accurate volatility forecasts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For this case study, we'll use a single asset (SPY) with both daily and intraday data\n",
    "# We'll simulate intraday data since we don't have actual high-frequency data\n",
    "\n",
    "# Function to simulate intraday data\n",
    "def simulate_intraday_data(daily_prices, n_intraday=78):\n",
    "    \"\"\"\n",
    "    Simulate intraday price data based on daily prices.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    daily_prices : pd.Series\n",
    "        Daily price series\n",
    "    n_intraday : int\n",
    "        Number of intraday observations per day (e.g., 78 for 5-minute data in 6.5-hour trading day)\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    pd.DataFrame\n",
    "        DataFrame with simulated intraday prices\n",
    "    \"\"\"\n",
    "    # Get daily returns\n",
    "    daily_returns = daily_prices.pct_change().dropna()\n",
    "    \n",
    "    # Calculate daily volatility\n",
    "    daily_vol = daily_returns.std()\n",
    "    \n",
    "    # Calculate intraday volatility (assuming square-root-of-time rule)\n",
    "    intraday_vol = daily_vol / np.sqrt(n_intraday)\n",
    "    \n",
    "    # Initialize DataFrame for intraday data\n",
    "    intraday_data = []\n",
    "    \n",
    "    # Generate intraday data for each day\n",
    "    for day, (date, daily_price) in enumerate(daily_prices.items()):\n",
    "        # Skip the first day (no return available)\n",
    "        if day == 0:\n",
    "            continue\n",
    "            \n",
    "        # Get daily return for this day\n",
    "        daily_ret = daily_returns.iloc[day-1]\n",
    "        \n",
    "        # Calculate open price (previous day's close)\n",
    "        open_price = daily_prices.iloc[day-1]\n",
    "        \n",
    "        # Calculate close price (this day's close)\n",
    "        close_price = daily_price\n",
    "        \n",
    "        # Generate intraday returns with the same total return\n",
    "        # We'll use a random walk with drift\n",
    "        intraday_drift = daily_ret / n_intraday\n",
    "        intraday_returns = np.random.normal(intraday_drift, intraday_vol, n_intraday)\n",
    "        \n",
    "        # Adjust to match daily return\n",
    "        actual_return = (1 + intraday_returns).prod() - 1\n",
    "        adjustment = daily_ret / actual_return\n",
    "        intraday_returns = intraday_returns * adjustment\n",
    "        \n",
    "        # Generate intraday prices\n",
    "        intraday_prices = open_price * np.cumprod(1 + intraday_returns)\n",
    "        \n",
    "        # Ensure the last price matches the close price\n",
    "        intraday_prices[-1] = close_price\n",
    "        \n",
    "        # Create timestamps for this day\n",
    "        # Assuming 9:30 AM to 4:00 PM trading hours\n",
    "        start_time = pd.Timestamp(date.date()).replace(hour=9, minute=30)\n",
    "        end_time = pd.Timestamp(date.date()).replace(hour=16, minute=0)\n",
    "        timestamps = pd.date_range(start=start_time, end=end_time, periods=n_intraday)\n",
    "        \n",
    "        # Add to intraday data\n",
    "        for i, (ts, price) in enumerate(zip(timestamps, intraday_prices)):\n",
    "            intraday_data.append({\n",
    "                'Timestamp': ts,\n",
    "                'Price': price,\n",
    "                'Date': date.date()\n",
    "            })\n",
    "    \n",
    "    # Convert to DataFrame\n",
    "    intraday_df = pd.DataFrame(intraday_data)\n",
    "    intraday_df.set_index('Timestamp', inplace=True)\n",
    "    \n",
    "    return intraday_df\n",
    "\n",
    "# Simulate intraday data\n",
    "print(\"Simulating intraday data...\")\n",
    "intraday_data = simulate_intraday_data(market_prices, n_intraday=78)  # 5-minute data\n",
    "print(f\"Generated {len(intraday_data)} intraday observations\")\n",
    "\n",
    "# Display sample of intraday data\n",
    "print(\"\nSample of intraday data:\")\n",
    "intraday_data.head(10)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot a sample day of intraday data\n",
    "# Select a random day\n",
    "sample_date = intraday_data['Date'].unique()[100]  # Some day in the middle\n",
    "sample_day = intraday_data[intraday_data['Date'] == sample_date]\n",
    "\n",
    "plt.figure(figsize=(14, 7))\n",
    "plt.plot(sample_day.index, sample_day['Price'])\n",
    "plt.title(f'Simulated Intraday Prices for {sample_date}')\n",
    "plt.xlabel('Time')\n",
    "plt.ylabel('Price')\n",
    "plt.grid(True)\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: Calculate realized volatility measures\n",
    "# We'll calculate daily realized volatility using intraday returns\n",
    "\n",
    "def calculate_realized_volatility(intraday_data):\n",
    "    \"\"\"\n",
    "    Calculate daily realized volatility measures from intraday data.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    intraday_data : pd.DataFrame\n",
    "        DataFrame with intraday prices\n",
    "        \n",
    "    Returns\n",
    "    -------\n",
    "    pd.DataFrame\n",
    "        DataFrame with daily realized volatility measures\n",
    "    \"\"\"\n",
    "    # Calculate intraday returns\n",
    "    intraday_data['Return'] = intraday_data['Price'].pct_change()\n",
    "    \n",
    "    # Group by date\n",
    "    daily_groups = intraday_data.groupby('Date')\n",
    "    \n",
    "    # Initialize results\n",
    "    realized_vol = []\n",
    "    \n",
    "    # Calculate realized volatility for each day\n",
    "    for date, group in daily_groups:\n",
    "        # Skip days with insufficient data\n",
    "        if len(group) < 10:\n",
    "            continue\n",
    "            \n",
    "        # Get returns for this day\n",
    "        returns = group['Return'].dropna().values\n",
    "        \n",
    "        # Calculate realized variance (sum of squared returns)\n",
    "        rv = np.sum(returns**2) * 100**2  # Convert to percentage squared\n",
    "        \n",
    "        # Calculate bipower variation (robust to jumps)\n",
    "        # BPV = (π/2) * sum(|r_i| * |r_{i-1}|)\n",
    "        abs_returns = np.abs(returns)\n",
    "        bpv = (np.pi/2) * np.sum(abs_returns[1:] * abs_returns[:-1]) * 100**2\n",
    "        \n",
    "        # Calculate jump component\n",
    "        jump = max(0, rv - bpv)\n",
    "        \n",
    "        # Store results\n",
    "        realized_vol.append({\n",
    "            'Date': date,\n",
    "            'RV': rv,  # Realized variance\n",
    "            'RV_Annualized': rv * 252,  # Annualized\n",
    "            'RV_Sqrt': np.sqrt(rv),  # Realized volatility\n",
    "            'RV_Sqrt_Annualized': np.sqrt(rv) * np.sqrt(252),  # Annualized\n",
    "            'BPV': bpv,  # Bipower variation\n",
    "            'BPV_Sqrt': np.sqrt(bpv),  # Square root of bipower variation\n",
    "            'Jump': jump,  # Jump component\n",
    "            'Jump_Ratio': jump / rv if rv > 0 else 0  # Jump ratio\n",
    "        })\n",
    "    \n",
    "    # Convert to DataFrame\n",
    "    rv_df = pd.DataFrame(realized_vol)\n",
    "    rv_df.set_index('Date', inplace=True)\n",
    "    \n",
    "    return rv_df\n",
    "\n",
    "# Calculate realized volatility\n",
    "realized_vol_df = calculate_realized_volatility(intraday_data)\n",
    "\n",
    "# Display summary statistics\n",
    "print(\"Realized Volatility Summary Statistics:\")\n",
    "realized_vol_df.describe()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot realized volatility measures\n",
    "fig, axes = plt.subplots(3, 1, figsize=(14, 12), sharex=True)\n",
    "\n",
    "# Plot realized volatility (annualized)\n",
    "axes[0].plot(realized_vol_df.index, realized_vol_df['RV_Sqrt_Annualized'], color='blue')\n",
    "axes[0].set_title('Realized Volatility (Annualized %)')\n",
    "axes[0].set_ylabel('Volatility (%)')\n",
    "axes[0].grid(True)\n",
    "\n",
    "# Plot bipower variation (annualized)\n",
    "axes[1].plot(realized_vol_df.index, realized_vol_df['BPV_Sqrt'] * np.sqrt(252), color='green')\n",
    "axes[1].set_title('Bipower Variation (Annualized %)')\n",
    "axes[1].set_ylabel('Volatility (%)')\n",
    "axes[1].grid(True)\n",
    "\n",
    "# Plot jump ratio\n",
    "axes[2].plot(realized_vol_df.index, realized_vol_df['Jump_Ratio'], color='red')\n",
    "axes[2].set_title('Jump Ratio (Jump Component / Realized Variance)')\n",
    "axes[2].set_ylabel('Ratio')\n",
    "axes[2].set_xlabel('Date')\n",
    "axes[2].grid(True)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 2: Estimate GARCH model using daily returns\n",
    "# We'll use a GARCH(1,1) model with Student's t distribution\n",
    "\n",
    "# Prepare daily returns\n",
    "daily_returns_array = market_returns.values\n",
    "\n",
    "# Create and estimate GARCH model\n",
    "print(\"Estimating GARCH(1,1) model...\")\n",
    "garch_model = GARCH(p=1, q=1, distribution=StudentT())\n",
    "garch_results = garch_model.fit(daily_returns_array)\n",
    "print(\"GARCH estimation complete.\")\n",
    "\n",
    "# Display estimation results\n",
    "print(\"\nGARCH(1,1) Estimation Results:\")\n",
    "print(f\"Log-Likelihood: {garch_results.log_likelihood:.4f}\")\n",
    "print(f\"AIC: {garch_results.aic:.4f}\")\n",
    "print(f\"BIC: {garch_results.bic:.4f}\")\n",
    "print(\"\nParameter Estimates:\")\n",
    "for name, value, std_err, t_stat, p_value in zip(\n",
    "    garch_results.parameter_names,\n",
    "    garch_results.parameters,\n",
    "    garch_results.std_errors,\n",
    "    garch_results.t_stats,\n",
    "    garch_results.p_values\n",
    "):\n",
    "    print(f\"{name}: {value:.6f} (SE: {std_err:.6f}, t: {t_stat:.4f}, p: {p_value:.4f})\")\n",
    "\n",
    "# Extract conditional volatility\n",
    "garch_vol = np.sqrt(garch_results.conditional_variance)\n",
    "garch_vol_annual = garch_vol * np.sqrt(252)  # Annualized\n",
    "\n",
    "# Create DataFrame with GARCH volatility\n",
    "garch_vol_df = pd.DataFrame({\n",
    "    'GARCH_Vol': garch_vol,\n",
    "    'GARCH_Vol_Annual': garch_vol_annual\n",
    "}, index=market_returns.index)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 3: Combine GARCH and realized volatility\n",
    "# We'll merge the two datasets and compare the volatility estimates\n",
    "\n",
    "# Convert realized volatility index to datetime\n",
    "realized_vol_df.index = pd.to_datetime(realized_vol_df.index)\n",
    "\n",
    "# Merge GARCH and realized volatility\n",
    "# We'll use the date from realized_vol_df as the index\n",
    "combined_vol = pd.merge(\n",
    "    realized_vol_df,\n",
    "    garch_vol_df,\n",
    "    left_index=True,\n",
    "    right_index=True,\n",
    "    how='inner'\n",
    ")\n",
    "\n",
    "# Display the combined dataset\n",
    "print(f\"Combined dataset has {len(combined_vol)} observations\")\n",
    "combined_vol.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot comparison of GARCH and realized volatility\n",
    "plt.figure(figsize=(14, 7))\n",
    "\n",
    "# Plot GARCH volatility\n",
    "plt.plot(combined_vol.index, combined_vol['GARCH_Vol_Annual'], \n",
    "         color='blue', label='GARCH Volatility')\n",
    "\n",
    "# Plot realized volatility\n",
    "plt.plot(combined_vol.index, combined_vol['RV_Sqrt_Annualized'], \n",
    "         color='red', label='Realized Volatility')\n",
    "\n",
    "# Plot bipower variation\n",
    "plt.plot(combined_vol.index, combined_vol['BPV_Sqrt'] * np.sqrt(252), \n",
    "         color='green', label='Bipower Variation')\n",
    "\n",
    "plt.title('Comparison of Volatility Measures (Annualized %)')\n",
    "plt.ylabel('Volatility (%)')\n",
    "plt.xlabel('Date')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Calculate correlation between volatility measures\n",
    "vol_corr = combined_vol[['RV_Sqrt_Annualized', 'BPV_Sqrt', 'GARCH_Vol_Annual']].corr()\n",
    "vol_corr.columns = ['Realized Vol', 'Bipower Var', 'GARCH Vol']\n",
    "vol_corr.index = ['Realized Vol', 'Bipower Var', 'GARCH Vol']\n",
    "\n",
    "print(\"\nCorrelation between Volatility Measures:\")\n",
    "vol_corr\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 4: Develop a hybrid volatility model\n",
    "# We'll create a model that combines GARCH and realized volatility\n",
    "\n",
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "# Prepare data for regression\n",
    "# We'll use lagged values of GARCH and realized volatility to predict future realized volatility\n",
    "combined_vol['RV_Sqrt_Lag1'] = combined_vol['RV_Sqrt'].shift(1)\n",
    "combined_vol['BPV_Sqrt_Lag1'] = combined_vol['BPV_Sqrt'].shift(1)\n",
    "combined_vol['GARCH_Vol_Lag1'] = combined_vol['GARCH_Vol'].shift(1)\n",
    "\n",
    "# Drop missing values\n",
    "regression_data = combined_vol.dropna()\n",
    "\n",
    "# Define features and target\n",
    "X = regression_data[['RV_Sqrt_Lag1', 'BPV_Sqrt_Lag1', 'GARCH_Vol_Lag1']]\n",
    "y = regression_data['RV_Sqrt']\n",
    "\n",
    "# Split data into training and testing sets\n",
    "train_size = int(len(regression_data) * 0.8)\n",
    "X_train, X_test = X.iloc[:train_size], X.iloc[train_size:]\n",
    "y_train, y_test = y.iloc[:train_size], y.iloc[train_size:]\n",
    "\n",
    "# Fit linear regression model\n",
    "model = LinearRegression()\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions\n",
    "y_pred_train = model.predict(X_train)\n",
    "y_pred_test = model.predict(X_test)\n",
    "\n",
    "# Calculate R-squared\n",
    "r2_train = model.score(X_train, y_train)\n",
    "r2_test = model.score(X_test, y_test)\n",
    "\n",
    "print(\"Hybrid Volatility Model Results:\")\n",
    "print(f\"R-squared (Training): {r2_train:.4f}\")\n",
    "print(f\"R-squared (Testing): {r2_test:.4f}\")\n",
    "print(\"\nCoefficients:\")\n",
    "for feature, coef in zip(X.columns, model.coef_):\n",
    "    print(f\"{feature}: {coef:.6f}\")\n",
    "print(f\"Intercept: {model.intercept_:.6f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add predictions to the dataset\n",
    "regression_data['Hybrid_Vol'] = model.predict(X)\n",
    "regression_data['Hybrid_Vol_Annual'] = regression_data['Hybrid_Vol'] * np.sqrt(252)\n",
    "\n",
    "# Plot actual vs. predicted volatility\n",
    "plt.figure(figsize=(14, 7))\n",
    "\n",
    "# Plot actual realized volatility\n",
    "plt.plot(regression_data.index, regression_data['RV_Sqrt'] * np.sqrt(252), \n",
    "         color='blue', label='Actual Realized Volatility')\n",
    "\n",
    "# Plot predicted volatility\n",
    "plt.plot(regression_data.index, regression_data['Hybrid_Vol_Annual'], \n",
    "         color='red', label='Hybrid Model Prediction')\n",
    "\n",
    "# Add vertical line to separate training and testing sets\n",
    "split_date = regression_data.index[train_size]\n",
    "plt.axvline(x=split_date, color='black', linestyle='--', label='Train/Test Split')\n",
    "\n",
    "plt.title('Actual vs. Predicted Volatility (Annualized %)')\n",
    "plt.ylabel('Volatility (%)')\n",
    "plt.xlabel('Date')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 5: Evaluate forecasting performance\n",
    "# We'll compare the forecasting performance of different volatility models\n",
    "\n",
    "# Calculate forecast errors\n",
    "regression_data['GARCH_Error'] = regression_data['RV_Sqrt'] - regression_data['GARCH_Vol_Lag1']\n",
    "regression_data['RV_Error'] = regression_data['RV_Sqrt'] - regression_data['RV_Sqrt_Lag1']\n",
    "regression_data['BPV_Error'] = regression_data['RV_Sqrt'] - regression_data['BPV_Sqrt_Lag1']\n",
    "regression_data['Hybrid_Error'] = regression_data['RV_Sqrt'] - regression_data['Hybrid_Vol']\n",
    "\n",
    "# Calculate error metrics\n",
    "def calculate_error_metrics(errors):\n",
    "    \"\"\"\n",
    "    Calculate error metrics for volatility forecasts.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    errors : pd.Series\n",
    "        Series of forecast errors\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    dict\n",
    "        Dictionary of error metrics\n",
    "    \"\"\"\n",
    "    return {\n",
    "        'MAE': np.mean(np.abs(errors)),\n",
    "        'RMSE': np.sqrt(np.mean(errors**2)),\n",
    "        'MAPE': np.mean(np.abs(errors / regression_data['RV_Sqrt'])) * 100\n",
    "    }\n",
    "\n",
    "# Calculate error metrics for each model\n",
    "error_metrics = {}\n",
    "for model_name in ['GARCH', 'RV', 'BPV', 'Hybrid']:\n",
    "    # Calculate metrics for training set\n",
    "    train_errors = regression_data[f'{model_name}_Error'].iloc[:train_size]\n",
    "    error_metrics[f'{model_name}_Train'] = calculate_error_metrics(train_errors)\n",
    "    \n",
    "    # Calculate metrics for testing set\n",
    "    test_errors = regression_data[f'{model_name}_Error'].iloc[train_size:]\n",
    "    error_metrics[f'{model_name}_Test'] = calculate_error_metrics(test_errors)\n",
    "\n",
    "# Create summary DataFrame\n",
    "error_summary = []\n",
    "for model_name in ['GARCH', 'RV', 'BPV', 'Hybrid']:\n",
    "    for dataset in ['Train', 'Test']:\n",
    "        metrics = error_metrics[f'{model_name}_{dataset}']\n",
    "        error_summary.append({\n",
    "            'Model': model_name,\n",
    "            'Dataset': dataset,\n",
    "            'MAE': metrics['MAE'],\n",
    "            'RMSE': metrics['RMSE'],\n",
    "            'MAPE': metrics['MAPE']\n",
    "        })\n",
    "\n",
    "error_summary_df = pd.DataFrame(error_summary)\n",
    "\n",
    "print(\"Forecast Error Metrics:\")\n",
    "error_summary_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize error metrics\n",
    "fig, axes = plt.subplots(3, 1, figsize=(14, 12))\n",
    "\n",
    "# Filter for test set only\n",
    "test_errors = error_summary_df[error_summary_df['Dataset'] == 'Test']\n",
    "\n",
    "# Plot MAE\n",
    "axes[0].bar(test_errors['Model'], test_errors['MAE'])\n",
    "axes[0].set_title('Mean Absolute Error (MAE)')\n",
    "axes[0].set_ylabel('MAE')\n",
    "axes[0].grid(axis='y')\n",
    "\n",
    "# Plot RMSE\n",
    "axes[1].bar(test_errors['Model'], test_errors['RMSE'])\n",
    "axes[1].set_title('Root Mean Squared Error (RMSE)')\n",
    "axes[1].set_ylabel('RMSE')\n",
    "axes[1].grid(axis='y')\n",
    "\n",
    "# Plot MAPE\n",
    "axes[2].bar(test_errors['Model'], test_errors['MAPE'])\n",
    "axes[2].set_title('Mean Absolute Percentage Error (MAPE)')\n",
    "axes[2].set_ylabel('MAPE (%)')\n",
    "axes[2].grid(axis='y')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 6: Apply the hybrid model to options pricing\n",
    "# We'll use the Black-Scholes formula to price options using different volatility estimates\n",
    "\n",
    "from scipy.stats import norm\n",
    "\n",
    "def black_scholes(S, K, T, r, sigma, option_type='call'):\n",
    "    \"\"\"\n",
    "    Calculate Black-Scholes option price.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    S : float\n",
    "        Current stock price\n",
    "    K : float\n",
    "        Strike price\n",
    "    T : float\n",
    "        Time to maturity (in years)\n",
    "    r : float\n",
    "        Risk-free interest rate (annual)\n",
    "    sigma : float\n",
    "        Volatility (annual)\n",
    "    option_type : str\n",
    "        'call' or 'put'\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    float\n",
    "        Option price\n",
    "    \"\"\"\n",
    "    d1 = (np.log(S / K) + (r + 0.5 * sigma**2) * T) / (sigma * np.sqrt(T))\n",
    "    d2 = d1 - sigma * np.sqrt(T)\n",
    "    \n",
    "    if option_type == 'call':\n",
    "        price = S * norm.cdf(d1) - K * np.exp(-r * T) * norm.cdf(d2)\n",
    "    else:  # put\n",
    "        price = K * np.exp(-r * T) * norm.cdf(-d2) - S * norm.cdf(-d1)\n",
    "    \n",
    "    return price\n",
    "\n",
    "# Calculate option prices using different volatility estimates\n",
    "# We'll use the last available data point\n",
    "last_data = regression_data.iloc[-1]\n",
    "\n",
    "# Current price (assume it's the last available price)\n",
    "S = market_prices.iloc[-1]\n",
    "\n",
    "# Define option parameters\n",
    "K_values = [0.9 * S, 0.95 * S, S, 1.05 * S, 1.1 * S]  # Strike prices\n",
    "T_values = [1/12, 1/6, 1/4, 1/2, 1]  # Time to maturity (in years)\n",
    "r = 0.03  # Risk-free rate (3%)\n",
    "\n",
    "# Get volatility estimates (convert to decimal)\n",
    "garch_vol = last_data['GARCH_Vol_Annual'] / 100\n",
    "rv_vol = last_data['RV_Sqrt_Annualized'] / 100\n",
    "hybrid_vol = last_data['Hybrid_Vol_Annual'] / 100\n",
    "\n",
    "# Calculate option prices\n",
    "option_prices = []\n",
    "for K in K_values:\n",
    "    for T in T_values:\n",
    "        for option_type in ['call', 'put']:\n",
    "            # Calculate prices using different volatility estimates\n",
    "            garch_price = black_scholes(S, K, T, r, garch_vol, option_type)\n",
    "            rv_price = black_scholes(S, K, T, r, rv_vol, option_type)\n",
    "            hybrid_price = black_scholes(S, K, T, r, hybrid_vol, option_type)\n",
    "            \n",
    "            # Store results\n",
    "            option_prices.append({\n",
    "                'Strike': K,\n",
    "                'Moneyness': K / S,\n",
    "                'Maturity': T,\n",
    "                'Type': option_type.capitalize(),\n",
    "                'GARCH_Price': garch_price,\n",
    "                'RV_Price': rv_price,\n",
    "                'Hybrid_Price': hybrid_price,\n",
    "                'GARCH_RV_Diff': garch_price - rv_price,\n",
    "                'Hybrid_RV_Diff': hybrid_price - rv_price\n",
    "            })\n",
    "\n",
    "# Convert to DataFrame\n",
    "option_prices_df = pd.DataFrame(option_prices)\n",
    "\n",
    "# Display option prices\n",
    "print(\"Option Prices Using Different Volatility Estimates:\")\n",
    "print(f\"Current Price: ${S:.2f}\")\n",
    "print(f\"GARCH Volatility: {garch_vol*100:.2f}%\")\n",
    "print(f\"Realized Volatility: {rv_vol*100:.2f}%\")\n",
    "print(f\"Hybrid Volatility: {hybrid_vol*100:.2f}%\")\n",
    "print(\"\nSample of Option Prices:\")\n",
    "option_prices_df.head(10)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize option prices\n",
    "# We'll focus on at-the-money options with different maturities\n",
    "atm_options = option_prices_df[option_prices_df['Moneyness'].between(0.99, 1.01)]\n",
    "\n",
    "# Plot option prices by maturity\n",
    "fig, axes = plt.subplots(2, 1, figsize=(14, 10))\n",
    "\n",
    "# Filter for calls and puts\n",
    "calls = atm_options[atm_options['Type'] == 'Call']\n",
    "puts = atm_options[atm_options['Type'] == 'Put']\n",
    "\n",
    "# Plot call prices\n",
    "axes[0].plot(calls['Maturity'], calls['GARCH_Price'], 'o-', label='GARCH')\n",
    "axes[0].plot(calls['Maturity'], calls['RV_Price'], 's-', label='Realized Volatility')\n",
    "axes[0].plot(calls['Maturity'], calls['Hybrid_Price'], '^-', label='Hybrid Model')\n",
    "axes[0].set_title('At-the-Money Call Option Prices by Maturity')\n",
    "axes[0].set_xlabel('Time to Maturity (Years)')\n",
    "axes[0].set_ylabel('Option Price ($)')\n",
    "axes[0].grid(True)\n",
    "axes[0].legend()\n",
    "\n",
    "# Plot put prices\n",
    "axes[1].plot(puts['Maturity'], puts['GARCH_Price'], 'o-', label='GARCH')\n",
    "axes[1].plot(puts['Maturity'], puts['RV_Price'], 's-', label='Realized Volatility')\n",
    "axes[1].plot(puts['Maturity'], puts['Hybrid_Price'], '^-', label='Hybrid Model')\n",
    "axes[1].set_title('At-the-Money Put Option Prices by Maturity')\n",
    "axes[1].set_xlabel('Time to Maturity (Years)')\n",
    "axes[1].set_ylabel('Option Price ($)')\n",
    "axes[1].grid(True)\n",
    "axes[1].legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Plot option prices by moneyness for a specific maturity\n",
    "maturity = 0.25  # 3 months\n",
    "maturity_options = option_prices_df[option_prices_df['Maturity'] == maturity]\n",
    "\n",
    "fig, axes = plt.subplots(2, 1, figsize=(14, 10))\n",
    "\n",
    "# Filter for calls and puts\n",
    "calls = maturity_options[maturity_options['Type'] == 'Call']\n",
    "puts = maturity_options[maturity_options['Type'] == 'Put']\n",
    "\n",
    "# Plot call prices\n",
    "axes[0].plot(calls['Moneyness'], calls['GARCH_Price'], 'o-', label='GARCH')\n",
    "axes[0].plot(calls['Moneyness'], calls['RV_Price'], 's-', label='Realized Volatility')\n",
    "axes[0].plot(calls['Moneyness'], calls['Hybrid_Price'], '^-', label='Hybrid Model')\n",
    "axes[0].set_title(f'Call Option Prices by Moneyness (Maturity = {maturity} years)')\n",
    "axes[0].set_xlabel('Moneyness (Strike/Spot)')\n",
    "axes[0].set_ylabel('Option Price ($)')\n",
    "axes[0].grid(True)\n",
    "axes[0].legend()\n",
    "\n",
    "# Plot put prices\n",
    "axes[1].plot(puts['Moneyness'], puts['GARCH_Price'], 'o-', label='GARCH')\n",
    "axes[1].plot(puts['Moneyness'], puts['RV_Price'], 's-', label='Realized Volatility')\n",
    "axes[1].plot(puts['Moneyness'], puts['Hybrid_Price'], '^-', label='Hybrid Model')\n",
    "axes[1].set_title(f'Put Option Prices by Moneyness (Maturity = {maturity} years)')\n",
    "axes[1].set_xlabel('Moneyness (Strike/Spot)')\n",
    "axes[1].set_ylabel('Option Price ($)')\n",
    "axes[1].grid(True)\n",
    "axes[1].legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 7: Analyze the economic value of improved volatility forecasts\n",
    "# We'll simulate a simple options trading strategy based on volatility forecasts\n",
    "\n",
    "def simulate_options_strategy(price_series, vol_forecasts, actual_vol, \n",
    "                             initial_capital=10000, trade_size=1000):\n",
    "    \"\"\"\n",
    "    Simulate a simple options trading strategy based on volatility forecasts.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    price_series : pd.Series\n",
    "        Series of asset prices\n",
    "    vol_forecasts : dict\n",
    "        Dictionary of volatility forecasts from different models\n",
    "    actual_vol : pd.Series\n",
    "        Series of realized volatility (actual)\n",
    "    initial_capital : float\n",
    "        Initial capital\n",
    "    trade_size : float\n",
    "        Size of each trade in dollars\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    pd.DataFrame\n",
    "        DataFrame with strategy performance\n",
    "    \"\"\"\n",
    "    # Initialize results\n",
    "    results = []\n",
    "    \n",
    "    # Get common dates\n",
    "    common_dates = sorted(set(vol_forecasts[list(vol_forecasts.keys())[0]].index) & \n",
    "                         set(actual_vol.index))\n",
    "    \n",
    "    # Initialize portfolio values\n",
    "    portfolio_values = {model: initial_capital for model in vol_forecasts.keys()}\n",
    "    \n",
    "    # Simulate trading strategy\n",
    "    for i, date in enumerate(common_dates[:-1]):\n",
    "        next_date = common_dates[i + 1]\n",
    "        \n",
    "        # Get current price\n",
    "        current_price = price_series.loc[date]\n",
    "        \n",
    "        # Get forecasted volatilities\n",
    "        forecasts = {model: vol_series.loc[date] for model, vol_series in vol_forecasts.items()}\n",
    "        \n",
    "        # Get actual volatility (next day)\n",
    "        next_vol = actual_vol.loc[next_date]\n",
    "        \n",
    "        # Simulate trading for each model\n",
    "        for model, forecast in forecasts.items():\n",
    "            # Convert to decimal\n",
    "            forecast_vol = forecast / 100\n",
    "            actual_vol_val = next_vol / 100\n",
    "            \n",
    "            # Calculate option prices\n",
    "            # We'll use at-the-money options with 1-month maturity\n",
    "            K = current_price\n",
    "            T = 1/12  # 1 month\n",
    "            r = 0.03  # 3% risk-free rate\n",
    "            \n",
    "            # Calculate option prices using forecasted volatility\n",
    "            call_price_forecast = black_scholes(current_price, K, T, r, forecast_vol, 'call')\n",
    "            put_price_forecast = black_scholes(current_price, K, T, r, forecast_vol, 'put')\n",
    "            \n",
    "            # Calculate option prices using actual volatility\n",
    "            call_price_actual = black_scholes(current_price, K, T, r, actual_vol_val, 'call')\n",
    "            put_price_actual = black_scholes(current_price, K, T, r, actual_vol_val, 'put')\n",
    "            \n",
    "            # Trading strategy:\n",
    "            # If forecasted vol > actual vol: options are overpriced, sell options\n",
    "            # If forecasted vol < actual vol: options are underpriced, buy options\n",
    "            \n",
    "            # Calculate profit/loss\n",
    "            if forecast_vol > actual_vol_val:  # Sell options\n",
    "                # Sell straddle (both call and put)\n",
    "                premium_received = call_price_forecast + put_price_forecast\n",
    "                cost_to_close = call_price_actual + put_price_actual\n",
    "                pnl = (premium_received - cost_to_close) * (trade_size / premium_received)\n",
    "                trade_type = 'Sell Straddle'\n",
    "            else:  # Buy options\n",
    "                # Buy straddle (both call and put)\n",
    "                premium_paid = call_price_forecast + put_price_forecast\n",
    "                value_at_close = call_price_actual + put_price_actual\n",
    "                pnl = (value_at_close - premium_paid) * (trade_size / premium_paid)\n",
    "                trade_type = 'Buy Straddle'\n",
    "            \n",
    "            # Update portfolio value\n",
    "            portfolio_values[model] += pnl\n",
    "            \n",
    "            # Store results\n",
    "            results.append({\n",
    "                'Date': next_date,\n",
    "                'Model': model,\n",
    "                'Forecast_Vol': forecast_vol * 100,\n",
    "                'Actual_Vol': actual_vol_val * 100,\n",
    "                'Trade_Type': trade_type,\n",
    "                'PnL': pnl,\n",
    "                'Portfolio_Value': portfolio_values[model]\n",
    "            })\n",
    "    \n",
    "    return pd.DataFrame(results)\n",
    "\n",
    "# Prepare volatility forecasts\n",
    "vol_forecasts = {\n",
    "    'GARCH': garch_vol_df['GARCH_Vol_Annual'],\n",
    "    'RV': realized_vol_df['RV_Sqrt_Annualized'],\n",
    "    'Hybrid': regression_data['Hybrid_Vol_Annual']\n",
    "}\n",
    "\n",
    "# Actual volatility (next day's realized volatility)\n",
    "actual_vol = realized_vol_df['RV_Sqrt_Annualized']\n",
    "\n",
    "# Simulate options trading strategy\n",
    "strategy_results = simulate_options_strategy(\n",
    "    market_prices, vol_forecasts, actual_vol, initial_capital=10000, trade_size=1000)\n",
    "\n",
    "# Display strategy results\n",
    "print(\"Options Trading Strategy Results:\")\n",
    "strategy_results.head(10)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze strategy performance\n",
    "# Calculate performance metrics for each model\n",
    "performance_metrics = []\n",
    "for model in vol_forecasts.keys():\n",
    "    model_results = strategy_results[strategy_results['Model'] == model]\n",
    "    \n",
    "    # Calculate metrics\n",
    "    total_pnl = model_results['PnL'].sum()\n",
    "    final_value = model_results['Portfolio_Value'].iloc[-1]\n",
    "    total_return = (final_value / 10000 - 1) * 100\n",
    "    win_rate = (model_results['PnL'] > 0).mean() * 100\n",
    "    avg_win = model_results.loc[model_results['PnL'] > 0, 'PnL'].mean()\n",
    "    avg_loss = model_results.loc[model_results['PnL'] < 0, 'PnL'].mean()\n",
    "    profit_factor = abs(model_results.loc[model_results['PnL'] > 0, 'PnL'].sum() / \n",
    "                       model_results.loc[model_results['PnL'] < 0, 'PnL'].sum())\n",
    "    \n",
    "    # Calculate Sharpe ratio (assuming 252 trading days per year)\n",
    "    daily_returns = model_results['PnL'] / 10000  # Approximate daily returns\n",
    "    sharpe = (daily_returns.mean() * 252) / (daily_returns.std() * np.sqrt(252))\n",
    "    \n",
    "    # Store metrics\n",
    "    performance_metrics.append({\n",
    "        'Model': model,\n",
    "        'Total_PnL': total_pnl,\n",
    "        'Final_Value': final_value,\n",
    "        'Total_Return': total_return,\n",
    "        'Win_Rate': win_rate,\n",
    "        'Avg_Win': avg_win,\n",
    "        'Avg_Loss': avg_loss,\n",
    "        'Profit_Factor': profit_factor,\n",
    "        'Sharpe_Ratio': sharpe\n",
    "    })\n",
    "\n",
    "# Convert to DataFrame\n",
    "performance_df = pd.DataFrame(performance_metrics)\n",
    "\n",
    "print(\"Strategy Performance Metrics:\")\n",
    "performance_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot portfolio equity curves\n",
    "plt.figure(figsize=(14, 7))\n",
    "\n",
    "# Plot equity curves for each model\n",
    "for model in vol_forecasts.keys():\n",
    "    model_results = strategy_results[strategy_results['Model'] == model]\n",
    "    plt.plot(model_results['Date'], model_results['Portfolio_Value'], label=model)\n",
    "\n",
    "plt.title('Options Trading Strategy Performance')\n",
    "plt.xlabel('Date')\n",
    "plt.ylabel('Portfolio Value ($)')\n",
    "plt.grid(True)\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Plot key performance metrics\n",
    "fig, axes = plt.subplots(2, 2, figsize=(14, 10))\n",
    "\n",
    "# Total return\n",
    "axes[0, 0].bar(performance_df['Model'], performance_df['Total_Return'])\n",
    "axes[0, 0].set_title('Total Return (%)')\n",
    "axes[0, 0].set_ylabel('Return (%)')\n",
    "axes[0, 0].grid(axis='y')\n",
    "\n",
    "# Sharpe ratio\n",
    "axes[0, 1].bar(performance_df['Model'], performance_df['Sharpe_Ratio'])\n",
    "axes[0, 1].set_title('Sharpe Ratio')\n",
    "axes[0, 1].set_ylabel('Sharpe Ratio')\n",
    "axes[0, 1].grid(axis='y')\n",
    "\n",
    "# Win rate\n",
    "axes[1, 0].bar(performance_df['Model'], performance_df['Win_Rate'])\n",
    "axes[1, 0].set_title('Win Rate (%)')\n",
    "axes[1, 0].set_ylabel('Win Rate (%)')\n",
    "axes[1, 0].grid(axis='y')\n",
    "\n",
    "# Profit factor\n",
    "axes[1, 1].bar(performance_df['Model'], performance_df['Profit_Factor'])\n",
    "axes[1, 1].set_title('Profit Factor')\n",
    "axes[1, 1].set_ylabel('Profit Factor')\n",
    "axes[1, 1].grid(axis='y')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "In this notebook, we've demonstrated several real-world applications of the MFE Toolbox for solving complex financial econometrics problems. We've shown how to:\n",
    "\n",
    "1. **Implement comprehensive portfolio risk management** by combining multivariate volatility modeling with Value-at-Risk estimation, stress testing, and risk decomposition.\n",
    "\n",
    "2. **Identify and analyze market regimes** using bootstrap methods and time series analysis, including regime transition probabilities and performance characteristics.\n",
    "\n",
    "3. **Improve volatility forecasting for options trading** by combining GARCH models with realized volatility measures, demonstrating the economic value of better forecasts.\n",
    "\n",
    "These case studies showcase the power and flexibility of the MFE Toolbox for financial analysis. By integrating multiple components of the toolbox, we can build sophisticated analytical workflows that provide valuable insights for investment decision-making, risk management, and trading strategies.\n",
    "\n",
    "The Python-based implementation of the MFE Toolbox makes it easy to combine these techniques with other Python libraries and tools, creating a seamless workflow for financial data analysis and modeling."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
