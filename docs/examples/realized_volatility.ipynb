# Realized Volatility Estimation with High-Frequency Data

This notebook demonstrates how to use the MFE Toolbox for high-frequency financial econometrics, focusing on realized volatility estimation from intraday data. We'll cover:

1. Loading and preprocessing high-frequency data
2. Time format conversions and handling irregularly spaced observations
3. Basic realized variance estimation
4. Robust estimators (bipower variation, realized kernel)
5. Noise-robust estimators for microstructure noise
6. Multivariate realized covariance estimation
7. Practical applications and visualization

The MFE Toolbox provides a comprehensive suite of Python-based tools for high-frequency financial econometrics, leveraging NumPy, Pandas, and Numba for efficient computation.


# Import necessary libraries
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
from datetime import datetime, time
import asyncio

# Import MFE Toolbox components
from mfe.models.realized import (
    RealizedVariance, BipowerVariation, RealizedKernel, 
    MultiscaleVariance, TwoScaleVariance, QMLEVariance,
    RealizedSemivariance, RealizedCovariance, MultivariateRealizedKernel,
    price_filter, variance_optimal_sampling
)

# Import time conversion utilities
from mfe.models.realized import (
    seconds2unit, unit2seconds, wall2unit, unit2wall,
    seconds2wall, wall2seconds
)

# Set plotting style
plt.style.use('seaborn-v0_8-whitegrid')
sns.set_context("notebook", font_scale=1.2)

# Set random seed for reproducibility
np.random.seed(42)


## 1. High-Frequency Data Handling

High-frequency financial data typically consists of irregularly spaced observations with timestamps. We'll start by generating synthetic high-frequency data and demonstrating how to handle it using Pandas.

def generate_hf_data(n_days=1, n_intraday=1000, volatility=0.001, mean_interval=30):
    """
    Generate synthetic high-frequency data with irregularly spaced timestamps.
    
    Parameters:
    -----------
    n_days : int
        Number of trading days
    n_intraday : int
        Approximate number of observations per day
    volatility : float
        Volatility parameter for returns
    mean_interval : float
        Mean time between observations in seconds
        
    Returns:
    --------
    pd.DataFrame with timestamp index and price column
    """
    # Generate random intervals between observations (in seconds)
    intervals = np.random.exponential(scale=mean_interval, size=n_days * n_intraday)
    
    # Create timestamps
    timestamps = []
    current_time = pd.Timestamp('2023-01-01 09:30:00')  # Market open
    
    for day in range(n_days):
        # Set to market open for this day
        day_open = current_time.replace(day=day+1, hour=9, minute=30, second=0)
        current_time = day_open
        
        # Generate observations for this day
        day_intervals = intervals[day * n_intraday:(day + 1) * n_intraday]
        
        for interval in day_intervals:
            # Add timestamp
            timestamps.append(current_time)
            
            # Move to next observation time
            current_time += pd.Timedelta(seconds=interval)
            
            # If we've passed market close (4:00 PM), move to next day
            if current_time.time() > time(16, 0):
                break
    
    # Generate log price process (random walk)
    n_obs = len(timestamps)
    returns = np.random.normal(0, volatility, n_obs)
    log_prices = np.cumsum(returns)
    prices = 100 * np.exp(log_prices)  # Start at price 100
    
    # Create DataFrame
    df = pd.DataFrame({
        'price': prices
    }, index=timestamps)
    
    return df

# Generate 5 days of high-frequency data
hf_data = generate_hf_data(n_days=5, n_intraday=1000, volatility=0.001)

# Display basic information
print(f"Dataset spans from {hf_data.index.min()} to {hf_data.index.max()}")
print(f"Total observations: {len(hf_data)}")
print(f"Average observations per day: {len(hf_data)/5:.1f}")

# Display the first few rows
hf_data.head()


# Plot the high-frequency price data
plt.figure(figsize=(12, 6))
plt.plot(hf_data.index, hf_data['price'])
plt.title('High-Frequency Price Data')
plt.xlabel('Time')
plt.ylabel('Price')
plt.xticks(rotation=45)
plt.tight_layout()
plt.show()


### 1.1 Handling Irregularly Spaced Data

High-frequency data is typically irregularly spaced. We'll demonstrate how to convert it to regular intervals using Pandas resampling functionality.

# Resample to regular intervals (e.g., 5-minute bars)
regular_data = hf_data.resample('5min').last()

# Fill missing values using forward fill
regular_data = regular_data.fillna(method='ffill')

# Plot both irregular and regular data for the first day
day1_data = hf_data[hf_data.index.date == pd.Timestamp('2023-01-01').date()]
day1_regular = regular_data[regular_data.index.date == pd.Timestamp('2023-01-01').date()]

plt.figure(figsize=(12, 8))

plt.subplot(2, 1, 1)
plt.plot(day1_data.index, day1_data['price'], 'o-', markersize=2)
plt.title('Irregularly Spaced High-Frequency Data (Day 1)')
plt.ylabel('Price')

plt.subplot(2, 1, 2)
plt.plot(day1_regular.index, day1_regular['price'], 'o-', markersize=3)
plt.title('Regularly Sampled Data (5-minute intervals)')
plt.ylabel('Price')

plt.tight_layout()
plt.show()

# Compare number of observations
print(f"Irregular data points (Day 1): {len(day1_data)}")
print(f"Regular 5-min data points (Day 1): {len(day1_regular)}")


### 1.2 Time Format Conversions

The MFE Toolbox provides utilities for converting between different time formats, which is useful for handling intraday patterns and standardizing time across different markets.

# Create a trading day timeline
date_range = pd.date_range(
    start='2023-01-01 09:30:00',  # Market open
    end='2023-01-01 16:00:00',    # Market close
    freq='30min'                  # 30-minute intervals
)

# Demonstrate time conversions
results = []
for t in date_range:
    wall_time = t.time()  # Wall clock time
    
    # Convert wall clock time to unit time (normalized between 0 and 1)
    # Unit time represents the fraction of the trading day
    unit_time = wall2unit(wall_time)
    
    # Convert wall clock time to seconds since midnight
    seconds = wall2seconds(wall_time)
    
    # Convert back to wall clock time for verification
    wall_time_check = seconds2wall(seconds)
    
    # Convert unit time to wall clock time
    wall_from_unit = unit2wall(unit_time)
    
    results.append({
        'timestamp': t,
        'wall_time': wall_time,
        'unit_time': unit_time,
        'seconds': seconds,
        'wall_from_unit': wall_from_unit
    })

# Create DataFrame with results
time_conversions = pd.DataFrame(results)

# Display conversions
time_conversions[['timestamp', 'wall_time', 'unit_time', 'seconds', 'wall_from_unit']]


# Visualize unit time conversion
plt.figure(figsize=(10, 6))
plt.plot(time_conversions['wall_time'].apply(lambda x: x.strftime('%H:%M')), 
         time_conversions['unit_time'], 'o-')
plt.title('Wall Clock Time to Unit Time Conversion')
plt.xlabel('Wall Clock Time')
plt.ylabel('Unit Time (0-1)')
plt.grid(True)
plt.xticks(rotation=45)
plt.tight_layout()
plt.show()


### 1.3 Price Filtering and Cleaning

High-frequency data often contains errors and outliers. The MFE Toolbox provides functions for filtering and cleaning price data.

# Generate sample data with outliers
np.random.seed(42)
n_obs = 1000

# Create timestamps
timestamps = pd.date_range(
    start='2023-01-01 09:30:00',
    periods=n_obs,
    freq='30s'
)

# Generate random walk with occasional outliers
log_prices = np.cumsum(np.random.normal(0, 0.001, n_obs))

# Add outliers (approximately 1% of observations)
outlier_idx = np.random.choice(n_obs, size=int(n_obs * 0.01), replace=False)
for idx in outlier_idx:
    log_prices[idx] += np.random.choice([-1, 1]) * np.random.uniform(0.05, 0.1)

prices = 100 * np.exp(log_prices)

# Create DataFrame
data_with_outliers = pd.DataFrame({
    'price': prices
}, index=timestamps)

# Apply price filter
filtered_prices = price_filter(
    prices=data_with_outliers['price'].values,
    timestamps=data_with_outliers.index.values,
    k=3.0  # Filter threshold (3 standard deviations)
)

# Create DataFrame with filtered prices
filtered_data = pd.DataFrame({
    'price': filtered_prices
}, index=timestamps)

# Plot original and filtered prices
plt.figure(figsize=(12, 8))

plt.subplot(2, 1, 1)
plt.plot(data_with_outliers.index, data_with_outliers['price'])
plt.title('Original High-Frequency Price Data with Outliers')
plt.ylabel('Price')

plt.subplot(2, 1, 2)
plt.plot(filtered_data.index, filtered_data['price'])
plt.title('Filtered High-Frequency Price Data')
plt.ylabel('Price')

plt.tight_layout()
plt.show()

# Identify outliers
outliers = data_with_outliers[data_with_outliers['price'] != filtered_data['price']]
print(f"Number of outliers detected: {len(outliers)}")
print(f"Percentage of outliers: {len(outliers) / len(data_with_outliers) * 100:.2f}%")


## 2. Realized Volatility Estimators

Now we'll explore various realized volatility estimators provided by the MFE Toolbox. We'll start with the basic realized variance estimator and then move to more sophisticated estimators.

### 2.1 Generating Multi-Day Data with Known Volatility

First, let's generate multi-day data with known volatility patterns to evaluate the estimators.

def generate_multiday_data(n_days=5, n_intraday=100, volatility_pattern=None):
    """
    Generate multi-day high-frequency data with specified volatility pattern.
    
    Parameters:
    -----------
    n_days : int
        Number of trading days
    n_intraday : int
        Number of observations per day
    volatility_pattern : list or None
        List of daily volatilities. If None, a default pattern is used.
        
    Returns:
    --------
    tuple: (DataFrame with high-frequency data, array of true daily integrated variance)
    """
    # Default volatility pattern if not provided
    if volatility_pattern is None:
        # Day 1: normal, Day 2: high, Day 3: normal, Day 4: low, Day 5: high
        volatility_pattern = [0.015, 0.025, 0.015, 0.010, 0.030]
    
    # Ensure we have enough volatility values
    if len(volatility_pattern) < n_days:
        volatility_pattern = volatility_pattern * (n_days // len(volatility_pattern) + 1)
        volatility_pattern = volatility_pattern[:n_days]
    
    # Create timestamps
    timestamps = []
    for day in range(n_days):
        day_date = pd.Timestamp(f'2023-01-{day+1:02d}')
        for i in range(n_intraday):
            # 9:30 AM to 4:00 PM (390 minutes = 6.5 hours)
            minute = 9*60 + 30 + i * (6.5*60 / n_intraday)
            hour = int(minute // 60)
            minute = int(minute % 60)
            timestamps.append(day_date + pd.Timedelta(hours=hour, minutes=minute))
    
    # Generate returns with specified volatility pattern
    returns = np.zeros(n_days * n_intraday)
    true_iv = np.zeros(n_days)  # True integrated variance
    
    for day in range(n_days):
        # Daily volatility
        daily_vol = volatility_pattern[day]
        
        # Intraday volatility (scaled by sqrt of number of observations)
        intraday_vol = daily_vol / np.sqrt(n_intraday)
        
        # Generate returns for this day
        start_idx = day * n_intraday
        end_idx = (day + 1) * n_intraday
        day_returns = np.random.normal(0, intraday_vol, n_intraday)
        returns[start_idx:end_idx] = day_returns
        
        # Calculate true integrated variance (sum of squared returns)
        true_iv[day] = np.sum(day_returns**2)
    
    # Convert to prices
    log_prices = np.cumsum(returns)
    prices = 100 * np.exp(log_prices)  # Start at price 100
    
    # Create DataFrame
    df = pd.DataFrame({
        'price': prices
    }, index=timestamps)
    
    return df, true_iv

# Generate 5 days of data with varying volatility
multiday_data, true_iv = generate_multiday_data(n_days=5, n_intraday=390)  # 1-minute data

# Plot the data
plt.figure(figsize=(12, 6))
plt.plot(multiday_data.index, multiday_data['price'])
plt.title('Multi-Day High-Frequency Price Data with Varying Volatility')
plt.xlabel('Time')
plt.ylabel('Price')
plt.xticks(rotation=45)
plt.tight_layout()
plt.show()

# Print true daily integrated variance
print("True Daily Integrated Variance:")
for day in range(5):
    print(f"Day {day+1}: {true_iv[day]:.6f}")


### 2.2 Basic Realized Variance

The simplest realized volatility estimator is the realized variance, which is the sum of squared intraday returns.

# Create a realized variance estimator
rv_estimator = RealizedVariance()

# Estimate daily realized variance at different sampling frequencies
rv_1min = rv_estimator.compute(
    prices=multiday_data['price'].values,
    timestamps=multiday_data.index.values,
    sampling='1min'  # 1-minute sampling
)

rv_5min = rv_estimator.compute(
    prices=multiday_data['price'].values,
    timestamps=multiday_data.index.values,
    sampling='5min'  # 5-minute sampling
)

rv_10min = rv_estimator.compute(
    prices=multiday_data['price'].values,
    timestamps=multiday_data.index.values,
    sampling='10min'  # 10-minute sampling
)

# Convert to annualized volatility (standard deviation)
# Assuming 252 trading days per year
annualized_vol_true = np.sqrt(true_iv * 252) * 100  # In percentage
annualized_vol_1min = np.sqrt(rv_1min * 252) * 100
annualized_vol_5min = np.sqrt(rv_5min * 252) * 100
annualized_vol_10min = np.sqrt(rv_10min * 252) * 100

# Print results
print("Daily Realized Variance and Annualized Volatility:")
for day in range(5):
    print(f"Day {day+1}:")
    print(f"  True IV = {true_iv[day]:.6f}, Annualized Vol = {annualized_vol_true[day]:.2f}%")
    print(f"  RV (1min) = {rv_1min[day]:.6f}, Annualized Vol = {annualized_vol_1min[day]:.2f}%")
    print(f"  RV (5min) = {rv_5min[day]:.6f}, Annualized Vol = {annualized_vol_5min[day]:.2f}%")
    print(f"  RV (10min) = {rv_10min[day]:.6f}, Annualized Vol = {annualized_vol_10min[day]:.2f}%")


# Plot realized volatility at different sampling frequencies
plt.figure(figsize=(12, 6))

x = np.arange(1, 6)
width = 0.2

plt.bar(x - 1.5*width, annualized_vol_true, width, label='True')
plt.bar(x - 0.5*width, annualized_vol_1min, width, label='RV (1min)')
plt.bar(x + 0.5*width, annualized_vol_5min, width, label='RV (5min)')
plt.bar(x + 1.5*width, annualized_vol_10min, width, label='RV (10min)')

plt.title('Realized Volatility at Different Sampling Frequencies')
plt.xlabel('Day')
plt.ylabel('Annualized Volatility (%)')
plt.xticks(x)
plt.legend()
plt.grid(True, alpha=0.3)
plt.tight_layout()
plt.show()


### 2.3 Bipower Variation

Bipower variation is robust to jumps in the price process. Let's generate data with jumps and compare realized variance with bipower variation.

def generate_data_with_jumps(n_days=5, n_intraday=390, base_vol=0.015, jump_days=None, jump_sizes=None):
    """
    Generate multi-day high-frequency data with jumps on specified days.
    
    Parameters:
    -----------
    n_days : int
        Number of trading days
    n_intraday : int
        Number of observations per day
    base_vol : float
        Base volatility level
    jump_days : list or None
        List of days (1-indexed) with jumps. If None, defaults to [2, 4]
    jump_sizes : list or None
        List of jump sizes. If None, defaults to [0.02, -0.015]
        
    Returns:
    --------
    tuple: (DataFrame with high-frequency data, array of true daily integrated variance,
            array of jump contributions to variance)
    """
    # Default jump days and sizes if not provided
    if jump_days is None:
        jump_days = [2, 4]  # Days with jumps (1-indexed)
    if jump_sizes is None:
        jump_sizes = [0.02, -0.015]  # Jump sizes
    
    # Ensure jump_days and jump_sizes have the same length
    assert len(jump_days) == len(jump_sizes), "jump_days and jump_sizes must have the same length"
    
    # Create timestamps
    timestamps = []
    for day in range(n_days):
        day_date = pd.Timestamp(f'2023-01-{day+1:02d}')
        for i in range(n_intraday):
            # 9:30 AM to 4:00 PM (390 minutes = 6.5 hours)
            minute = 9*60 + 30 + i * (6.5*60 / n_intraday)
            hour = int(minute // 60)
            minute = int(minute % 60)
            timestamps.append(day_date + pd.Timedelta(hours=hour, minutes=minute))
    
    # Generate continuous returns
    intraday_vol = base_vol / np.sqrt(n_intraday)
    continuous_returns = np.random.normal(0, intraday_vol, n_days * n_intraday)
    
    # Copy continuous returns to create returns with jumps
    jump_returns = continuous_returns.copy()
    
    # Add jumps at specific times
    jump_contribution = np.zeros(n_days)  # Jump contribution to variance
    for i, day in enumerate(jump_days):
        # Convert to 0-indexed
        day_idx = day - 1
        if day_idx < n_days:
            # Add jump at middle of the day
            jump_time = day_idx * n_intraday + n_intraday // 2
            jump_returns[jump_time] += jump_sizes[i]
            
            # Record jump contribution to variance
            jump_contribution[day_idx] = jump_sizes[i]**2
    
    # Calculate true integrated variance (without jumps)
    true_iv = np.zeros(n_days)
    for day in range(n_days):
        start_idx = day * n_intraday
        end_idx = (day + 1) * n_intraday
        day_returns = continuous_returns[start_idx:end_idx]
        true_iv[day] = np.sum(day_returns**2)
    
    # Convert to prices
    log_prices = np.cumsum(jump_returns)
    prices = 100 * np.exp(log_prices)  # Start at price 100
    
    # Create DataFrame
    df = pd.DataFrame({
        'price': prices
    }, index=timestamps)
    
    return df, true_iv, jump_contribution

# Generate data with jumps
jump_data, true_cont_iv, jump_contribution = generate_data_with_jumps(
    n_days=5, 
    n_intraday=390,  # 1-minute data
    jump_days=[2, 4], 
    jump_sizes=[0.02, -0.015]
)

# Plot the data
plt.figure(figsize=(12, 6))
plt.plot(jump_data.index, jump_data['price'])

# Mark jump days
for day in [2, 4]:  # 1-indexed days with jumps
    day_idx = day - 1  # Convert to 0-indexed
    jump_time = jump_data.index[day_idx * 390 + 390 // 2]  # Middle of the day
    plt.axvline(x=jump_time, color='r', linestyle='--', 
                label=f'Jump on Day {day}' if day == 2 else None)

plt.title('High-Frequency Price Data with Jumps')
plt.xlabel('Time')
plt.ylabel('Price')
plt.legend()
plt.xticks(rotation=45)
plt.tight_layout()
plt.show()


# Create estimators
rv_estimator = RealizedVariance()
bv_estimator = BipowerVariation()

# Estimate daily realized variance and bipower variation
rv = rv_estimator.compute(
    prices=jump_data['price'].values,
    timestamps=jump_data.index.values,
    sampling='5min'
)

bv = bv_estimator.compute(
    prices=jump_data['price'].values,
    timestamps=jump_data.index.values,
    sampling='5min'
)

# Estimate jump component
jump = np.maximum(0, rv - bv)

# Convert to annualized volatility
annualized_vol_rv = np.sqrt(rv * 252) * 100  # In percentage
annualized_vol_bv = np.sqrt(bv * 252) * 100
annualized_vol_true = np.sqrt(true_cont_iv * 252) * 100

# Print results
print("Comparison of Realized Variance and Bipower Variation:")
for day in range(5):
    print(f"Day {day+1}:")
    print(f"  True Continuous IV = {true_cont_iv[day]:.6f}, Annualized Vol = {annualized_vol_true[day]:.2f}%")
    print(f"  Jump Contribution = {jump_contribution[day]:.6f}")
    print(f"  RV = {rv[day]:.6f}, Annualized Vol = {annualized_vol_rv[day]:.2f}%")
    print(f"  BV = {bv[day]:.6f}, Annualized Vol = {annualized_vol_bv[day]:.2f}%")
    print(f"  Estimated Jump = {jump[day]:.6f}")
    print(f"  Jump Ratio = {jump[day]/rv[day]*100:.2f}%")


# Plot comparison
plt.figure(figsize=(12, 6))

x = np.arange(1, 6)
width = 0.25

plt.bar(x - width, annualized_vol_true, width, label='True Continuous')
plt.bar(x, annualized_vol_rv, width, label='RV')
plt.bar(x + width, annualized_vol_bv, width, label='BV')

plt.title('Comparison of Realized Volatility Estimators with Jumps')
plt.xlabel('Day')
plt.ylabel('Annualized Volatility (%)')
plt.xticks(x)
plt.legend()
plt.grid(True, alpha=0.3)
plt.tight_layout()
plt.show()

# Plot jump component
plt.figure(figsize=(10, 6))
plt.bar(x, jump/rv*100)
plt.title('Jump Component as Percentage of Realized Variance')
plt.xlabel('Day')
plt.ylabel('Jump Component (%)')
plt.xticks(x)
plt.grid(True, alpha=0.3)
plt.tight_layout()
plt.show()


### 2.4 Handling Microstructure Noise

High-frequency data often contains microstructure noise. Let's generate data with noise and compare different noise-robust estimators.

def generate_data_with_noise(n_days=5, n_intraday=390, base_vol=0.015, noise_std=0.001):
    """
    Generate multi-day high-frequency data with microstructure noise.
    
    Parameters:
    -----------
    n_days : int
        Number of trading days
    n_intraday : int
        Number of observations per day
    base_vol : float
        Base volatility level
    noise_std : float
        Standard deviation of microstructure noise
        
    Returns:
    --------
    tuple: (DataFrame with noisy high-frequency data, array of true daily integrated variance)
    """
    # Create timestamps
    timestamps = []
    for day in range(n_days):
        day_date = pd.Timestamp(f'2023-01-{day+1:02d}')
        for i in range(n_intraday):
            # 9:30 AM to 4:00 PM (390 minutes = 6.5 hours)
            minute = 9*60 + 30 + i * (6.5*60 / n_intraday)
            hour = int(minute // 60)
            minute = int(minute % 60)
            timestamps.append(day_date + pd.Timedelta(hours=hour, minutes=minute))
    
    # Generate efficient price process
    intraday_vol = base_vol / np.sqrt(n_intraday)
    efficient_returns = np.random.normal(0, intraday_vol, n_days * n_intraday)
    efficient_log_prices = np.cumsum(efficient_returns)
    
    # Calculate true integrated variance
    true_iv = np.zeros(n_days)
    for day in range(n_days):
        start_idx = day * n_intraday
        end_idx = (day + 1) * n_intraday
        day_returns = efficient_returns[start_idx:end_idx]
        true_iv[day] = np.sum(day_returns**2)
    
    # Add microstructure noise
    noise = np.random.normal(0, noise_std, n_days * n_intraday)
    observed_log_prices = efficient_log_prices + noise
    
    # Convert to prices
    efficient_prices = np.exp(efficient_log_prices)
    observed_prices = np.exp(observed_log_prices)
    
    # Create DataFrames
    efficient_data = pd.DataFrame({
        'price': efficient_prices
    }, index=timestamps)
    
    observed_data = pd.DataFrame({
        'price': observed_prices
    }, index=timestamps)
    
    return observed_data, efficient_data, true_iv

# Generate data with microstructure noise
noisy_data, efficient_data, true_iv = generate_data_with_noise(
    n_days=5, 
    n_intraday=390,  # 1-minute data
    base_vol=0.015,
    noise_std=0.001
)

# Plot a small section of the data to see the noise
# Take the first 100 observations of day 1
start_idx = 0
end_idx = 100
plot_timestamps = noisy_data.index[start_idx:end_idx]

plt.figure(figsize=(12, 6))
plt.plot(plot_timestamps, efficient_data.iloc[start_idx:end_idx]['price'], 'b-', label='Efficient Price')
plt.plot(plot_timestamps, noisy_data.iloc[start_idx:end_idx]['price'], 'r-', alpha=0.7, label='Observed Price')
plt.title('Efficient vs. Observed Prices with Microstructure Noise')
plt.xlabel('Time')
plt.ylabel('Price')
plt.legend()
plt.xticks(rotation=45)
plt.tight_layout()
plt.show()


# Create estimators
rv_estimator = RealizedVariance()
rk_estimator = RealizedKernel(kernel_type='parzen')
msrv_estimator = MultiscaleVariance()
tsrv_estimator = TwoScaleVariance()
qmle_estimator = QMLEVariance()

# Estimate volatility using different estimators and sampling frequencies
rv_1min = rv_estimator.compute(
    prices=noisy_data['price'].values,
    timestamps=noisy_data.index.values,
    sampling='1min'
)

rv_5min = rv_estimator.compute(
    prices=noisy_data['price'].values,
    timestamps=noisy_data.index.values,
    sampling='5min'
)

rv_10min = rv_estimator.compute(
    prices=noisy_data['price'].values,
    timestamps=noisy_data.index.values,
    sampling='10min'
)

# Noise-robust estimators
rk = rk_estimator.compute(
    prices=noisy_data['price'].values,
    timestamps=noisy_data.index.values
)

msrv = msrv_estimator.compute(
    prices=noisy_data['price'].values,
    timestamps=noisy_data.index.values
)

tsrv = tsrv_estimator.compute(
    prices=noisy_data['price'].values,
    timestamps=noisy_data.index.values
)

qmle = qmle_estimator.compute(
    prices=noisy_data['price'].values,
    timestamps=noisy_data.index.values
)

# Convert to annualized volatility
annualized_vol_true = np.sqrt(true_iv * 252) * 100  # In percentage
annualized_vol_1min = np.sqrt(rv_1min * 252) * 100
annualized_vol_5min = np.sqrt(rv_5min * 252) * 100
annualized_vol_10min = np.sqrt(rv_10min * 252) * 100
annualized_vol_rk = np.sqrt(rk * 252) * 100
annualized_vol_msrv = np.sqrt(msrv * 252) * 100
annualized_vol_tsrv = np.sqrt(tsrv * 252) * 100
annualized_vol_qmle = np.sqrt(qmle * 252) * 100

# Calculate mean absolute percentage error (MAPE)
mape_1min = np.mean(np.abs(rv_1min/true_iv - 1)) * 100
mape_5min = np.mean(np.abs(rv_5min/true_iv - 1)) * 100
mape_10min = np.mean(np.abs(rv_10min/true_iv - 1)) * 100
mape_rk = np.mean(np.abs(rk/true_iv - 1)) * 100
mape_msrv = np.mean(np.abs(msrv/true_iv - 1)) * 100
mape_tsrv = np.mean(np.abs(tsrv/true_iv - 1)) * 100
mape_qmle = np.mean(np.abs(qmle/true_iv - 1)) * 100

# Print results
print("Comparison of Volatility Estimators with Microstructure Noise:")
print(f"Mean Absolute Percentage Error (MAPE):")
print(f"  RV (1min): {mape_1min:.2f}%")
print(f"  RV (5min): {mape_5min:.2f}%")
print(f"  RV (10min): {mape_10min:.2f}%")
print(f"  Realized Kernel: {mape_rk:.2f}%")
print(f"  Multiscale RV: {mape_msrv:.2f}%")
print(f"  Two-Scale RV: {mape_tsrv:.2f}%")
print(f"  QMLE: {mape_qmle:.2f}%")


# Plot comparison of estimators
plt.figure(figsize=(15, 8))

x = np.arange(1, 6)
width = 0.1

plt.bar(x - 3*width, annualized_vol_true, width, label='True')
plt.bar(x - 2*width, annualized_vol_1min, width, label='RV (1min)')
plt.bar(x - width, annualized_vol_5min, width, label='RV (5min)')
plt.bar(x, annualized_vol_10min, width, label='RV (10min)')
plt.bar(x + width, annualized_vol_rk, width, label='RK')
plt.bar(x + 2*width, annualized_vol_msrv, width, label='MSRV')
plt.bar(x + 3*width, annualized_vol_tsrv, width, label='TSRV')

plt.title('Comparison of Volatility Estimators with Microstructure Noise')
plt.xlabel('Day')
plt.ylabel('Annualized Volatility (%)')
plt.xticks(x)
plt.legend()
plt.grid(True, alpha=0.3)
plt.tight_layout()
plt.show()

# Plot MAPE comparison
plt.figure(figsize=(12, 6))
estimators = ['RV (1min)', 'RV (5min)', 'RV (10min)', 'RK', 'MSRV', 'TSRV', 'QMLE']
mapes = [mape_1min, mape_5min, mape_10min, mape_rk, mape_msrv, mape_tsrv, mape_qmle]

plt.bar(estimators, mapes)
plt.title('Mean Absolute Percentage Error (MAPE) of Volatility Estimators')
plt.ylabel('MAPE (%)')
plt.xticks(rotation=45)
plt.grid(True, alpha=0.3)
plt.tight_layout()
plt.show()


### 2.5 Finding the Optimal Sampling Frequency

The MFE Toolbox provides a function to find the optimal sampling frequency that balances the bias-variance tradeoff in the presence of microstructure noise.

# Generate a single day of high-frequency data with microstructure noise
single_day_data, _, true_iv_single = generate_data_with_noise(
    n_days=1, 
    n_intraday=390,  # 1-minute data
    base_vol=0.015,
    noise_std=0.001
)

# Find optimal sampling frequency
optimal_freq, optimal_rv = variance_optimal_sampling(
    prices=single_day_data['price'].values,
    timestamps=single_day_data.index.values,
    noise_estimate=None  # Automatically estimate noise
)

# Estimate realized variance at different sampling frequencies
rv_estimator = RealizedVariance()
sampling_frequencies = [1, 2, 3, 5, 10, 15, 20, 30, 60]  # in minutes
rv_estimates = []

for freq in sampling_frequencies:
    rv = rv_estimator.compute(
        prices=single_day_data['price'].values,
        timestamps=single_day_data.index.values,
        sampling=f'{freq}min'
    )[0]  # Single day
    rv_estimates.append(rv)

# Print results
print(f"True Integrated Variance: {true_iv_single[0]:.6f}")
print(f"Optimal Sampling Frequency: {optimal_freq:.2f} minutes")
print(f"Realized Variance at Optimal Frequency: {optimal_rv:.6f}")
print(f"Bias: {(optimal_rv/true_iv_single[0]-1)*100:.2f}%")

# Print RV at different frequencies
print("\nRealized Variance at Different Sampling Frequencies:")
for i, freq in enumerate(sampling_frequencies):
    bias = (rv_estimates[i]/true_iv_single[0]-1)*100
    print(f"  {freq} min: RV = {rv_estimates[i]:.6f}, Bias = {bias:.2f}%")


# Plot RV vs. sampling frequency
plt.figure(figsize=(10, 6))
plt.plot(sampling_frequencies, rv_estimates, 'o-', label='RV Estimates')
plt.axhline(y=true_iv_single[0], color='r', linestyle='--', label='True IV')
plt.axvline(x=optimal_freq, color='g', linestyle='--', label=f'Optimal ({optimal_freq:.2f} min)')
plt.title('Realized Variance vs. Sampling Frequency')
plt.xlabel('Sampling Frequency (minutes)')
plt.ylabel('Realized Variance')
plt.legend()
plt.grid(True)
plt.tight_layout()
plt.show()


### 2.6 Realized Semivariance

Realized semivariance separates upside and downside risk, which is useful for analyzing asymmetric volatility.

# Generate data with asymmetric returns
def generate_asymmetric_data(n_days=5, n_intraday=390, base_vol=0.015):
    """
    Generate multi-day high-frequency data with asymmetric returns.
    
    Parameters:
    -----------
    n_days : int
        Number of trading days
    n_intraday : int
        Number of observations per day
    base_vol : float
        Base volatility level
        
    Returns:
    --------
    DataFrame with high-frequency data
    """
    # Create timestamps
    timestamps = []
    for day in range(n_days):
        day_date = pd.Timestamp(f'2023-01-{day+1:02d}')
        for i in range(n_intraday):
            # 9:30 AM to 4:00 PM (390 minutes = 6.5 hours)
            minute = 9*60 + 30 + i * (6.5*60 / n_intraday)
            hour = int(minute // 60)
            minute = int(minute % 60)
            timestamps.append(day_date + pd.Timedelta(hours=hour, minutes=minute))
    
    # Generate returns with asymmetry
    returns = np.zeros(n_days * n_intraday)
    
    for day in range(n_days):
        # Intraday volatility
        intraday_vol = base_vol / np.sqrt(n_intraday)
        
        # Generate returns for this day
        start_idx = day * n_intraday
        end_idx = (day + 1) * n_intraday
        
        # Generate normal returns
        day_returns = np.random.normal(0, intraday_vol, n_intraday)
        
        # Add asymmetry based on day number
        if day % 2 == 0:  # Days 1, 3, 5: More negative jumps
            # Add more negative jumps
            n_neg_jumps = 5
            n_pos_jumps = 2
        else:  # Days 2, 4: More positive jumps
            # Add more positive jumps
            n_neg_jumps = 2
            n_pos_jumps = 5
        
        # Add negative jumps
        neg_jump_idx = np.random.choice(n_intraday, size=n_neg_jumps, replace=False)
        for idx in neg_jump_idx:
            day_returns[idx] -= np.random.uniform(0.005, 0.01)
        
        # Add positive jumps
        pos_jump_idx = np.random.choice(n_intraday, size=n_pos_jumps, replace=False)
        for idx in pos_jump_idx:
            day_returns[idx] += np.random.uniform(0.005, 0.01)
        
        returns[start_idx:end_idx] = day_returns
    
    # Convert to prices
    log_prices = np.cumsum(returns)
    prices = 100 * np.exp(log_prices)  # Start at price 100
    
    # Create DataFrame
    df = pd.DataFrame({
        'price': prices
    }, index=timestamps)
    
    return df

# Generate data with asymmetric returns
asymmetric_data = generate_asymmetric_data(n_days=5, n_intraday=390)

# Plot the data
plt.figure(figsize=(12, 6))
plt.plot(asymmetric_data.index, asymmetric_data['price'])
plt.title('High-Frequency Price Data with Asymmetric Returns')
plt.xlabel('Time')
plt.ylabel('Price')
plt.xticks(rotation=45)
plt.tight_layout()
plt.show()


# Create estimators
rv_estimator = RealizedVariance()
rsv_estimator = RealizedSemivariance()

# Estimate daily realized variance
rv = rv_estimator.compute(
    prices=asymmetric_data['price'].values,
    timestamps=asymmetric_data.index.values,
    sampling='5min'
)

# Estimate daily realized semivariance (positive and negative)
rsv_pos = rsv_estimator.compute(
    prices=asymmetric_data['price'].values,
    timestamps=asymmetric_data.index.values,
    sampling='5min',
    type='positive'
)

rsv_neg = rsv_estimator.compute(
    prices=asymmetric_data['price'].values,
    timestamps=asymmetric_data.index.values,
    sampling='5min',
    type='negative'
)

# Convert to annualized volatility
annualized_vol_rv = np.sqrt(rv * 252) * 100
annualized_vol_pos = np.sqrt(rsv_pos * 252) * 100
annualized_vol_neg = np.sqrt(rsv_neg * 252) * 100

# Print results
print("Realized Variance and Semivariance:")
for day in range(5):
    print(f"Day {day+1}:")
    print(f"  RV = {rv[day]:.6f}, Annualized Vol = {annualized_vol_rv[day]:.2f}%")
    print(f"  RSV+ = {rsv_pos[day]:.6f}, Annualized Vol+ = {annualized_vol_pos[day]:.2f}%")
    print(f"  RSV- = {rsv_neg[day]:.6f}, Annualized Vol- = {annualized_vol_neg[day]:.2f}%")
    print(f"  Asymmetry Ratio = {rsv_neg[day]/rsv_pos[day]:.2f}")


# Plot comparison
plt.figure(figsize=(12, 6))

x = np.arange(1, 6)
width = 0.3

plt.bar(x - width, annualized_vol_pos, width, label='Upside Vol')
plt.bar(x, annualized_vol_rv, width, label='Total Vol')
plt.bar(x + width, annualized_vol_neg, width, label='Downside Vol')

plt.title('Comparison of Realized Volatility Components')
plt.xlabel('Day')
plt.ylabel('Annualized Volatility (%)')
plt.xticks(x)
plt.legend()
plt.grid(True, alpha=0.3)
plt.tight_layout()
plt.show()

# Plot asymmetry ratio
plt.figure(figsize=(10, 6))
plt.bar(x, rsv_neg/rsv_pos)
plt.axhline(y=1, color='r', linestyle='--', label='Symmetric')
plt.title('Downside/Upside Volatility Ratio')
plt.xlabel('Day')
plt.ylabel('Ratio')
plt.xticks(x)
plt.legend()
plt.grid(True, alpha=0.3)
plt.tight_layout()
plt.show()


## 3. Multivariate Realized Volatility

Now we'll explore multivariate realized volatility estimators for estimating covariance between multiple assets.

def generate_multivariate_data(n_days=5, n_intraday=390, correlations=None):
    """
    Generate multi-day high-frequency data for two correlated assets.
    
    Parameters:
    -----------
    n_days : int
        Number of trading days
    n_intraday : int
        Number of observations per day
    correlations : list or None
        List of daily correlations. If None, defaults to [0.3, 0.5, 0.7, 0.2, 0.6]
        
    Returns:
    --------
    tuple: (DataFrame with high-frequency data for two assets, array of true correlations)
    """
    # Default correlations if not provided
    if correlations is None:
        correlations = [0.3, 0.5, 0.7, 0.2, 0.6]
    
    # Ensure we have enough correlation values
    if len(correlations) < n_days:
        correlations = correlations * (n_days // len(correlations) + 1)
        correlations = correlations[:n_days]
    
    # Create timestamps
    timestamps = []
    for day in range(n_days):
        day_date = pd.Timestamp(f'2023-01-{day+1:02d}')
        for i in range(n_intraday):
            # 9:30 AM to 4:00 PM (390 minutes = 6.5 hours)
            minute = 9*60 + 30 + i * (6.5*60 / n_intraday)
            hour = int(minute // 60)
            minute = int(minute % 60)
            timestamps.append(day_date + pd.Timedelta(hours=hour, minutes=minute))
    
    # Initialize price arrays
    n_total = n_days * n_intraday
    log_prices1 = np.zeros(n_total)
    log_prices2 = np.zeros(n_total)
    
    # Generate correlated returns for each day
    for day in range(n_days):
        # Correlation matrix for this day
        corr = correlations[day]
        cov_matrix = np.array([[1.0, corr], [corr, 1.0]]) * (0.001**2)
        
        # Generate correlated returns
        day_returns = np.random.multivariate_normal(
            mean=[0, 0],
            cov=cov_matrix,
            size=n_intraday
        )
        
        # Accumulate returns to log prices
        start_idx = day * n_intraday
        end_idx = (day + 1) * n_intraday
        
        if day == 0:
            log_prices1[start_idx:end_idx] = np.cumsum(day_returns[:, 0])
            log_prices2[start_idx:end_idx] = np.cumsum(day_returns[:, 1])
        else:
            log_prices1[start_idx:end_idx] = log_prices1[start_idx-1] + np.cumsum(day_returns[:, 0])
            log_prices2[start_idx:end_idx] = log_prices2[start_idx-1] + np.cumsum(day_returns[:, 1])
    
    # Convert to prices
    prices1 = 100 * np.exp(log_prices1)  # Start at price 100
    prices2 = 100 * np.exp(log_prices2)  # Start at price 100
    
    # Create DataFrame
    df = pd.DataFrame({
        'price1': prices1,
        'price2': prices2
    }, index=timestamps)
    
    return df, np.array(correlations)

# Generate multivariate data
multivariate_data, true_correlations = generate_multivariate_data(n_days=5, n_intraday=390)

# Plot the data
plt.figure(figsize=(12, 6))
plt.plot(multivariate_data.index, multivariate_data['price1'], 'b-', label='Asset 1')
plt.plot(multivariate_data.index, multivariate_data['price2'], 'r-', label='Asset 2')
plt.title('High-Frequency Price Data for Two Correlated Assets')
plt.xlabel('Time')
plt.ylabel('Price')
plt.legend()
plt.xticks(rotation=45)
plt.tight_layout()
plt.show()

# Print true correlations
print("True Daily Correlations:")
for day in range(5):
    print(f"Day {day+1}: {true_correlations[day]:.2f}")


# Create realized covariance estimator
rcov_estimator = RealizedCovariance()

# Estimate daily realized covariance
rcov = rcov_estimator.compute(
    prices=[multivariate_data['price1'].values, multivariate_data['price2'].values],
    timestamps=multivariate_data.index.values,
    sampling='5min'
)

# Extract variances and covariances
var1 = rcov[:, 0, 0]  # Variance of asset 1
var2 = rcov[:, 1, 1]  # Variance of asset 2
cov12 = rcov[:, 0, 1]  # Covariance between assets 1 and 2

# Calculate realized correlation
rcorr = cov12 / np.sqrt(var1 * var2)

# Print results
print("Daily Realized Covariance and Correlation:")
for day in range(5):
    print(f"Day {day+1}:")
    print(f"  True Correlation: {true_correlations[day]:.2f}")
    print(f"  Realized Correlation: {rcorr[day]:.2f}")
    print(f"  Realized Variance (Asset 1): {var1[day]:.6f}")
    print(f"  Realized Variance (Asset 2): {var2[day]:.6f}")
    print(f"  Realized Covariance: {cov12[day]:.6f}")


# Plot realized correlation vs. true correlation
plt.figure(figsize=(10, 6))
plt.plot(range(1, 6), true_correlations, 'o-', label='True Correlation')
plt.plot(range(1, 6), rcorr, 's-', label='Realized Correlation')
plt.title('True vs. Realized Correlation')
plt.xlabel('Day')
plt.ylabel('Correlation')
plt.xticks(range(1, 6))
plt.legend()
plt.grid(True)
plt.tight_layout()
plt.show()

# Plot realized covariance matrix for day 3 (highest correlation)
plt.figure(figsize=(8, 6))
sns.heatmap(rcov[2], annot=True, fmt='.6f', cmap='coolwarm',
            xticklabels=['Asset 1', 'Asset 2'], yticklabels=['Asset 1', 'Asset 2'])
plt.title(f'Realized Covariance Matrix (Day 3)')
plt.tight_layout()
plt.show()


### 3.1 Multivariate Realized Kernel

Now let's add microstructure noise to the multivariate data and compare the realized covariance with the multivariate realized kernel.

def generate_multivariate_data_with_noise(n_days=5, n_intraday=390, correlations=None, noise_std=0.001):
    """
    Generate multi-day high-frequency data for two correlated assets with microstructure noise.
    
    Parameters:
    -----------
    n_days : int
        Number of trading days
    n_intraday : int
        Number of observations per day
    correlations : list or None
        List of daily correlations. If None, defaults to [0.3, 0.5, 0.7, 0.2, 0.6]
    noise_std : float
        Standard deviation of microstructure noise
        
    Returns:
    --------
    tuple: (DataFrame with noisy high-frequency data for two assets, array of true correlations)
    """
    # Generate clean multivariate data
    clean_data, true_correlations = generate_multivariate_data(n_days, n_intraday, correlations)
    
    # Add microstructure noise
    n_total = len(clean_data)
    noise1 = np.random.normal(0, noise_std, n_total)
    noise2 = np.random.normal(0, noise_std, n_total)
    
    # Create noisy data
    noisy_data = pd.DataFrame({
        'price1': clean_data['price1'] * np.exp(noise1),
        'price2': clean_data['price2'] * np.exp(noise2)
    }, index=clean_data.index)
    
    return noisy_data, true_correlations

# Generate multivariate data with noise
noisy_multivariate_data, true_correlations = generate_multivariate_data_with_noise(
    n_days=5, 
    n_intraday=390,
    noise_std=0.001
)

# Create estimators
rcov_estimator = RealizedCovariance()
mrk_estimator = MultivariateRealizedKernel(kernel_type='parzen')

# Estimate daily realized covariance
rcov = rcov_estimator.compute(
    prices=[noisy_multivariate_data['price1'].values, noisy_multivariate_data['price2'].values],
    timestamps=noisy_multivariate_data.index.values,
    sampling='5min'
)

# Estimate daily multivariate realized kernel
mrk = mrk_estimator.compute(
    prices=[noisy_multivariate_data['price1'].values, noisy_multivariate_data['price2'].values],
    timestamps=noisy_multivariate_data.index.values
)

# Extract correlations
rcov_corr = rcov[:, 0, 1] / np.sqrt(rcov[:, 0, 0] * rcov[:, 1, 1])
mrk_corr = mrk[:, 0, 1] / np.sqrt(mrk[:, 0, 0] * mrk[:, 1, 1])

# Print results
print("Comparison of Realized Covariance and Multivariate Realized Kernel:")
for day in range(5):
    print(f"Day {day+1}:")
    print(f"  True Correlation: {true_correlations[day]:.2f}")
    print(f"  RC Correlation: {rcov_corr[day]:.2f}")
    print(f"  MRK Correlation: {mrk_corr[day]:.2f}")
    print(f"  RC Bias: {(rcov_corr[day]/true_correlations[day]-1)*100:.2f}%")
    print(f"  MRK Bias: {(mrk_corr[day]/true_correlations[day]-1)*100:.2f}%")


# Plot comparison
plt.figure(figsize=(12, 6))

x = np.arange(1, 6)
width = 0.25

plt.bar(x - width, true_correlations, width, label='True')
plt.bar(x, rcov_corr, width, label='RC')
plt.bar(x + width, mrk_corr, width, label='MRK')

plt.title('Comparison of Correlation Estimators with Microstructure Noise')
plt.xlabel('Day')
plt.ylabel('Correlation')
plt.xticks(x)
plt.legend()
plt.grid(True, alpha=0.3)
plt.tight_layout()
plt.show()

# Calculate mean absolute percentage error (MAPE)
mape_rcov = np.mean(np.abs(rcov_corr/true_correlations - 1)) * 100
mape_mrk = np.mean(np.abs(mrk_corr/true_correlations - 1)) * 100

# Plot MAPE comparison
plt.figure(figsize=(8, 6))
estimators = ['RC', 'MRK']
mapes = [mape_rcov, mape_mrk]

plt.bar(estimators, mapes)
plt.title('Mean Absolute Percentage Error (MAPE) of Correlation Estimators')
plt.ylabel('MAPE (%)')
plt.grid(True, alpha=0.3)
plt.tight_layout()
plt.show()


## 4. Asynchronous Processing for Large Datasets

The MFE Toolbox supports asynchronous processing for handling large high-frequency datasets efficiently.

# Generate a large high-frequency dataset
np.random.seed(42)
n_days = 20
n_intraday = 390  # 1-minute data

# Create timestamps
timestamps = []
for day in range(n_days):
    day_date = pd.Timestamp(f'2023-01-{day+1:02d}')
    for i in range(n_intraday):
        # 9:30 AM to 4:00 PM (390 minutes = 6.5 hours)
        minute = 9*60 + 30 + i * (6.5*60 / n_intraday)
        hour = int(minute // 60)
        minute = int(minute % 60)
        timestamps.append(day_date + pd.Timedelta(hours=hour, minutes=minute))

# Generate price process with time-varying volatility
n_total = n_days * n_intraday
volatility = np.ones(n_total) * 0.001

# Add volatility clusters
for i in range(3):
    cluster_start = np.random.randint(0, n_total - n_intraday)
    cluster_length = np.random.randint(n_intraday, 3 * n_intraday)
    cluster_end = min(cluster_start + cluster_length, n_total)
    volatility[cluster_start:cluster_end] *= np.random.uniform(2, 4)

returns = np.random.normal(0, volatility)
log_prices = np.cumsum(returns)
prices = 100 * np.exp(log_prices)  # Start at price 100

# Create DataFrame
large_data = pd.DataFrame({
    'price': prices
}, index=timestamps)

# Create estimators
rv_estimator = RealizedVariance()
rk_estimator = RealizedKernel()

# Define asynchronous function for realized variance estimation
async def compute_rv_async():
    # Define progress callback
    def progress_callback(percent, message):
        print(f"{percent:.1f}% complete: {message}")
    
    # Compute realized variance asynchronously
    rv = await rv_estimator.compute_async(
        prices=large_data['price'].values,
        timestamps=large_data.index.values,
        sampling='5min',
        progress_callback=progress_callback
    )
    
    return rv

# Define asynchronous function for realized kernel estimation
async def compute_rk_async():
    # Define progress callback
    def progress_callback(percent, message):
        print(f"{percent:.1f}% complete: {message}")
    
    # Compute realized kernel asynchronously
    rk = await rk_estimator.compute_async(
        prices=large_data['price'].values,
        timestamps=large_data.index.values,
        progress_callback=progress_callback
    )
    
    return rk

# Run asynchronous computations
async def main():
    print("Computing Realized Variance...")
    rv = await compute_rv_async()
    
    print("\nComputing Realized Kernel...")
    rk = await compute_rk_async()
    
    return rv, rk

# Execute the async function
rv, rk = await main()


# Convert to annualized volatility
annualized_vol_rv = np.sqrt(rv * 252) * 100  # In percentage
annualized_vol_rk = np.sqrt(rk * 252) * 100

# Plot results
plt.figure(figsize=(12, 8))

plt.subplot(2, 1, 1)
plt.plot(range(1, n_days+1), annualized_vol_rv, 'o-', label='RV')
plt.plot(range(1, n_days+1), annualized_vol_rk, 's-', label='RK')
plt.title('Realized Volatility Estimates')
plt.ylabel('Annualized Volatility (%)')
plt.legend()
plt.grid(True)

plt.subplot(2, 1, 2)
plt.plot(range(1, n_days+1), annualized_vol_rk / annualized_vol_rv, 'o-')
plt.axhline(y=1, color='r', linestyle='--')
plt.title('Ratio of RK to RV')
plt.xlabel('Day')
plt.ylabel('Ratio')
plt.grid(True)

plt.tight_layout()
plt.show()


## 5. Practical Applications

### 5.1 Intraday Volatility Pattern

Let's examine the intraday volatility pattern using high-frequency data.

def generate_data_with_intraday_pattern(n_days=5, n_intraday=390):
    """
    Generate high-frequency data with U-shaped intraday volatility pattern.
    
    Parameters:
    -----------
    n_days : int
        Number of trading days
    n_intraday : int
        Number of observations per day
        
    Returns:
    --------
    DataFrame with high-frequency data
    """
    # Create timestamps
    timestamps = []
    for day in range(n_days):
        day_date = pd.Timestamp(f'2023-01-{day+1:02d}')
        for i in range(n_intraday):
            # 9:30 AM to 4:00 PM (390 minutes = 6.5 hours)
            minute = 9*60 + 30 + i * (6.5*60 / n_intraday)
            hour = int(minute // 60)
            minute = int(minute % 60)
            timestamps.append(day_date + pd.Timedelta(hours=hour, minutes=minute))
    
    # Generate U-shaped intraday volatility pattern
    intraday_pattern = np.zeros(n_intraday)
    for i in range(n_intraday):
        # Normalize to [0, 1]
        x = i / (n_intraday - 1)
        # U-shape: high at open and close, low in the middle
        intraday_pattern[i] = 0.002 - 0.0015 * np.sin(np.pi * x)
    
    # Generate returns with intraday pattern
    returns = np.zeros(n_days * n_intraday)
    for day in range(n_days):
        start_idx = day * n_intraday
        end_idx = (day + 1) * n_intraday
        
        # Generate returns with time-varying volatility
        for i in range(n_intraday):
            idx = start_idx + i
            returns[idx] = np.random.normal(0, intraday_pattern[i])
    
    # Convert to prices
    log_prices = np.cumsum(returns)
    prices = 100 * np.exp(log_prices)  # Start at price 100
    
    # Create DataFrame
    df = pd.DataFrame({
        'price': prices,
        'returns': returns
    }, index=timestamps)
    
    return df, intraday_pattern

# Generate data with intraday pattern
intraday_data, true_pattern = generate_data_with_intraday_pattern(n_days=20, n_intraday=390)

# Calculate absolute returns by time of day
intraday_data['abs_returns'] = np.abs(intraday_data['returns'])
intraday_data['time'] = intraday_data.index.time

# Group by time of day and calculate average absolute returns
intraday_vol = intraday_data.groupby('time')['abs_returns'].mean()

# Convert to hourly intervals for better visualization
hourly_vol = intraday_vol.resample('30min').mean()

# Plot intraday volatility pattern
plt.figure(figsize=(12, 6))
plt.plot(hourly_vol.index, hourly_vol.values, 'o-', linewidth=2)
plt.title('Intraday Volatility Pattern')
plt.xlabel('Time of Day')
plt.ylabel('Average Absolute Returns')
plt.grid(True)
plt.xticks(rotation=45)
plt.tight_layout()
plt.show()


### 5.2 Jump Detection

Let's implement a jump detection procedure using the ratio of bipower variation to realized variance.

# Generate data with jumps
jump_data, true_cont_iv, jump_contribution = generate_data_with_jumps(
    n_days=10, 
    n_intraday=390,  # 1-minute data
    jump_days=[2, 5, 8], 
    jump_sizes=[0.02, -0.015, 0.025]
)

# Create estimators
rv_estimator = RealizedVariance()
bv_estimator = BipowerVariation()

# Compute realized variance and bipower variation
rv = rv_estimator.compute(
    prices=jump_data['price'].values,
    timestamps=jump_data.index.values,
    sampling='5min'
)

bv = bv_estimator.compute(
    prices=jump_data['price'].values,
    timestamps=jump_data.index.values,
    sampling='5min'
)

# Compute jump component
jump_component = np.maximum(0, rv - bv)

# Compute relative jump measure
relative_jump = jump_component / rv

# Compute z-statistic for jump detection
# Under the null of no jumps, this follows a standard normal distribution
n_obs_per_day = 78  # Approximate number of 5-minute intervals in a trading day
z_statistic = (rv - bv) / np.sqrt((np.pi**2/4 + np.pi - 5) * (1/n_obs_per_day) * bv**2)

# Critical value for 99% confidence
critical_value = 2.576

# Detect significant jumps
significant_jumps = z_statistic > critical_value

# Print results
print("Jump Detection Results:")
for day in range(10):
    print(f"Day {day+1}:")
    print(f"  RV = {rv[day]:.6f}")
    print(f"  BV = {bv[day]:.6f}")
    print(f"  Jump Component = {jump_component[day]:.6f}")
    print(f"  Relative Jump = {relative_jump[day]*100:.2f}%")
    print(f"  Z-statistic = {z_statistic[day]:.4f}")
    print(f"  Significant Jump: {'Yes' if significant_jumps[day] else 'No'}")
    print(f"  True Jump: {'Yes' if day+1 in [2, 5, 8] else 'No'}")


# Plot prices
plt.figure(figsize=(12, 8))

plt.subplot(2, 1, 1)
plt.plot(jump_data.index, jump_data['price'])

# Mark jump days
for day in [2, 5, 8]:  # 1-indexed days with jumps
    day_idx = day - 1  # Convert to 0-indexed
    jump_time = jump_data.index[day_idx * 390 + 390 // 2]  # Middle of the day
    plt.axvline(x=jump_time, color='r', linestyle='--', 
                label=f'Jump on Day {day}' if day == 2 else None)

plt.title('Price Process with Jumps')
plt.ylabel('Price')
plt.legend()

# Plot jump measures
plt.subplot(2, 1, 2)
plt.bar(range(1, 11), relative_jump*100)
plt.axhline(y=0, color='k', linestyle='-')

# Mark significant jumps
for day in range(10):
    if significant_jumps[day]:
        plt.plot(day+1, relative_jump[day]*100, 'ro', markersize=10)

plt.title('Relative Jump Measure (% of RV)')
plt.xlabel('Day')
plt.ylabel('Jump Component (%)')
plt.xticks(range(1, 11))

plt.tight_layout()
plt.show()

# Plot z-statistics
plt.figure(figsize=(10, 6))
plt.bar(range(1, 11), z_statistic)
plt.axhline(y=critical_value, color='r', linestyle='--', label='99% Critical Value')

# Mark true jump days
for day in [2, 5, 8]:
    plt.plot(day, z_statistic[day-1], 'go', markersize=10)

plt.title('Jump Test Z-Statistics')
plt.xlabel('Day')
plt.ylabel('Z-Statistic')
plt.xticks(range(1, 11))
plt.legend()
plt.tight_layout()
plt.show()


## 6. Conclusion

In this notebook, we've explored the high-frequency financial econometrics capabilities of the MFE Toolbox. We've covered:

1. **Data Handling**: Loading, preprocessing, and filtering high-frequency data using Pandas
2. **Time Conversions**: Converting between different time formats for intraday analysis
3. **Basic Realized Variance**: Estimating volatility from intraday returns
4. **Robust Estimators**: Using bipower variation for jump-robust volatility estimation
5. **Noise-Robust Estimators**: Handling microstructure noise with realized kernel, multiscale variance, etc.
6. **Multivariate Analysis**: Estimating covariance and correlation between assets
7. **Asynchronous Processing**: Efficiently handling large datasets
8. **Practical Applications**: Intraday patterns and jump detection

The MFE Toolbox provides a comprehensive suite of Python-based tools for high-frequency financial econometrics, leveraging NumPy, Pandas, and Numba for efficient computation. These tools are essential for modern risk management, market microstructure research, and high-frequency trading applications.