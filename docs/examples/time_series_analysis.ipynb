# docs/examples/time_series_analysis.ipynb
```python
{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Time Series Analysis with the MFE Toolbox\n",
    "\n",
    "This notebook demonstrates the time series analysis capabilities of the MFE Toolbox, a comprehensive Python-based suite for financial econometrics. We'll explore ARMA/ARMAX modeling, forecasting, impulse response analysis, and seasonal decomposition using the toolbox's modern Python implementation.\n",
    "\n",
    "## Overview\n",
    "\n",
    "Time series analysis is a fundamental component of financial econometrics, enabling researchers and practitioners to model the dynamic behavior of economic and financial variables over time. The MFE Toolbox provides a comprehensive suite of time series modeling tools, including:\n",
    "\n",
    "- ARMA/ARMAX modeling and forecasting\n",
    "- Unit root testing and stationarity analysis\n",
    "- Impulse response analysis\n",
    "- Time series decomposition and filtering\n",
    "- Vector autoregression (VAR) analysis\n",
    "- Granger causality testing\n",
    "- Heterogeneous autoregression (HAR) modeling\n",
    "\n",
    "These tools are implemented using Python's scientific stack, leveraging NumPy for efficient array operations, Pandas for time series handling, Statsmodels for econometric modeling, and Numba for performance acceleration."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup and Imports\n",
    "\n",
    "Let's start by importing the necessary modules from the MFE Toolbox and other required libraries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import core libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from datetime import datetime, timedelta\n",
    "import asyncio\n",
    "\n",
    "# Import MFE Toolbox components\n",
    "from mfe.models.time_series import ARMA, VAR, HAR\n",
    "from mfe.models.time_series.correlation import acf, pacf, plot_acf, plot_pacf\n",
    "from mfe.models.time_series.unit_root import adf_test, kpss_test\n",
    "from mfe.models.time_series.filters import hp_filter, bk_filter, beveridge_nelson_decomposition\n",
    "from mfe.models.time_series.impulse_response import impulse_response\n",
    "from mfe.models.time_series.causality import granger_causality\n",
    "from mfe.models.time_series.diagnostics import residual_diagnostics, forecast_evaluation\n",
    "\n",
    "# Set plotting style\n",
    "%matplotlib inline\n",
    "plt.style.use('ggplot')\n",
    "sns.set_context(\"notebook\", font_scale=1.2)\n",
    "\n",
    "# Set random seed for reproducibility\n",
    "np.random.seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Generating and Exploring Time Series Data\n",
    "\n",
    "Let's start by generating some synthetic time series data with known properties. We'll create an AR(2) process with some seasonality and a trend component."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a date range for our time series\n",
    "dates = pd.date_range(start='2018-01-01', periods=1000, freq='B')\n",
    "\n",
    "# Generate an AR(2) process with seasonality and trend\n",
    "n = len(dates)\n",
    "ar_params = [0.7, -0.2]  # AR(2) parameters\n",
    "ma_params = [0.3]        # MA(1) parameter\n",
    "sigma = 0.5              # Innovation standard deviation\n",
    "\n",
    "# Initialize the series\n",
    "y = np.zeros(n)\n",
    "e = np.random.normal(0, sigma, n)  # Random innovations\n",
    "\n",
    "# Add AR(2) dynamics\n",
    "for t in range(2, n):\n",
    "    y[t] = ar_params[0] * y[t-1] + ar_params[1] * y[t-2] + e[t] + ma_params[0] * e[t-1]\n",
    "\n",
    "# Add seasonality (weekly pattern)\n",
    "seasonality = 0.5 * np.sin(2 * np.pi * np.arange(n) / 5)  # 5-day business week cycle\n",
    "\n",
    "# Add trend\n",
    "trend = 0.001 * np.arange(n)\n",
    "\n",
    "# Combine components\n",
    "y_final = y + seasonality + trend\n",
    "\n",
    "# Create a Pandas Series with DatetimeIndex\n",
    "ts_data = pd.Series(y_final, index=dates, name='value')\n",
    "\n",
    "# Display the first few observations\n",
    "print(\"First 10 observations:\")\n",
    "print(ts_data.head(10))\n",
    "\n",
    "# Plot the time series\n",
    "plt.figure(figsize=(14, 6))\n",
    "plt.plot(ts_data.index, ts_data.values)\n",
    "plt.title('Simulated Time Series with AR(2) Dynamics, Seasonality, and Trend')\n",
    "plt.xlabel('Date')\n",
    "plt.ylabel('Value')\n",
    "plt.grid(True)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exploring Time Series Properties\n",
    "\n",
    "Let's examine the autocorrelation and partial autocorrelation functions to understand the time series dynamics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a figure with ACF and PACF plots\n",
    "fig, (ax1, ax2) = plt.subplots(2, 1, figsize=(14, 10))\n",
    "\n",
    "# Plot ACF\n",
    "plot_acf(ts_data, lags=30, ax=ax1)\n",
    "ax1.set_title('Autocorrelation Function (ACF)')\n",
    "\n",
    "# Plot PACF\n",
    "plot_pacf(ts_data, lags=30, ax=ax2)\n",
    "ax2.set_title('Partial Autocorrelation Function (PACF)')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Print ACF and PACF values for the first 10 lags\n",
    "acf_values = acf(ts_data, nlags=10)\n",
    "pacf_values = pacf(ts_data, nlags=10)\n",
    "\n",
    "print(\"ACF values:\")\n",
    "for i, val in enumerate(acf_values):\n",
    "    print(f\"Lag {i}: {val:.4f}\")\n",
    "\n",
    "print(\"\\nPACF values:\")\n",
    "for i, val in enumerate(pacf_values):\n",
    "    print(f\"Lag {i}: {val:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing for Stationarity\n",
    "\n",
    "Before modeling, it's important to check whether the time series is stationary. We'll use the Augmented Dickey-Fuller (ADF) and KPSS tests."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run ADF test\n",
    "adf_result = adf_test(ts_data)\n",
    "\n",
    "print(\"Augmented Dickey-Fuller Test:\")\n",
    "print(f\"Test Statistic: {adf_result['adf_stat']:.4f}\")\n",
    "print(f\"p-value: {adf_result['p_value']:.4f}\")\n",
    "print(f\"Critical Values: 1%: {adf_result['critical_values']['1%']:.4f}, " +\n",
    "      f\"5%: {adf_result['critical_values']['5%']:.4f}, " +\n",
    "      f\"10%: {adf_result['critical_values']['10%']:.4f}\")\n",
    "print(f\"Is Stationary: {adf_result['is_stationary']}\")\n",
    "\n",
    "# Run KPSS test\n",
    "kpss_result = kpss_test(ts_data)\n",
    "\n",
    "print(\"\\nKPSS Test:\")\n",
    "print(f\"Test Statistic: {kpss_result['kpss_stat']:.4f}\")\n",
    "print(f\"p-value: {kpss_result['p_value']:.4f}\")\n",
    "print(f\"Critical Values: 1%: {kpss_result['critical_values']['1%']:.4f}, " +\n",
    "      f\"5%: {kpss_result['critical_values']['5%']:.4f}, " +\n",
    "      f\"10%: {kpss_result['critical_values']['10%']:.4f}\")\n",
    "print(f\"Is Stationary: {kpss_result['is_stationary']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If the tests indicate non-stationarity, we might need to difference the data. Let's create a differenced series and check its stationarity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Difference the time series\n",
    "diff_ts = ts_data.diff().dropna()\n",
    "\n",
    "# Plot the differenced series\n",
    "plt.figure(figsize=(14, 6))\n",
    "plt.plot(diff_ts.index, diff_ts.values)\n",
    "plt.title('First Difference of the Time Series')\n",
    "plt.xlabel('Date')\n",
    "plt.ylabel('Differenced Value')\n",
    "plt.grid(True)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Run ADF test on differenced series\n",
    "adf_diff_result = adf_test(diff_ts)\n",
    "\n",
    "print(\"Augmented Dickey-Fuller Test (Differenced Series):\")\n",
    "print(f\"Test Statistic: {adf_diff_result['adf_stat']:.4f}\")\n",
    "print(f\"p-value: {adf_diff_result['p_value']:.4f}\")\n",
    "print(f\"Is Stationary: {adf_diff_result['is_stationary']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. ARMA/ARMAX Modeling\n",
    "\n",
    "Now that we've explored the time series properties, let's fit an ARMA model to the data. Based on the ACF and PACF plots, we'll try an ARMA(2,1) model to match the data generating process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create and fit an ARMA(2,1) model\n",
    "arma_model = ARMA(ar_order=2, ma_order=1, include_constant=True)\n",
    "arma_result = arma_model.fit(ts_data)\n",
    "\n",
    "# Print model summary\n",
    "print(arma_result.summary())\n",
    "\n",
    "# Access model parameters\n",
    "constant = arma_result.params.constant\n",
    "ar_params = arma_result.params.ar\n",
    "ma_params = arma_result.params.ma\n",
    "\n",
    "print(f\"\\nEstimated Parameters:\")\n",
    "print(f\"Constant: {constant:.4f}\")\n",
    "print(f\"AR parameters: {ar_params}\")\n",
    "print(f\"MA parameters: {ma_params}\")\n",
    "\n",
    "# Compare with true parameters\n",
    "print(f\"\\nTrue Parameters:\")\n",
    "print(f\"AR parameters: {ar_params}\")\n",
    "print(f\"MA parameters: {ma_params}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Diagnostics\n",
    "\n",
    "Let's examine the model residuals to check if our ARMA model has captured the dynamics of the time series."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get model residuals\n",
    "residuals = arma_result.residuals\n",
    "\n",
    "# Run diagnostic tests on residuals\n",
    "diag_results = residual_diagnostics(residuals)\n",
    "\n",
    "print(\"Residual Diagnostics:\")\n",
    "print(f\"Jarque-Bera Test (Normality): p-value = {diag_results['jarque_bera_p']:.6f}\")\n",
    "print(f\"Ljung-Box Test (Serial Correlation): p-value = {diag_results['ljung_box_p']:.6f}\")\n",
    "print(f\"ARCH LM Test (Heteroskedasticity): p-value = {diag_results['arch_lm_p']:.6f}\")\n",
    "\n",
    "# Create diagnostic plots\n",
    "fig, ((ax1, ax2), (ax3, ax4)) = plt.subplots(2, 2, figsize=(14, 10))\n",
    "\n",
    "# Plot residuals\n",
    "ax1.plot(residuals.index, residuals)\n",
    "ax1.set_title('Residuals')\n",
    "ax1.set_xlabel('Date')\n",
    "ax1.axhline(y=0, color='r', linestyle='-')\n",
    "\n",
    "# Histogram of residuals\n",
    "ax2.hist(residuals, bins=30, density=True, alpha=0.7)\n",
    "ax2.set_title('Residual Histogram')\n",
    "ax2.set_xlabel('Residual Value')\n",
    "\n",
    "# Add normal distribution curve\n",
    "x = np.linspace(min(residuals), max(residuals), 100)\n",
    "mean, std = np.mean(residuals), np.std(residuals)\n",
    "ax2.plot(x, 1/(std * np.sqrt(2 * np.pi)) * np.exp(-(x - mean)**2 / (2 * std**2)), \n",
    "         linewidth=2, color='r')\n",
    "\n",
    "# ACF of residuals\n",
    "plot_acf(residuals, lags=20, ax=ax3)\n",
    "ax3.set_title('ACF of Residuals')\n",
    "\n",
    "# PACF of residuals\n",
    "plot_pacf(residuals, lags=20, ax=ax4)\n",
    "ax4.set_title('PACF of Residuals')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# QQ plot for residuals\n",
    "plt.figure(figsize=(10, 6))\n",
    "from scipy import stats\n",
    "stats.probplot(residuals, dist=\"norm\", plot=plt)\n",
    "plt.title('Q-Q Plot of Residuals')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Selection\n",
    "\n",
    "Let's try different ARMA model specifications and select the best one based on information criteria."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define model orders to test\n",
    "ar_orders = range(0, 4)\n",
    "ma_orders = range(0, 4)\n",
    "\n",
    "# Store results\n",
    "results = []\n",
    "\n",
    "# Estimate models with different orders\n",
    "for p in ar_orders:\n",
    "    for q in ma_orders:\n",
    "        model = ARMA(ar_order=p, ma_order=q, include_constant=True)\n",
    "        try:\n",
    "            result = model.fit(ts_data)\n",
    "            results.append({\n",
    "                'ar_order': p,\n",
    "                'ma_order': q,\n",
    "                'aic': result.aic,\n",
    "                'bic': result.bic,\n",
    "                'loglikelihood': result.loglikelihood,\n",
    "                'result': result\n",
    "            })\n",
    "            print(f\"ARMA({p},{q}): AIC={result.aic:.4f}, BIC={result.bic:.4f}\")\n",
    "        except Exception as e:\n",
    "            print(f\"ARMA({p},{q}) failed: {str(e)}\")\n",
    "\n",
    "# Find the best model according to AIC\n",
    "best_aic = min(results, key=lambda x: x['aic'])\n",
    "print(f\"\\nBest model by AIC: ARMA({best_aic['ar_order']},{best_aic['ma_order']})\")\n",
    "print(f\"AIC: {best_aic['aic']:.4f}, BIC: {best_aic['bic']:.4f}\")\n",
    "\n",
    "# Find the best model according to BIC\n",
    "best_bic = min(results, key=lambda x: x['bic'])\n",
    "print(f\"Best model by BIC: ARMA({best_bic['ar_order']},{best_bic['ma_order']})\")\n",
    "print(f\"AIC: {best_bic['aic']:.4f}, BIC: {best_bic['bic']:.4f}\")\n",
    "\n",
    "# Create a heatmap of AIC values\n",
    "aic_matrix = np.zeros((len(ar_orders), len(ma_orders)))\n",
    "for result in results:\n",
    "    aic_matrix[result['ar_order'], result['ma_order']] = result['aic']\n",
    "\n",
    "plt.figure(figsize=(10, 8))\n",
    "sns.heatmap(aic_matrix, annot=True, fmt='.2f', cmap='viridis',\n",
    "            xticklabels=ma_orders, yticklabels=ar_orders)\n",
    "plt.title('AIC Values for Different ARMA(p,q) Models')\n",
    "plt.xlabel('MA Order (q)')\n",
    "plt.ylabel('AR Order (p)')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ARMAX Model with Exogenous Variables\n",
    "\n",
    "Now let's create an ARMAX model by adding exogenous variables. We'll create some synthetic exogenous variables for this example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create exogenous variables\n",
    "np.random.seed(123)\n",
    "exog1 = np.sin(np.linspace(0, 10, len(ts_data)))  # Seasonal component\n",
    "exog2 = np.random.normal(0, 1, len(ts_data))      # Random component\n",
    "\n",
    "# Create DataFrame with exogenous variables\n",
    "exog_data = pd.DataFrame({\n",
    "    'seasonal': exog1,\n",
    "    'random': exog2\n",
    "}, index=ts_data.index)\n",
    "\n",
    "# Plot exogenous variables\n",
    "plt.figure(figsize=(14, 8))\n",
    "\n",
    "plt.subplot(2, 1, 1)\n",
    "plt.plot(exog_data.index, exog_data['seasonal'])\n",
    "plt.title('Exogenous Variable 1: Seasonal Component')\n",
    "plt.xlabel('Date')\n",
    "\n",
    "plt.subplot(2, 1, 2)\n",
    "plt.plot(exog_data.index, exog_data['random'])\n",
    "plt.title('Exogenous Variable 2: Random Component')\n",
    "plt.xlabel('Date')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Create and fit an ARMAX model\n",
    "armax_model = ARMA(\n",
    "    ar_order=best_aic['ar_order'], \n",
    "    ma_order=best_aic['ma_order'], \n",
    "    include_constant=True,\n",
    "    exog=exog_data  # Pass exogenous variables\n",
    ")\n",
    "\n",
    "armax_result = armax_model.fit(ts_data)\n",
    "\n",
    "# Print model summary\n",
    "print(armax_result.summary())\n",
    "\n",
    "# Access model parameters\n",
    "constant = armax_result.params.constant\n",
    "ar_params = armax_result.params.ar\n",
    "ma_params = armax_result.params.ma\n",
    "exog_params = armax_result.params.exog  # Array of exogenous variable coefficients\n",
    "\n",
    "print(f\"\\nEstimated Parameters:\")\n",
    "print(f\"Constant: {constant:.4f}\")\n",
    "print(f\"AR parameters: {ar_params}\")\n",
    "print(f\"MA parameters: {ma_params}\")\n",
    "print(f\"Exogenous parameters: {exog_params}\")\n",
    "\n",
    "# Compare ARMA and ARMAX models\n",
    "print(f\"\\nModel Comparison:\")\n",
    "print(f\"ARMA AIC: {best_aic['aic']:.4f}, BIC: {best_aic['bic']:.4f}\")\n",
    "print(f\"ARMAX AIC: {armax_result.aic:.4f}, BIC: {armax_result.bic:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Forecasting\n",
    "\n",
    "Now let's generate forecasts using our ARMA and ARMAX models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate forecasts from the ARMA model\n",
    "forecast_horizon = 30\n",
    "arma_forecasts = best_aic['result'].forecast(horizon=forecast_horizon)\n",
    "\n",
    "# Create forecast dates (business days after the last data point)\n",
    "forecast_dates = pd.date_range(\n",
    "    start=ts_data.index[-1] + pd.Timedelta(days=1), \n",
    "    periods=forecast_horizon, \n",
    "    freq='B'\n",
    ")\n",
    "\n",
    "# Create a DataFrame for the ARMA forecasts\n",
    "arma_forecast_df = pd.DataFrame({\n",
    "    'point_forecast': arma_forecasts.mean,\n",
    "    'lower_ci': arma_forecasts.lower_ci,\n",
    "    'upper_ci': arma_forecasts.upper_ci\n",
    "}, index=forecast_dates)\n",
    "\n",
    "# Generate forecasts from the ARMAX model\n",
    "# For ARMAX forecasting, we need future values of exogenous variables\n",
    "# Here we'll create simple projections\n",
    "future_exog = pd.DataFrame({\n",
    "    'seasonal': np.sin(np.linspace(10, 12, forecast_horizon)),  # Continue the sine wave\n",
    "    'random': np.random.normal(0, 1, forecast_horizon)          # New random values\n",
    "}, index=forecast_dates)\n",
    "\n",
    "armax_forecasts = armax_result.forecast(horizon=forecast_horizon, exog_future=future_exog)\n",
    "\n",
    "# Create a DataFrame for the ARMAX forecasts\n",
    "armax_forecast_df = pd.DataFrame({\n",
    "    'point_forecast': armax_forecasts.mean,\n",
    "    'lower_ci': armax_forecasts.lower_ci,\n",
    "    'upper_ci': armax_forecasts.upper_ci\n",
    "}, index=forecast_dates)\n",
    "\n",
    "# Plot the historical data and forecasts\n",
    "plt.figure(figsize=(14, 8))\n",
    "\n",
    "# Plot historical data (last 90 days)\n",
    "plt.plot(ts_data.index[-90:], ts_data.values[-90:], label='Historical Data', color='blue')\n",
    "\n",
    "# Plot ARMA forecasts\n",
    "plt.plot(arma_forecast_df.index, arma_forecast_df['point_forecast'], \n",
    "         label='ARMA Forecast', color='red', linestyle='--')\n",
    "plt.fill_between(\n",
    "    arma_forecast_df.index, \n",
    "    arma_forecast_df['lower_ci'], \n",
    "    arma_forecast_df['upper_ci'], \n",
    "    color='red', alpha=0.2, label='ARMA 95% CI'\n",
    ")\n",
    "\n",
    "# Plot ARMAX forecasts\n",
    "plt.plot(armax_forecast_df.index, armax_forecast_df['point_forecast'], \n",
    "         label='ARMAX Forecast', color='green', linestyle='--')\n",
    "plt.fill_between(\n",
    "    armax_forecast_df.index, \n",
    "    armax_forecast_df['lower_ci'], \n",
    "    armax_forecast_df['upper_ci'], \n",
    "    color='green', alpha=0.2, label='ARMAX 95% CI'\n",
    ")\n",
    "\n",
    "plt.title('ARMA and ARMAX Forecasts')\n",
    "plt.xlabel('Date')\n",
    "plt.ylabel('Value')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Print forecast values\n",
    "print(\"ARMA Forecasts:\")\n",
    "print(arma_forecast_df.head())\n",
    "\n",
    "print(\"\\nARMAX Forecasts:\")\n",
    "print(armax_forecast_df.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Asynchronous Forecasting\n",
    "\n",
    "For long-horizon forecasts or when generating many simulation paths, the MFE Toolbox provides asynchronous forecasting capabilities using Python's async/await pattern."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define an asynchronous function to generate forecasts\n",
    "async def generate_forecasts():\n",
    "    # Define a progress callback function\n",
    "    def progress_callback(percent, message):\n",
    "        print(f\"{percent:.1f}% complete: {message}\")\n",
    "    \n",
    "    # Generate forecasts asynchronously with many simulation paths\n",
    "    forecasts = await best_aic['result'].forecast_async(\n",
    "        horizon=50,\n",
    "        num_simulations=5000,  # Large number of simulation paths\n",
    "        progress_callback=progress_callback\n",
    "    )\n",
    "    \n",
    "    return forecasts\n",
    "\n",
    "# Run the asynchronous function\n",
    "async_forecasts = await generate_forecasts()\n",
    "\n",
    "# Create forecast dates\n",
    "async_forecast_dates = pd.date_range(\n",
    "    start=ts_data.index[-1] + pd.Timedelta(days=1), \n",
    "    periods=len(async_forecasts.mean), \n",
    "    freq='B'\n",
    ")\n",
    "\n",
    "# Plot the data and forecasts\n",
    "plt.figure(figsize=(14, 8))\n",
    "plt.plot(ts_data.index[-90:], ts_data.values[-90:], label='Historical Data')\n",
    "plt.plot(async_forecast_dates, async_forecasts.mean, label='Point Forecast', color='purple')\n",
    "\n",
    "# Add confidence intervals\n",
    "plt.fill_between(\n",
    "    async_forecast_dates,\n",
    "    async_forecasts.lower_ci,\n",
    "    async_forecasts.upper_ci,\n",
    "    color='purple', alpha=0.2, label='95% Confidence Interval'\n",
    ")\n",
    "\n",
    "# Add prediction intervals from simulation\n",
    "plt.fill_between(\n",
    "    async_forecast_dates,\n",
    "    async_forecasts.prediction_lower,\n",
    "    async_forecasts.prediction_upper,\n",
    "    color='purple', alpha=0.1, label='95% Prediction Interval'\n",
    ")\n",
    "\n",
    "plt.title('ARMA Forecasts with 5,000 Simulation Paths')\n",
    "plt.xlabel('Date')\n",
    "plt.ylabel('Value')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Forecast Evaluation\n",
    "\n",
    "Let's evaluate the forecast accuracy by comparing the forecasts with actual values. We'll use a holdout sample for this purpose."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the data into training and test sets\n",
    "train_end = int(len(ts_data) * 0.8)\n",
    "train_data = ts_data[:train_end]\n",
    "test_data = ts_data[train_end:]\n",
    "\n",
    "# Fit ARMA model on training data\n",
    "eval_model = ARMA(ar_order=best_aic['ar_order'], ma_order=best_aic['ma_order'], include_constant=True)\n",
    "eval_result = eval_model.fit(train_data)\n",
    "\n",
    "# Generate forecasts for the test period\n",
    "eval_forecasts = eval_result.forecast(horizon=len(test_data))\n",
    "\n",
    "# Plot actual vs. forecasted values\n",
    "plt.figure(figsize=(14, 6))\n",
    "plt.plot(test_data.index, test_data.values, label='Actual')\n",
    "plt.plot(test_data.index, eval_forecasts.mean, label='Forecast', linestyle='--')\n",
    "plt.fill_between(\n",
    "    test_data.index,\n",
    "    eval_forecasts.lower_ci,\n",
    "    eval_forecasts.upper_ci,\n",
    "    alpha=0.2, label='95% Confidence Interval'\n",
    ")\n",
    "plt.title('Forecast Evaluation: Actual vs. Predicted')\n",
    "plt.xlabel('Date')\n",
    "plt.ylabel('Value')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Evaluate forecasts\n",
    "evaluation = forecast_evaluation(\n",
    "    actual=test_data.values,\n",
    "    forecast=eval_forecasts.mean,\n",
    "    metrics=['mse', 'mae', 'rmse', 'mape', 'theil_u']\n",
    ")\n",
    "\n",
    "print(\"Forecast Evaluation Metrics:\")\n",
    "for metric, value in evaluation.items():\n",
    "    print(f\"{metric.upper()}: {value:.6f}\")\n",
    "\n",
    "# Calculate forecast errors\n",
    "forecast_errors = test_data.values - eval_forecasts.mean\n",
    "\n",
    "# Plot forecast errors\n",
    "plt.figure(figsize=(14, 6))\n",
    "plt.plot(test_data.index, forecast_errors)\n",
    "plt.axhline(y=0, color='r', linestyle='--')\n",
    "plt.title('Forecast Errors')\n",
    "plt.xlabel('Date')\n",
    "plt.ylabel('Error')\n",
    "plt.grid(True)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Plot histogram of forecast errors\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.hist(forecast_errors, bins=30, density=True, alpha=0.7)\n",
    "plt.title('Distribution of Forecast Errors')\n",
    "plt.xlabel('Error')\n",
    "plt.ylabel('Density')\n",
    "plt.grid(True)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Time Series Decomposition and Filtering\n",
    "\n",
    "Let's decompose our time series into trend, seasonal, and irregular components using various filtering techniques."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply Hodrick-Prescott filter\n",
    "hp_result = hp_filter(ts_data.values, lamb=1600)  # 1600 is standard for quarterly data\n",
    "\n",
    "# Plot HP filter results\n",
    "plt.figure(figsize=(14, 10))\n",
    "\n",
    "plt.subplot(3, 1, 1)\n",
    "plt.plot(ts_data.index, ts_data.values, label='Original Data')\n",
    "plt.plot(ts_data.index, hp_result['trend'], label='HP Trend', linewidth=2)\n",
    "plt.title('Original Data and HP Trend')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "\n",
    "plt.subplot(3, 1, 2)\n",
    "plt.plot(ts_data.index, hp_result['trend'], label='HP Trend')\n",
    "plt.title('HP Trend Component')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "\n",
    "plt.subplot(3, 1, 3)\n",
    "plt.plot(ts_data.index, hp_result['cycle'], label='HP Cycle')\n",
    "plt.title('HP Cyclical Component')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply Baxter-King filter\n",
    "bk_result = bk_filter(\n",
    "    ts_data.values,\n",
    "    low=6,    # 6 periods for lower cutoff (e.g., 1.5 years for quarterly data)\n",
    "    high=32,  # 32 periods for upper cutoff (e.g., 8 years for quarterly data)\n",
    "    K=12      # Lead-lag length\n",
    ")\n",
    "\n",
    "# Create index for BK filter results (accounting for truncated ends)\n",
    "bk_index = ts_data.index[12:-12]\n",
    "\n",
    "# Plot BK filter results\n",
    "plt.figure(figsize=(14, 8))\n",
    "\n",
    "plt.subplot(2, 1, 1)\n",
    "plt.plot(ts_data.index, ts_data.values, label='Original Data')\n",
    "plt.plot(bk_index, bk_result['trend'], label='BK Trend', linewidth=2)\n",
    "plt.title('Original Data and BK Trend')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "\n",
    "plt.subplot(2, 1, 2)\n",
    "plt.plot(bk_index, bk_result['cycle'], label='BK Cycle')\n",
    "plt.title('BK Cyclical Component')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Seasonal decomposition using statsmodels\n",
    "from statsmodels.tsa.seasonal import seasonal_decompose\n",
    "\n",
    "# Create data with stronger seasonality for better visualization\n",
    "seasonal_data = pd.Series(\n",
    "    ts_data.values + 2 * np.sin(np.arange(len(ts_data)) * 2 * np.pi / 5),  # Amplify the 5-day seasonality\n",
    "    index=ts_data.index\n",
    ")\n",
    "\n",
    "# Decompose the series\n",
    "decomposition = seasonal_decompose(seasonal_data, model='additive', period=5)  # 5-day business week\n",
    "\n",
    "# Plot the decomposition\n",
    "plt.figure(figsize=(14, 12))\n",
    "\n",
    "plt.subplot(4, 1, 1)\n",
    "plt.plot(decomposition.observed)\n",
    "plt.title('Original Data')\n",
    "plt.grid(True)\n",
    "\n",
    "plt.subplot(4, 1, 2)\n",
    "plt.plot(decomposition.trend)\n",
    "plt.title('Trend Component')\n",
    "plt.grid(True)\n",
    "\n",
    "plt.subplot(4, 1, 3)\n",
    "plt.plot(decomposition.seasonal)\n",
    "plt.title('Seasonal Component')\n",
    "plt.grid(True)\n",
    "\n",
    "plt.subplot(4, 1, 4)\n",
    "plt.plot(decomposition.resid)\n",
    "plt.title('Residual Component')\n",
    "plt.grid(True)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Vector Autoregression (VAR) Analysis\n",
    "\n",
    "Let's create a multivariate time series and fit a VAR model to capture the interdependencies between variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a VAR(1) process\n",
    "np.random.seed(42)\n",
    "n = 500\n",
    "\n",
    "# Define coefficient matrix\n",
    "A = np.array([[0.5, 0.1], \n",
    "              [0.2, 0.3]])  # 2x2 coefficient matrix\n",
    "\n",
    "# Initialize data\n",
    "y = np.zeros((n, 2))\n",
    "\n",
    "# Generate VAR(1) process\n",
    "for t in range(1, n):\n",
    "    y[t] = A @ y[t-1] + np.random.multivariate_normal([0, 0], np.eye(2))\n",
    "\n",
    "# Create DataFrame with time index\n",
    "var_dates = pd.date_range(start='2018-01-01', periods=n, freq='B')\n",
    "var_data = pd.DataFrame(y, index=var_dates, columns=['y1', 'y2'])\n",
    "\n",
    "# Plot the multivariate time series\n",
    "plt.figure(figsize=(14, 8))\n",
    "\n",
    "plt.subplot(2, 1, 1)\n",
    "plt.plot(var_data.index, var_data['y1'])\n",
    "plt.title('Variable 1')\n",
    "plt.grid(True)\n",
    "\n",
    "plt.subplot(2, 1, 2)\n",
    "plt.plot(var_data.index, var_data['y2'])\n",
    "plt.title('Variable 2')\n",
    "plt.grid(True)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Create and fit a VAR model\n",
    "var_model = VAR(lag_order=1)\n",
    "var_result = var_model.fit(var_data)\n",
    "\n",
    "# Print model summary\n",
    "print(var_result.summary())\n",
    "\n",
    "# Access coefficient matrices\n",
    "coef_matrix = var_result.coef_matrix\n",
    "print(\"\\nEstimated Coefficient Matrix:\")\n",
    "print(coef_matrix)\n",
    "\n",
    "print(\"\\nTrue Coefficient Matrix:\")\n",
    "print(A)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Impulse Response Analysis\n",
    "\n",
    "Let's analyze how variables respond to shocks using impulse response functions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute impulse responses\n",
    "irf = impulse_response(\n",
    "    coef_matrices=var_result.coef_matrices,\n",
    "    horizon=20,\n",
    "    identification='cholesky'  # Cholesky decomposition for identification\n",
    ")\n",
    "\n",
    "# Plot impulse responses\n",
    "plt.figure(figsize=(14, 10))\n",
    "\n",
    "# Response of y1 to shocks\n",
    "plt.subplot(2, 2, 1)\n",
    "plt.plot(irf[:, 0, 0], label='Response of y1 to y1 shock')\n",
    "plt.axhline(y=0, color='r', linestyle='--')\n",
    "plt.title('Response of y1 to y1 shock')\n",
    "plt.xlabel('Horizon')\n",
    "plt.ylabel('Response')\n",
    "plt.grid(True)\n",
    "\n",
    "plt.subplot(2, 2, 2)\n",
    "plt.plot(irf[:, 0, 1], label='Response of y1 to y2 shock')\n",
    "plt.axhline(y=0, color='r', linestyle='--')\n",
    "plt.title('Response of y1 to y2 shock')\n",
    "plt.xlabel('Horizon')\n",
    "plt.ylabel('Response')\n",
    "plt.grid(True)\n",
    "\n",
    "# Response of y2 to shocks\n",
    "plt.subplot(2, 2, 3)\n",
    "plt.plot(irf[:, 1, 0], label='Response of y2 to y1 shock')\n",
    "plt.axhline(y=0, color='r', linestyle='--')\n",
    "plt.title('Response of y2 to y1 shock')\n",
    "plt.xlabel('Horizon')\n",
    "plt.ylabel('Response')\n",
    "plt.grid(True)\n",
    "\n",
    "plt.subplot(2, 2, 4)\n",
    "plt.plot(irf[:, 1, 1], label='Response of y2 to y2 shock')\n",
    "plt.axhline(y=0, color='r', linestyle='--')\n",
    "plt.title('Response of y2 to y2 shock')\n",
    "plt.xlabel('Horizon')\n",
    "plt.ylabel('Response')\n",
    "plt.grid(True)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Granger Causality\n",
    "\n",
    "Let's test for Granger causality between the variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test Granger causality\n",
    "gc_y1_to_y2 = granger_causality(var_data['y1'], var_data['y2'], max_lag=5)\n",
    "gc_y2_to_y1 = granger_causality(var_data['y2'], var_data['y1'], max_lag=5)\n",
    "\n",
    "print(\"Granger Causality Tests:\")\n",
    "print(\"\\ny1 -> y2:\")\n",
    "for lag, result in gc_y1_to_y2.items():\n",
    "    print(f\"Lag {lag}: F-stat = {result['f_stat']:.4f}, p-value = {result['p_value']:.4f}\")\n",
    "\n",
    "print(\"\\ny2 -> y1:\")\n",
    "for lag, result in gc_y2_to_y1.items():\n",
    "    print(f\"Lag {lag}: F-stat = {result['f_stat']:.4f}, p-value = {result['p_value']:.4f}\")\n",
    "\n",
    "# Create a heatmap of p-values\n",
    "p_values_y1_to_y2 = [result['p_value'] for lag, result in sorted(gc_y1_to_y2.items())]\n",
    "p_values_y2_to_y1 = [result['p_value'] for lag, result in sorted(gc_y2_to_y1.items())]\n",
    "\n",
    "p_value_matrix = np.array([p_values_y1_to_y2, p_values_y2_to_y1])\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.heatmap(p_value_matrix, annot=True, fmt='.4f', cmap='viridis_r',\n",
    "            xticklabels=range(1, 6), yticklabels=['y1 -> y2', 'y2 -> y1'])\n",
    "plt.title('Granger Causality p-values')\n",
    "plt.xlabel('Lag Order')\n",
    "plt.ylabel('Causality Direction')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### VAR Forecasting\n",
    "\n",
    "Let's generate forecasts from our VAR model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate forecasts\n",
    "var_forecasts = var_result.forecast(horizon=20)\n",
    "\n",
    "# Create forecast dates\n",
    "var_forecast_dates = pd.date_range(\n",
    "    start=var_data.index[-1] + pd.Timedelta(days=1), \n",
    "    periods=20, \n",
    "    freq='B'\n",
    ")\n",
    "\n",
    "# Plot the data and forecasts\n",
    "plt.figure(figsize=(14, 10))\n",
    "\n",
    "plt.subplot(2, 1, 1)\n",
    "plt.plot(var_data.index[-60:], var_data['y1'][-60:], label='y1 Data')\n",
    "plt.plot(\n",
    "    var_forecast_dates,\n",
    "    var_forecasts.mean[:, 0],\n",
    "    label='y1 Forecast',\n",
    "    color='red',\n",
    "    linestyle='--'\n",
    ")\n",
    "plt.fill_between(\n",
    "    var_forecast_dates,\n",
    "    var_forecasts.lower_ci[:, 0],\n",
    "    var_forecasts.upper_ci[:, 0],\n",
    "    color='red', alpha=0.2, label='95% CI'\n",
    ")\n",
    "plt.title('VAR(1) Model: y1 Forecast')\n",
    "plt.xlabel('Date')\n",
    "plt.ylabel('Value')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "\n",
    "plt.subplot(2, 1, 2)\n",
    "plt.plot(var_data.index[-60:], var_data['y2'][-60:], label='y2 Data')\n",
    "plt.plot(\n",
    "    var_forecast_dates,\n",
    "    var_forecasts.mean[:, 1],\n",
    "    label='y2 Forecast',\n",
    "    color='blue',\n",
    "    linestyle='--'\n",
    ")\n",
    "plt.fill_between(\n",
    "    var_forecast_dates,\n",
    "    var_forecasts.lower_ci[:, 1],\n",
    "    var_forecasts.upper_ci[:, 1],\n",
    "    color='blue', alpha=0.2, label='95% CI'\n",
    ")\n",
    "plt.title('VAR(1) Model: y2 Forecast')\n",
    "plt.xlabel('Date')\n",
    "plt.ylabel('Value')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Heterogeneous Autoregression (HAR) Models\n",
    "\n",
    "HAR models are particularly useful for modeling realized volatility. Let's create a simulated realized volatility series and fit a HAR model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a persistent series similar to realized volatility\n",
    "np.random.seed(42)\n",
    "n = 500\n",
    "rv = np.zeros(n)\n",
    "rv[0] = 0.1\n",
    "\n",
    "# Generate a HAR-like process\n",
    "for t in range(1, n):\n",
    "    # Daily, weekly, and monthly components\n",
    "    daily = 0.4 * rv[t-1]\n",
    "    weekly = 0.3 * np.mean(rv[max(0, t-5):t])  # 5-day average (weekly)\n",
    "    monthly = 0.2 * np.mean(rv[max(0, t-22):t])  # 22-day average (monthly)\n",
    "    \n",
    "    rv[t] = 0.1 + daily + weekly + monthly + np.random.normal(0, 0.05)\n",
    "\n",
    "# Create DataFrame with time index\n",
    "rv_dates = pd.date_range(start='2018-01-01', periods=n, freq='B')\n",
    "rv_data = pd.Series(rv, index=rv_dates, name='realized_volatility')\n",
    "\n",
    "# Plot the realized volatility series\n",
    "plt.figure(figsize=(14, 6))\n",
    "plt.plot(rv_data.index, rv_data.values)\n",
    "plt.title('Simulated Realized Volatility')\n",
    "plt.xlabel('Date')\n",
    "plt.ylabel('Realized Volatility')\n",
    "plt.grid(True)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Create and fit a HAR model\n",
    "har_model = HAR(lags=[1, 5, 22])  # Daily, weekly, monthly components\n",
    "har_result = har_model.fit(rv_data)\n",
    "\n",
    "# Print model summary\n",
    "print(har_result.summary())\n",
    "\n",
    "# Access model parameters\n",
    "constant = har_result.params.constant\n",
    "beta_d = har_result.params.beta[0]  # Daily coefficient\n",
    "beta_w = har_result.params.beta[1]  # Weekly coefficient\n",
    "beta_m = har_result.params.beta[2]  # Monthly coefficient\n",
    "\n",
    "print(f\"\\nEstimated Parameters:\")\n",
    "print(f\"Constant: {constant:.4f}\")\n",
    "print(f\"Daily coefficient: {beta_d:.4f}\")\n",
    "print(f\"Weekly coefficient: {beta_w:.4f}\")\n",
    "print(f\"Monthly coefficient: {beta_m:.4f}\")\n",
    "\n",
    "print(f\"\\nTrue Parameters:\")\n",
    "print(f\"Constant: 0.1000\")\n",
    "print(f\"Daily coefficient: 0.4000\")\n",
    "print(f\"Weekly coefficient: 0.3000\")\n",
    "print(f\"Monthly coefficient: 0.2000\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### HAR Model Forecasting\n",
    "\n",
    "Let's generate forecasts from our HAR model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate forecasts\n",
    "har_forecasts = har_result.forecast(horizon=30)\n",
    "\n",
    "# Create forecast dates\n",
    "har_forecast_dates = pd.date_range(\n",
    "    start=rv_data.index[-1] + pd.Timedelta(days=1), \n",
    "    periods=30, \n",
    "    freq='B'\n",
    ")\n",
    "\n",
    "# Plot the data and forecasts\n",
    "plt.figure(figsize=(14, 6))\n",
    "plt.plot(rv_data.index[-60:], rv_data.values[-60:], label='Realized Volatility')\n",
    "plt.plot(\n",
    "    har_forecast_dates,\n",
    "    har_forecasts.mean,\n",
    "    label='HAR Forecast',\n",
    "    color='red',\n",
    "    linestyle='--'\n",
    ")\n",
    "plt.fill_between(\n",
    "    har_forecast_dates,\n",
    "    har_forecasts.lower_ci,\n",
    "    har_forecasts.upper_ci,\n",
    "    color='red', alpha=0.2, label='95% CI'\n",
    ")\n",
    "plt.title('HAR Model Forecast of Realized Volatility')\n",
    "plt.xlabel('Date')\n",
    "plt.ylabel('Realized Volatility')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Integration with Pandas Time Series Functionality\n",
    "\n",
    "The MFE Toolbox integrates seamlessly with Pandas time series functionality. Let's demonstrate some of these integrations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a DataFrame with actual data and forecasts\n",
    "combined_data = pd.DataFrame({\n",
    "    'actual': ts_data,\n",
    "    'fitted': pd.Series(arma_result.fitted_values, index=ts_data.index)\n",
    "})\n",
    "\n",
    "# Add forecasts\n",
    "forecast_df = pd.DataFrame({\n",
    "    'forecast': arma_forecasts.mean,\n",
    "    'lower_ci': arma_forecasts.lower_ci,\n",
    "    'upper_ci': arma_forecasts.upper_ci\n",
    "}, index=forecast_dates)\n",
    "\n",
    "# Combine actual data and forecasts\n",
    "full_df = pd.concat([combined_data, forecast_df], axis=1)\n",
    "\n",
    "# Resample to monthly frequency\n",
    "monthly_data = full_df.resample('M').mean()\n",
    "\n",
    "# Plot monthly data\n",
    "plt.figure(figsize=(14, 6))\n",
    "plt.plot(monthly_data.index, monthly_data['actual'], label='Actual (Monthly)')\n",
    "plt.plot(monthly_data.index, monthly_data['fitted'], label='Fitted (Monthly)')\n",
    "plt.plot(monthly_data.index, monthly_data['forecast'], label='Forecast (Monthly)', color='red')\n",
    "plt.fill_between(\n",
    "    monthly_data.index,\n",
    "    monthly_data['lower_ci'],\n",
    "    monthly_data['upper_ci'],\n",
    "    color='red', alpha=0.2, label='95% Confidence Interval'\n",
    ")\n",
    "plt.title('ARMA Model with Monthly Resampling')\n",
    "plt.xlabel('Date')\n",
    "plt.ylabel('Value')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Perform time series operations with Pandas\n",
    "print(\"Time Series Statistics:\")\n",
    "print(f\"Rolling 20-day mean at end: {ts_data.rolling(window=20).mean().iloc[-1]:.4f}\")\n",
    "print(f\"Expanding mean at end: {ts_data.expanding().mean().iloc[-1]:.4f}\")\n",
    "print(f\"Year-to-date mean: {ts_data[ts_data.index.year == ts_data.index[-1].year].mean():.4f}\")\n",
    "\n",
    "# Create interactive visualization with Plotly\n",
    "try:\n",
    "    import plotly.express as px\n",
    "    import plotly.graph_objects as go\n",
    "    from plotly.subplots import make_subplots\n",
    "    \n",
    "    # Create a DataFrame for plotting\n",
    "    plot_df = pd.DataFrame({\n",
    "        'Actual': ts_data[-90:],\n",
    "        'Fitted': pd.Series(arma_result.fitted_values[-90:], index=ts_data.index[-90:])\n",
    "    })\n",
    "    \n",
    "    # Add forecasts\n",
    "    forecast_plot_df = pd.DataFrame({\n",
    "        'Forecast': arma_forecasts.mean,\n",
    "        'Lower CI': arma_forecasts.lower_ci,\n",
    "        'Upper CI': arma_forecasts.upper_ci\n",
    "    }, index=forecast_dates)\n",
    "    \n",
    "    # Create interactive plot\n",
    "    fig = make_subplots(rows=1, cols=1, shared_xaxes=True)\n",
    "    \n",
    "    # Add actual data\n",
    "    fig.add_trace(\n",
    "        go.Scatter(x=plot_df.index, y=plot_df['Actual'], name='Actual', line=dict(color='blue'))\n",
    "    )\n",
    "    \n",
    "    # Add fitted values\n",
    "    fig.add_trace(\n",
    "        go.Scatter(x=plot_df.index, y=plot_df['Fitted'], name='Fitted', line=dict(color='green'))\n",
    "    )\n",
    "    \n",
    "    # Add forecast\n",
    "    fig.add_trace(\n",
    "        go.Scatter(x=forecast_plot_df.index, y=forecast_plot_df['Forecast'], name='Forecast', \n",
    "                  line=dict(color='red', dash='dash'))\n",
    "    )\n",
    "    \n",
    "    # Add confidence interval\n",
    "    fig.add_trace(\n",
    "        go.Scatter(\n",
    "            x=forecast_plot_df.index.tolist() + forecast_plot_df.index.tolist()[::-1],\n",
    "            y=forecast_plot_df['Upper CI'].tolist() + forecast_plot_df['Lower CI'].tolist()[::-1],\n",
    "            fill='toself',\n",
    "            fillcolor='rgba(255,0,0,0.2)',\n",
    "            line=dict(color='rgba(255,0,0,0)'),\n",
    "            name='95% Confidence Interval'\n",
    "        )\n",
    "    )\n",
    "    \n",
    "    # Update layout\n",
    "    fig.update_layout(\n",
    "        title='ARMA Model Forecast with Interactive Visualization',\n",
    "        xaxis_title='Date',\n",
    "        yaxis_title='Value',\n",
    "        legend_title='Series',\n",
    "        hovermode='x unified'\n",
    "    )\n",
    "    \n",
    "    fig.show()\n",
    "    \n",
    "except ImportError:\n",
    "    print(\"Plotly is not installed. Install it with 'pip install plotly' for interactive visualizations.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Real-World Application: Economic Indicator Analysis\n",
    "\n",
    "Let's apply our time series analysis tools to a real-world economic indicator. We'll use the FRED API to fetch U.S. unemployment rate data and analyze it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Try to import pandas_datareader for FRED data\n",
    "try:\n",
    "    import pandas_datareader as pdr\n",
    "    \n",
    "    # Fetch U.S. unemployment rate data from FRED\n",
    "    start_date = '2000-01-01'\n",
    "    end_date = '2023-12-31'\n",
    "    unemployment = pdr.fred.FredReader('UNRATE', start=start_date, end=end_date).read()\n",
    "    \n",
    "    # Plot the unemployment rate\n",
    "    plt.figure(figsize=(14, 6))\n",
    "    plt.plot(unemployment.index, unemployment['UNRATE'])\n",
    "    plt.title('U.S. Unemployment Rate')\n",
    "    plt.xlabel('Date')\n",
    "    plt.ylabel('Unemployment Rate (%)')\n",
    "    plt.grid(True)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    # Test for stationarity\n",
    "    adf_result = adf_test(unemployment['UNRATE'])\n",
    "    print(\"Augmented Dickey-Fuller Test:\")\n",
    "    print(f\"Test Statistic: {adf_result['adf_stat']:.4f}\")\n",
    "    print(f\"p-value: {adf_result['p_value']:.4f}\")\n",
    "    print(f\"Is Stationary: {adf_result['is_stationary']}\")\n",
    "    \n",
    "    # If non-stationary, take first difference\n",
    "    if not adf_result['is_stationary']:\n",
    "        unemployment_diff = unemployment['UNRATE'].diff().dropna()\n",
    "        \n",
    "        # Plot the differenced series\n",
    "        plt.figure(figsize=(14, 6))\n",
    "        plt.plot(unemployment_diff.index, unemployment_diff.values)\n",
    "        plt.title('Change in U.S. Unemployment Rate')\n",
    "        plt.xlabel('Date')\n",
    "        plt.ylabel('Change in Unemployment Rate (%)')\n",
    "        plt.grid(True)\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "        \n",
    "        # Test differenced series for stationarity\n",
    "        adf_diff_result = adf_test(unemployment_diff)\n",
    "        print(\"\\nAugmented Dickey-Fuller Test (Differenced Series):\")\n",
    "        print(f\"Test Statistic: {adf_diff_result['adf_stat']:.4f}\")\n",
    "        print(f\"p-value: {adf_diff_result['p_value']:.4f}\")\n",
    "        print(f\"Is Stationary: {adf_diff_result['is_stationary']}\")\n",
    "        \n",
    "        # Use the differenced series for modeling\n",
    "        modeling_data = unemployment_diff\n",
    "    else:\n",
    "        # Use the original series for modeling\n",
    "        modeling_data = unemployment['UNRATE']\n",
    "    \n",
    "    # Plot ACF and PACF\n",
    "    fig, (ax1, ax2) = plt.subplots(2, 1, figsize=(14, 10))\n",
    "    plot_acf(modeling_data, lags=24, ax=ax1)\n",
    "    ax1.set_title('Autocorrelation Function (ACF)')\n",
    "    plot_pacf(modeling_data, lags=24, ax=ax2)\n",
    "    ax2.set_title('Partial Autocorrelation Function (PACF)')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    # Fit an ARMA model\n",
    "    # Choose orders based on ACF/PACF\n",
    "    arma_model = ARMA(ar_order=2, ma_order=1, include_constant=True)\n",
    "    arma_result = arma_model.fit(modeling_data)\n",
    "    \n",
    "    # Print model summary\n",
    "    print(arma_result.summary())\n",
    "    \n",
    "    # Generate forecasts\n",
    "    forecast_horizon = 12  # 12 months\n",
    "    forecasts = arma_result.forecast(horizon=forecast_horizon)\n",
    "    \n",
    "    # Create forecast dates\n",
    "    last_date = modeling_data.index[-1]\n",
    "    forecast_dates = pd.date_range(\n",
    "        start=last_date + pd.DateOffset(months=1), \n",
    "        periods=forecast_horizon, \n",
    "        freq='MS'  # Month start frequency\n",
    "    )\n",
    "    \n",
    "    # If we used differenced data, we need to convert forecasts back to levels\n",
    "    if not adf_result['is_stationary']:\n",
    "        # Get the last level value\n",
    "        last_level = unemployment['UNRATE'].iloc[-1]\n",
    "        \n",
    "        # Convert forecasts to levels\n",
    "        level_forecasts = np.zeros(forecast_horizon)\n",
    "        level_lower_ci = np.zeros(forecast_horizon)\n",
    "        level_upper_ci = np.zeros(forecast_horizon)\n",
    "        \n",
    "        level_forecasts[0] = last_level + forecasts.mean[0]\n",
    "        level_lower_ci[0] = last_level + forecasts.lower_ci[0]\n",
    "        level_upper_ci[0] = last_level + forecasts.upper_ci[0]\n",
    "        \n",
    "        for i in range(1, forecast_horizon):\n",
    "            level_forecasts[i] = level_forecasts[i-1] + forecasts.mean[i]\n",
    "            level_lower_ci[i] = level_forecasts[i-1] + forecasts.lower_ci[i]\n",
    "            level_upper_ci[i] = level_forecasts[i-1] + forecasts.upper_ci[i]\n",
    "        \n",
    "        # Create a DataFrame for the level forecasts\n",
    "        forecast_df = pd.DataFrame({\n",
    "            'forecast': level_forecasts,\n",
    "            'lower_ci': level_lower_ci,\n",
    "            'upper_ci': level_upper_ci\n",
    "        }, index=forecast_dates)\n",
    "        \n",
    "        # Plot the original data and level forecasts\n",
    "        plt.figure(figsize=(14, 6))\n",
    "        plt.plot(unemployment.index, unemployment['UNRATE'], label='Historical Data')\n",
    "        plt.plot(forecast_df.index, forecast_df['forecast'], label='Forecast', color='red', linestyle='--')\n",
    "        plt.fill_between(\n",
    "            forecast_df.index,\n",
    "            forecast_df['lower_ci'],\n",
    "            forecast_df['upper_ci'],\n",
    "            color='red', alpha=0.2, label='95% Confidence Interval'\n",
    "        )\n",
    "        plt.title('U.S. Unemployment Rate Forecast')\n",
    "        plt.xlabel('Date')\n",
    "        plt.ylabel('Unemployment Rate (%)')\n",
    "        plt.legend()\n",
    "        plt.grid(True)\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "    else:\n",
    "        # Create a DataFrame for the forecasts\n",
    "        forecast_df = pd.DataFrame({\n",
    "            'forecast': forecasts.mean,\n",
    "            'lower_ci': forecasts.lower_ci,\n",
    "            'upper_ci': forecasts.upper_ci\n",
    "        }, index=forecast_dates)\n",
    "        \n",
    "        # Plot the original data and forecasts\n",
    "        plt.figure(figsize=(14, 6))\n",
    "        plt.plot(modeling_data.index, modeling_data.values, label='Historical Data')\n",
    "        plt.plot(forecast_df.index, forecast_df['forecast'], label='Forecast', color='red', linestyle='--')\n",
    "        plt.fill_between(\n",
    "            forecast_df.index,\n",
    "            forecast_df['lower_ci'],\n",
    "            forecast_df['upper_ci'],\n",
    "            color='red', alpha=0.2, label='95% Confidence Interval'\n",
    "        )\n",
    "        plt.title('U.S. Unemployment Rate Forecast')\n",
    "        plt.xlabel('Date')\n",
    "        plt.ylabel('Unemployment Rate (%)')\n",
    "        plt.legend()\n",
    "        plt.grid(True)\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "    \n",
    "except ImportError:\n",
    "    print(\"pandas_datareader is not installed. Install it with 'pip install pandas_datareader' to fetch FRED data.\")\n",
    "    print(\"Using simulated data instead.\")\n",
    "    \n",
    "    # Create simulated unemployment data\n",
    "    dates = pd.date_range(start='2000-01-01', end='2023-12-31', freq='MS')\n",
    "    n = len(dates)\n",
    "    \n",
    "    # Create a trend with cycles and noise\n",
    "    trend = 5 + 0.01 * np.arange(n)  # Slight upward trend\n",
    "    cycle = 2 * np.sin(2 * np.pi * np.arange(n) / 48)  # 4-year cycle\n",
    "    noise = np.random.normal(0, 0.2, n)\n",
    "    \n",
    "    # Add recession spikes\n",
    "    recession_2001 = np.zeros(n)\n",
    "    recession_2008 = np.zeros(n)\n",
    "    recession_2020 = np.zeros(n)\n",
    "    \n",
    "    # 2001 recession\n",
    "    recession_start = np.where(dates >= pd.Timestamp('2001-03-01'))[0][0]\n",
    "    for i in range(12):\n",
    "        if recession_start + i < n:\n",
    "            recession_2001[recession_start + i] = 0.1 * i if i < 6 else 0.1 * (12 - i)\n",
    "    \n",
    "    # 2008 recession\n",
    "    recession_start = np.where(dates >= pd.Timestamp('2008-01-01'))[0][0]\n",
    "    for i in range(24):\n",
    "        if recession_start + i < n:\n",
    "            recession_2008[recession_start + i] = 0.2 * i if i < 12 else 0.2 * (24 - i)\n",
    "    \n",
    "    # 2020 COVID recession\n",
    "    recession_start = np.where(dates >= pd.Timestamp('2020-03-01'))[0][0]\n",
    "    for i in range(6):\n",
    "        if recession_start + i < n:\n",
    "            recession_2020[recession_start + i] = 3 if i < 2 else 3 - 0.5 * (i - 1)\n",
    "    \n",
    "    # Combine components\n",
    "    unemployment = trend + cycle + recession_2001 + recession_2008 + recession_2020 + noise\n",
    "    \n",
    "    # Create a Pandas Series\n",
    "    unemployment_series = pd.Series(unemployment, index=dates, name='UNRATE')\n",
    "    \n",
    "    # Plot the unemployment rate\n",
    "    plt.figure(figsize=(14, 6))\n",
    "    plt.plot(unemployment_series.index, unemployment_series.values)\n",
    "    plt.title('Simulated U.S. Unemployment Rate')\n",
    "    plt.xlabel('Date')\n",
    "    plt.ylabel('Unemployment Rate (%)')\n",
    "    plt.grid(True)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    # Continue with analysis as above..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "In this notebook, we've explored the time series analysis capabilities of the MFE Toolbox. We've covered:\n",
    "\n",
    "1. **Time Series Data Exploration**: Generating and analyzing time series data, including ACF/PACF analysis and stationarity testing.\n",
    "\n",
    "2. **ARMA/ARMAX Modeling**: Fitting ARMA and ARMAX models to time series data, model selection, and diagnostic checking.\n",
    "\n",
    "3. **Forecasting**: Generating forecasts from time series models, including asynchronous forecasting for long-horizon predictions.\n",
    "\n",
    "4. **Time Series Decomposition**: Decomposing time series into trend, seasonal, and irregular components using various filtering techniques.\n",
    "\n",
    "5. **Vector Autoregression (VAR)**: Modeling multivariate time series, impulse response analysis, and Granger causality testing.\n",
    "\n",
    "6. **Heterogeneous Autoregression (HAR)**: Modeling persistent time series like realized volatility.\n",
    "\n",
    "7. **Integration with Pandas**: Leveraging Pandas time series functionality for resampling, rolling statistics, and visualization.\n",
    "\n",
    "8. **Real-World Application**: Applying time series analysis to economic indicators.\n",
    "\n",
    "The MFE Toolbox provides a comprehensive suite of tools for time series analysis, leveraging the power of Python's scientific ecosystem. The toolbox's modern implementation with NumPy, Pandas, Statsmodels, and Numba acceleration makes it a powerful tool for financial econometrics and time series analysis."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
```