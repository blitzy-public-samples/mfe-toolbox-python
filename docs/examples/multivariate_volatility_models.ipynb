{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Multivariate Volatility Models\n",
    "\n",
    "This notebook demonstrates the use of multivariate volatility models in the MFE Toolbox. We'll cover:\n",
    "\n",
    "1. Introduction to multivariate volatility modeling\n",
    "2. Data preparation and exploration for multiple assets\n",
    "3. Correlation analysis and visualization\n",
    "4. Dynamic Conditional Correlation (DCC) model estimation\n",
    "5. BEKK model for covariance dynamics\n",
    "6. Constant Conditional Correlation (CCC) model\n",
    "7. Other multivariate models (OGARCH, RARCH, etc.)\n",
    "8. Portfolio volatility estimation and forecasting\n",
    "9. Asynchronous processing for computationally intensive models\n",
    "\n",
    "The MFE Toolbox provides a comprehensive set of multivariate volatility models for analyzing multiple asset returns, implemented using modern Python practices with NumPy, Pandas, and Numba acceleration."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Setup and Imports\n",
    "\n",
    "First, let's import the necessary modules and set up our environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standard imports\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from typing import Dict, List, Tuple, Optional, Union, Any\n",
    "import asyncio\n",
    "import datetime\n",
    "import warnings\n",
    "\n",
    "# MFE Toolbox imports\n",
    "import mfe\n",
    "from mfe.models.multivariate import DCC, BEKK, CCC, OGARCH, RARCH, RCC, RiskMetrics\n",
    "from mfe.models.univariate import GARCH\n",
    "from mfe.models.distributions import Normal, StudentT, GED, SkewedT\n",
    "from mfe.utils.data_transformations import returns_from_prices\n",
    "from mfe.utils.matrix_ops import cov2corr\n",
    "\n",
    "# Configure plotting\n",
    "plt.style.use('seaborn-v0_8-whitegrid')\n",
    "sns.set_context(\"notebook\", font_scale=1.2)\n",
    "plt.rcParams['figure.figsize'] = (12, 6)\n",
    "plt.rcParams['figure.dpi'] = 100\n",
    "\n",
    "# Display version information\n",
    "print(f\"MFE Toolbox version: {mfe.__version__}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Data Preparation and Exploration for Multiple Assets\n",
    "\n",
    "We'll use financial market data for multiple assets to demonstrate multivariate volatility modeling."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load sample data for multiple assets\n",
    "# In a real application, you might use yfinance, pandas-datareader, or your own data source\n",
    "# For this example, we'll create a function to download data using pandas-datareader\n",
    "\n",
    "def load_market_data(tickers: List[str], \n",
    "                     start_date: str = \"2010-01-01\", \n",
    "                     end_date: Optional[str] = None) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Load market data for multiple tickers.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    tickers : List[str]\n",
    "        List of ticker symbols to load\n",
    "    start_date : str\n",
    "        Start date in YYYY-MM-DD format\n",
    "    end_date : str, optional\n",
    "        End date in YYYY-MM-DD format, defaults to current date\n",
    "        \n",
    "    Returns\n",
    "    -------\n",
    "    pd.DataFrame\n",
    "        DataFrame containing adjusted close prices for all tickers\n",
    "    \"\"\"\n",
    "    try:\n",
    "        import pandas_datareader as pdr\n",
    "        \n",
    "        # Set end date to today if not provided\n",
    "        if end_date is None:\n",
    "            end_date = datetime.datetime.now().strftime(\"%Y-%m-%d\")\n",
    "            \n",
    "        # Initialize DataFrame to store results\n",
    "        all_data = pd.DataFrame()\n",
    "        \n",
    "        # Download data for each ticker\n",
    "        for ticker in tickers:\n",
    "            try:\n",
    "                data = pdr.get_data_yahoo(ticker, start=start_date, end=end_date)\n",
    "                all_data[ticker] = data['Adj Close']\n",
    "                print(f\"Downloaded data for {ticker}: {len(data)} days\")\n",
    "            except Exception as e:\n",
    "                print(f\"Error downloading data for {ticker}: {e}\")\n",
    "        \n",
    "        return all_data\n",
    "    except ImportError:\n",
    "        print(\"pandas-datareader not installed. Please install with: pip install pandas-datareader\")\n",
    "        # Create synthetic data as fallback\n",
    "        return create_synthetic_market_data(tickers, start_date, end_date)\n",
    "\n",
    "def create_synthetic_market_data(tickers: List[str], \n",
    "                                start_date: str = \"2010-01-01\", \n",
    "                                end_date: Optional[str] = None) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Create synthetic market data for multiple assets.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    tickers : List[str]\n",
    "        List of ticker symbols to simulate\n",
    "    start_date : str\n",
    "        Start date in YYYY-MM-DD format\n",
    "    end_date : str, optional\n",
    "        End date in YYYY-MM-DD format, defaults to current date\n",
    "        \n",
    "    Returns\n",
    "    -------\n",
    "    pd.DataFrame\n",
    "        DataFrame containing synthetic price data for all tickers\n",
    "    \"\"\"\n",
    "    # Parse dates\n",
    "    start = pd.to_datetime(start_date)\n",
    "    if end_date is None:\n",
    "        end = pd.to_datetime(datetime.datetime.now().strftime(\"%Y-%m-%d\"))\n",
    "    else:\n",
    "        end = pd.to_datetime(end_date)\n",
    "    \n",
    "    # Create date range (business days only)\n",
    "    dates = pd.date_range(start=start, end=end, freq='B')\n",
    "    \n",
    "    # Set random seed for reproducibility\n",
    "    np.random.seed(42)\n",
    "    \n",
    "    # Number of assets\n",
    "    n_assets = len(tickers)\n",
    "    n_days = len(dates)\n",
    "    \n",
    "    # Create correlation matrix with realistic correlations\n",
    "    # We'll use a factor model approach to ensure positive definiteness\n",
    "    # First, create random loadings on common factors\n",
    "    n_factors = 3  # Market, size, value factors for example\n",
    "    factor_loadings = np.random.uniform(0.3, 0.9, size=(n_assets, n_factors))\n",
    "    \n",
    "    # Create correlation matrix from factor loadings\n",
    "    corr_matrix = factor_loadings @ factor_loadings.T\n",
    "    # Add idiosyncratic variance to ensure diagonal of 1s\n",
    "    for i in range(n_assets):\n",
    "        corr_matrix[i, i] = 1.0\n",
    "    \n",
    "    # Ensure it's a valid correlation matrix\n",
    "    # Make it symmetric\n",
    "    corr_matrix = (corr_matrix + corr_matrix.T) / 2\n",
    "    # Normalize to ensure diagonal is 1\n",
    "    d = np.sqrt(np.diag(corr_matrix))\n",
    "    corr_matrix = corr_matrix / np.outer(d, d)\n",
    "    \n",
    "    # Create volatilities for each asset (annualized)\n",
    "    asset_vols = np.random.uniform(0.15, 0.35, size=n_assets)  # 15% to 35% annual vol\n",
    "    \n",
    "    # Convert to daily volatility\n",
    "    daily_vols = asset_vols / np.sqrt(252)\n",
    "    \n",
    "    # Create covariance matrix\n",
    "    cov_matrix = np.diag(daily_vols) @ corr_matrix @ np.diag(daily_vols)\n",
    "    \n",
    "    # Generate correlated returns\n",
    "    # We'll use Cholesky decomposition\n",
    "    chol = np.linalg.cholesky(cov_matrix)\n",
    "    \n",
    "    # Generate random normal returns\n",
    "    random_returns = np.random.normal(0, 1, size=(n_days, n_assets))\n",
    "    \n",
    "    # Transform to correlated returns\n",
    "    correlated_returns = random_returns @ chol.T\n",
    "    \n",
    "    # Add drift (expected return)\n",
    "    expected_returns = np.random.uniform(0.05, 0.15, size=n_assets) / 252  # 5% to 15% annual return\n",
    "    correlated_returns += expected_returns\n",
    "    \n",
    "    # Add volatility clustering\n",
    "    # We'll use a simple AR(1) process for volatility\n",
    "    vol_persistence = 0.95\n",
    "    vol_scale = np.ones((n_days, n_assets))\n",
    "    vol_innovation = np.random.normal(0, 0.1, size=(n_days, n_assets))\n",
    "    \n",
    "    for t in range(1, n_days):\n",
    "        vol_scale[t] = np.sqrt(0.05 + vol_persistence * vol_scale[t-1]**2 + 0.05 * vol_innovation[t]**2)\n",
    "    \n",
    "    # Apply time-varying volatility\n",
    "    correlated_returns = correlated_returns * vol_scale\n",
    "    \n",
    "    # Convert returns to prices\n",
    "    # Start with price of 100 for each asset\n",
    "    prices = np.zeros((n_days, n_assets))\n",
    "    prices[0] = 100.0\n",
    "    \n",
    "    for t in range(1, n_days):\n",
    "        prices[t] = prices[t-1] * (1 + correlated_returns[t])\n",
    "    \n",
    "    # Create DataFrame\n",
    "    price_df = pd.DataFrame(prices, index=dates, columns=tickers)\n",
    "    \n",
    "    print(f\"Created synthetic data for {n_assets} assets over {n_days} days\")\n",
    "    return price_df\n",
    "\n",
    "# Define tickers to analyze\n",
    "# We'll use major US indices and some sector ETFs\n",
    "tickers = ['SPY', 'QQQ', 'IWM', 'XLF', 'XLE', 'XLK', 'XLV', 'XLI']\n",
    "\n",
    "# Load data\n",
    "try:\n",
    "    price_data = load_market_data(tickers, \"2015-01-01\")\n",
    "    print(f\"Loaded data for {len(price_data.columns)} assets over {len(price_data)} days\")\n",
    "except Exception as e:\n",
    "    print(f\"Error loading data: {e}\")\n",
    "    print(\"Using synthetic data instead\")\n",
    "    price_data = create_synthetic_market_data(tickers, \"2015-01-01\")\n",
    "\n",
    "# Display the first few rows\n",
    "price_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check for missing values\n",
    "missing_values = price_data.isna().sum()\n",
    "print(\"Missing values per asset:\")\n",
    "print(missing_values)\n",
    "\n",
    "# Handle missing values if necessary\n",
    "if missing_values.sum() > 0:\n",
    "    print(\"\nHandling missing values...\")\n",
    "    # Forward fill missing values\n",
    "    price_data = price_data.fillna(method='ffill')\n",
    "    # Backward fill any remaining missing values (at the beginning)\n",
    "    price_data = price_data.fillna(method='bfill')\n",
    "    print(\"Missing values after handling:\", price_data.isna().sum().sum())\n",
    "\n",
    "# Calculate returns\n",
    "# We'll use log returns for volatility modeling\n",
    "returns_df = pd.DataFrame()\n",
    "for ticker in price_data.columns:\n",
    "    returns_df[ticker] = returns_from_prices(price_data[ticker], log=True) * 100  # Convert to percentage\n",
    "\n",
    "# Display summary statistics\n",
    "returns_summary = returns_df.describe()\n",
    "print(\"\nReturns Summary Statistics:\")\n",
    "returns_summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot price series\n",
    "plt.figure(figsize=(14, 8))\n",
    "\n",
    "# Normalize prices to start at 100 for easier comparison\n",
    "normalized_prices = price_data.div(price_data.iloc[0]) * 100\n",
    "\n",
    "for ticker in normalized_prices.columns:\n",
    "    plt.plot(normalized_prices.index, normalized_prices[ticker], label=ticker)\n",
    "\n",
    "plt.title('Normalized Price Series (Base = 100)')\n",
    "plt.ylabel('Normalized Price')\n",
    "plt.xlabel('Date')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot returns\n",
    "fig, axes = plt.subplots(len(returns_df.columns), 1, figsize=(14, 3*len(returns_df.columns)), sharex=True)\n",
    "\n",
    "for i, ticker in enumerate(returns_df.columns):\n",
    "    axes[i].plot(returns_df.index, returns_df[ticker], label=ticker)\n",
    "    axes[i].set_title(f'{ticker} Daily Returns (%)')\n",
    "    axes[i].set_ylabel('Returns (%)')\n",
    "    axes[i].legend(loc='upper right')\n",
    "    axes[i].grid(True)\n",
    "\n",
    "axes[-1].set_xlabel('Date')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Correlation Analysis and Visualization\n",
    "\n",
    "Let's analyze the correlation structure between the assets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate unconditional correlation matrix\n",
    "corr_matrix = returns_df.corr()\n",
    "\n",
    "# Plot correlation matrix as a heatmap\n",
    "plt.figure(figsize=(12, 10))\n",
    "sns.heatmap(corr_matrix, annot=True, cmap='coolwarm', vmin=-1, vmax=1, center=0,\n",
    "            square=True, linewidths=0.5, cbar_kws={\"shrink\": 0.8})\n",
    "plt.title('Unconditional Correlation Matrix')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate rolling correlations to see how they evolve over time\n",
    "# We'll focus on a few key pairs\n",
    "window_size = 60  # 60-day rolling window (approximately 3 months)\n",
    "\n",
    "# Define pairs to analyze\n",
    "pairs = [\n",
    "    ('SPY', 'QQQ'),   # S&P 500 vs NASDAQ\n",
    "    ('SPY', 'IWM'),   # S&P 500 vs Russell 2000\n",
    "    ('SPY', 'XLF'),   # S&P 500 vs Financials\n",
    "    ('SPY', 'XLE'),   # S&P 500 vs Energy\n",
    "    ('XLF', 'XLE'),   # Financials vs Energy\n",
    "    ('XLK', 'XLV')    # Technology vs Healthcare\n",
    "]\n",
    "\n",
    "# Calculate rolling correlations\n",
    "rolling_corrs = {}\n",
    "for asset1, asset2 in pairs:\n",
    "    rolling_corrs[(asset1, asset2)] = returns_df[asset1].rolling(window=window_size).corr(returns_df[asset2])\n",
    "\n",
    "# Plot rolling correlations\n",
    "plt.figure(figsize=(14, 8))\n",
    "\n",
    "for pair, corr in rolling_corrs.items():\n",
    "    plt.plot(corr.index, corr, label=f'{pair[0]} vs {pair[1]}')\n",
    "\n",
    "plt.title(f'{window_size}-Day Rolling Correlations')\n",
    "plt.ylabel('Correlation')\n",
    "plt.xlabel('Date')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.ylim(-0.2, 1.0)  # Adjust as needed\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate rolling volatilities\n",
    "rolling_vols = returns_df.rolling(window=window_size).std() * np.sqrt(252)  # Annualized\n",
    "\n",
    "# Plot rolling volatilities\n",
    "plt.figure(figsize=(14, 8))\n",
    "\n",
    "for ticker in rolling_vols.columns:\n",
    "    plt.plot(rolling_vols.index, rolling_vols[ticker], label=ticker)\n",
    "\n",
    "plt.title(f'{window_size}-Day Rolling Annualized Volatility')\n",
    "plt.ylabel('Volatility (%)')\n",
    "plt.xlabel('Date')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze the distribution of returns\n",
    "# We'll create a pairplot for a subset of assets\n",
    "subset_tickers = ['SPY', 'QQQ', 'XLF', 'XLE']  # Subset for clarity\n",
    "subset_returns = returns_df[subset_tickers]\n",
    "\n",
    "# Create pairplot\n",
    "sns.pairplot(subset_returns, diag_kind='kde', plot_kws={'alpha': 0.6})\n",
    "plt.suptitle('Pairwise Return Distributions and Scatter Plots', y=1.02, fontsize=16)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Dynamic Conditional Correlation (DCC) Model Estimation\n",
    "\n",
    "Now let's estimate a DCC model to capture time-varying correlations between assets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare data for DCC estimation\n",
    "# Convert returns DataFrame to NumPy array\n",
    "returns_array = returns_df.values\n",
    "\n",
    "# Create and estimate a DCC-GARCH model\n",
    "# First, we'll use normal innovations for simplicity\n",
    "dcc_model = DCC(returns_df.shape[1],  # Number of assets\n",
    "                p=1, q=1,             # DCC orders\n",
    "                univariate_model=GARCH,  # Univariate model for each series\n",
    "                univariate_params={'p': 1, 'q': 1},  # GARCH(1,1) for each series\n",
    "                distribution=Normal())\n",
    "\n",
    "# Fit the model\n",
    "print(\"Estimating DCC-GARCH model...\")\n",
    "dcc_results = dcc_model.fit(returns_array)\n",
    "print(\"DCC-GARCH estimation complete.\")\n",
    "\n",
    "# Display estimation results\n",
    "print(\"\nDCC-GARCH Estimation Results:\")\n",
    "print(f\"Log-Likelihood: {dcc_results.log_likelihood:.4f}\")\n",
    "print(f\"AIC: {dcc_results.aic:.4f}\")\n",
    "print(f\"BIC: {dcc_results.bic:.4f}\")\n",
    "\n",
    "# Display DCC parameters\n",
    "print(\"\nDCC Parameters:\")\n",
    "dcc_params = pd.DataFrame({\n",
    "    'Parameter': dcc_results.parameter_names[-2:],  # Last two parameters are DCC parameters\n",
    "    'Estimate': dcc_results.parameters[-2:],\n",
    "    'Std. Error': dcc_results.std_errors[-2:],\n",
    "    't-statistic': dcc_results.t_stats[-2:],\n",
    "    'p-value': dcc_results.p_values[-2:]\n",
    "})\n",
    "print(dcc_params)\n",
    "\n",
    "# Calculate persistence\n",
    "alpha = dcc_results.parameters[-2]  # DCC alpha parameter\n",
    "beta = dcc_results.parameters[-1]   # DCC beta parameter\n",
    "persistence = alpha + beta\n",
    "print(f\"\nDCC Persistence (α + β): {persistence:.4f}\")\n",
    "print(f\"Half-life: {np.log(0.5) / np.log(persistence):.2f} days\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract conditional correlations from DCC model\n",
    "conditional_correlations = dcc_results.conditional_correlations\n",
    "\n",
    "# Create a dictionary to store time series of pairwise correlations\n",
    "pairwise_correlations = {}\n",
    "for i, asset1 in enumerate(returns_df.columns):\n",
    "    for j, asset2 in enumerate(returns_df.columns):\n",
    "        if i < j:  # Only store unique pairs\n",
    "            pair = (asset1, asset2)\n",
    "            pairwise_correlations[pair] = [corr_matrix[i, j] for corr_matrix in conditional_correlations]\n",
    "\n",
    "# Convert to DataFrame for easier plotting\n",
    "corr_df = pd.DataFrame(pairwise_correlations, index=returns_df.index)\n",
    "\n",
    "# Plot conditional correlations for selected pairs\n",
    "plt.figure(figsize=(14, 8))\n",
    "\n",
    "for pair in pairs:\n",
    "    if pair in corr_df.columns:\n",
    "        plt.plot(corr_df.index, corr_df[pair], label=f'{pair[0]} vs {pair[1]}')\n",
    "    elif (pair[1], pair[0]) in corr_df.columns:  # Check reverse order\n",
    "        plt.plot(corr_df.index, corr_df[(pair[1], pair[0])], label=f'{pair[0]} vs {pair[1]}')\n",
    "\n",
    "plt.title('DCC-GARCH Conditional Correlations')\n",
    "plt.ylabel('Correlation')\n",
    "plt.xlabel('Date')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.ylim(-0.2, 1.0)  # Adjust as needed\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare DCC conditional correlations with rolling correlations\n",
    "plt.figure(figsize=(14, 8))\n",
    "\n",
    "# Select a specific pair for comparison\n",
    "comparison_pair = ('SPY', 'XLF')  # S&P 500 vs Financials\n",
    "\n",
    "# Get DCC correlation for this pair\n",
    "if comparison_pair in corr_df.columns:\n",
    "    dcc_corr = corr_df[comparison_pair]\n",
    "elif (comparison_pair[1], comparison_pair[0]) in corr_df.columns:  # Check reverse order\n",
    "    dcc_corr = corr_df[(comparison_pair[1], comparison_pair[0])]\n",
    "else:\n",
    "    dcc_corr = None\n",
    "\n",
    "# Get rolling correlation for this pair\n",
    "rolling_corr = rolling_corrs.get(comparison_pair)\n",
    "if rolling_corr is None:\n",
    "    rolling_corr = rolling_corrs.get((comparison_pair[1], comparison_pair[0]))\n",
    "\n",
    "# Plot both correlations\n",
    "if dcc_corr is not None:\n",
    "    plt.plot(dcc_corr.index, dcc_corr, label='DCC-GARCH', linewidth=2)\n",
    "if rolling_corr is not None:\n",
    "    plt.plot(rolling_corr.index, rolling_corr, label=f'{window_size}-Day Rolling', linewidth=2, alpha=0.7)\n",
    "\n",
    "plt.title(f'Comparison of Correlation Estimates: {comparison_pair[0]} vs {comparison_pair[1]}')\n",
    "plt.ylabel('Correlation')\n",
    "plt.xlabel('Date')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize the evolution of the full correlation matrix over time\n",
    "# We'll create a heatmap animation for selected dates\n",
    "\n",
    "# Select dates for visualization (e.g., every 6 months)\n",
    "date_indices = np.linspace(0, len(returns_df) - 1, 6, dtype=int)\n",
    "selected_dates = returns_df.index[date_indices]\n",
    "\n",
    "# Create a figure with subplots for each date\n",
    "fig, axes = plt.subplots(2, 3, figsize=(18, 12))\n",
    "axes = axes.flatten()\n",
    "\n",
    "for i, date_idx in enumerate(date_indices):\n",
    "    date = returns_df.index[date_idx]\n",
    "    corr_matrix = conditional_correlations[date_idx]\n",
    "    \n",
    "    # Create heatmap\n",
    "    sns.heatmap(corr_matrix, annot=True, cmap='coolwarm', vmin=-1, vmax=1, center=0,\n",
    "                square=True, linewidths=0.5, cbar=False, ax=axes[i],\n",
    "                xticklabels=returns_df.columns, yticklabels=returns_df.columns)\n",
    "    axes[i].set_title(f'Conditional Correlation Matrix: {date.strftime(\"%Y-%m-%d\")}')\n",
    "\n",
    "# Add a colorbar to the figure\n",
    "cbar_ax = fig.add_axes([0.92, 0.15, 0.02, 0.7])\n",
    "sm = plt.cm.ScalarMappable(cmap='coolwarm', norm=plt.Normalize(-1, 1))\n",
    "sm.set_array([])\n",
    "fig.colorbar(sm, cax=cbar_ax)\n",
    "\n",
    "plt.tight_layout(rect=[0, 0, 0.9, 1])\n",
    "plt.suptitle('Evolution of Conditional Correlation Matrix', fontsize=16, y=1.02)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract conditional volatilities from DCC model\n",
    "conditional_volatilities = dcc_results.conditional_volatilities\n",
    "\n",
    "# Convert to DataFrame for easier plotting\n",
    "vol_df = pd.DataFrame(conditional_volatilities, index=returns_df.index, columns=returns_df.columns)\n",
    "\n",
    "# Plot conditional volatilities\n",
    "plt.figure(figsize=(14, 8))\n",
    "\n",
    "for ticker in vol_df.columns:\n",
    "    plt.plot(vol_df.index, vol_df[ticker], label=ticker)\n",
    "\n",
    "plt.title('DCC-GARCH Conditional Volatilities')\n",
    "plt.ylabel('Volatility (%)')\n",
    "plt.xlabel('Date')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare DCC volatilities with rolling volatilities\n",
    "plt.figure(figsize=(14, 8))\n",
    "\n",
    "# Select a specific asset for comparison\n",
    "comparison_asset = 'SPY'\n",
    "\n",
    "# Plot both volatilities\n",
    "plt.plot(vol_df.index, vol_df[comparison_asset], label='DCC-GARCH', linewidth=2)\n",
    "plt.plot(rolling_vols.index, rolling_vols[comparison_asset], label=f'{window_size}-Day Rolling', \n",
    "         linewidth=2, alpha=0.7)\n",
    "\n",
    "plt.title(f'Comparison of Volatility Estimates: {comparison_asset}')\n",
    "plt.ylabel('Volatility (%)')\n",
    "plt.xlabel('Date')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. BEKK Model for Covariance Dynamics\n",
    "\n",
    "Let's estimate a BEKK model, which directly models the covariance matrix dynamics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For BEKK models, we'll use a subset of assets to reduce computational complexity\n",
    "# BEKK models have many parameters and can be computationally intensive\n",
    "subset_tickers = ['SPY', 'QQQ', 'XLF', 'XLE']\n",
    "subset_returns = returns_df[subset_tickers].values\n",
    "\n",
    "# Create and estimate a BEKK model\n",
    "bekk_model = BEKK(len(subset_tickers),  # Number of assets\n",
    "                 p=1, q=1,             # BEKK orders\n",
    "                 distribution=Normal())\n",
    "\n",
    "# Fit the model\n",
    "print(\"Estimating BEKK model...\")\n",
    "bekk_results = bekk_model.fit(subset_returns)\n",
    "print(\"BEKK estimation complete.\")\n",
    "\n",
    "# Display estimation results\n",
    "print(\"\nBEKK Estimation Results:\")\n",
    "print(f\"Log-Likelihood: {bekk_results.log_likelihood:.4f}\")\n",
    "print(f\"AIC: {bekk_results.aic:.4f}\")\n",
    "print(f\"BIC: {bekk_results.bic:.4f}\")\n",
    "print(f\"Number of Parameters: {len(bekk_results.parameters)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract conditional covariances from BEKK model\n",
    "bekk_covariances = bekk_results.conditional_covariance\n",
    "\n",
    "# Convert to conditional correlations for comparison\n",
    "bekk_correlations = []\n",
    "for cov_matrix in bekk_covariances:\n",
    "    corr_matrix = cov2corr(cov_matrix)\n",
    "    bekk_correlations.append(corr_matrix)\n",
    "\n",
    "# Create a dictionary to store time series of pairwise correlations\n",
    "bekk_pairwise_correlations = {}\n",
    "for i, asset1 in enumerate(subset_tickers):\n",
    "    for j, asset2 in enumerate(subset_tickers):\n",
    "        if i < j:  # Only store unique pairs\n",
    "            pair = (asset1, asset2)\n",
    "            bekk_pairwise_correlations[pair] = [corr_matrix[i, j] for corr_matrix in bekk_correlations]\n",
    "\n",
    "# Convert to DataFrame for easier plotting\n",
    "bekk_corr_df = pd.DataFrame(bekk_pairwise_correlations, index=returns_df.index)\n",
    "\n",
    "# Plot BEKK conditional correlations\n",
    "plt.figure(figsize=(14, 8))\n",
    "\n",
    "for pair in bekk_corr_df.columns:\n",
    "    plt.plot(bekk_corr_df.index, bekk_corr_df[pair], label=f'{pair[0]} vs {pair[1]}')\n",
    "\n",
    "plt.title('BEKK Conditional Correlations')\n",
    "plt.ylabel('Correlation')\n",
    "plt.xlabel('Date')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.ylim(-0.2, 1.0)  # Adjust as needed\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare BEKK and DCC correlations\n",
    "plt.figure(figsize=(14, 8))\n",
    "\n",
    "# Select a specific pair for comparison\n",
    "comparison_pair = ('SPY', 'XLF')  # S&P 500 vs Financials\n",
    "\n",
    "# Get BEKK correlation for this pair\n",
    "if comparison_pair in bekk_corr_df.columns:\n",
    "    bekk_corr = bekk_corr_df[comparison_pair]\n",
    "elif (comparison_pair[1], comparison_pair[0]) in bekk_corr_df.columns:  # Check reverse order\n",
    "    bekk_corr = bekk_corr_df[(comparison_pair[1], comparison_pair[0])]\n",
    "else:\n",
    "    bekk_corr = None\n",
    "\n",
    "# Get DCC correlation for this pair\n",
    "if comparison_pair in corr_df.columns:\n",
    "    dcc_corr = corr_df[comparison_pair]\n",
    "elif (comparison_pair[1], comparison_pair[0]) in corr_df.columns:  # Check reverse order\n",
    "    dcc_corr = corr_df[(comparison_pair[1], comparison_pair[0])]\n",
    "else:\n",
    "    dcc_corr = None\n",
    "\n",
    "# Plot both correlations\n",
    "if bekk_corr is not None:\n",
    "    plt.plot(bekk_corr.index, bekk_corr, label='BEKK', linewidth=2)\n",
    "if dcc_corr is not None:\n",
    "    plt.plot(dcc_corr.index, dcc_corr, label='DCC-GARCH', linewidth=2, alpha=0.7)\n",
    "\n",
    "plt.title(f'Comparison of Correlation Estimates: {comparison_pair[0]} vs {comparison_pair[1]}')\n",
    "plt.ylabel('Correlation')\n",
    "plt.xlabel('Date')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract conditional volatilities from BEKK model\n",
    "bekk_volatilities = np.array([[np.sqrt(cov_matrix[i, i]) for i in range(len(subset_tickers))] \n",
    "                             for cov_matrix in bekk_covariances])\n",
    "\n",
    "# Convert to DataFrame for easier plotting\n",
    "bekk_vol_df = pd.DataFrame(bekk_volatilities, index=returns_df.index, columns=subset_tickers)\n",
    "\n",
    "# Plot BEKK conditional volatilities\n",
    "plt.figure(figsize=(14, 8))\n",
    "\n",
    "for ticker in bekk_vol_df.columns:\n",
    "    plt.plot(bekk_vol_df.index, bekk_vol_df[ticker], label=ticker)\n",
    "\n",
    "plt.title('BEKK Conditional Volatilities')\n",
    "plt.ylabel('Volatility (%)')\n",
    "plt.xlabel('Date')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare BEKK and DCC volatilities\n",
    "plt.figure(figsize=(14, 8))\n",
    "\n",
    "# Select a specific asset for comparison\n",
    "comparison_asset = 'SPY'\n",
    "\n",
    "# Plot both volatilities\n",
    "plt.plot(bekk_vol_df.index, bekk_vol_df[comparison_asset], label='BEKK', linewidth=2)\n",
    "plt.plot(vol_df.index, vol_df[comparison_asset], label='DCC-GARCH', linewidth=2, alpha=0.7)\n",
    "\n",
    "plt.title(f'Comparison of Volatility Estimates: {comparison_asset}')\n",
    "plt.ylabel('Volatility (%)')\n",
    "plt.xlabel('Date')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Constant Conditional Correlation (CCC) Model\n",
    "\n",
    "Let's estimate a CCC model, which assumes constant correlations but time-varying volatilities."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create and estimate a CCC model\n",
    "ccc_model = CCC(returns_df.shape[1],  # Number of assets\n",
    "               univariate_model=GARCH,  # Univariate model for each series\n",
    "               univariate_params={'p': 1, 'q': 1},  # GARCH(1,1) for each series\n",
    "               distribution=Normal())\n",
    "\n",
    "# Fit the model\n",
    "print(\"Estimating CCC model...\")\n",
    "ccc_results = ccc_model.fit(returns_array)\n",
    "print(\"CCC estimation complete.\")\n",
    "\n",
    "# Display estimation results\n",
    "print(\"\nCCC Estimation Results:\")\n",
    "print(f\"Log-Likelihood: {ccc_results.log_likelihood:.4f}\")\n",
    "print(f\"AIC: {ccc_results.aic:.4f}\")\n",
    "print(f\"BIC: {ccc_results.bic:.4f}\")\n",
    "\n",
    "# Display constant correlation matrix\n",
    "print(\"\nConstant Correlation Matrix:\")\n",
    "constant_corr = ccc_results.constant_correlation\n",
    "constant_corr_df = pd.DataFrame(constant_corr, index=returns_df.columns, columns=returns_df.columns)\n",
    "print(constant_corr_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize the constant correlation matrix\n",
    "plt.figure(figsize=(12, 10))\n",
    "sns.heatmap(constant_corr_df, annot=True, cmap='coolwarm', vmin=-1, vmax=1, center=0,\n",
    "            square=True, linewidths=0.5, cbar_kws={\"shrink\": 0.8})\n",
    "plt.title('CCC Model: Constant Correlation Matrix')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract conditional volatilities from CCC model\n",
    "ccc_volatilities = ccc_results.conditional_volatilities\n",
    "\n",
    "# Convert to DataFrame for easier plotting\n",
    "ccc_vol_df = pd.DataFrame(ccc_volatilities, index=returns_df.index, columns=returns_df.columns)\n",
    "\n",
    "# Plot CCC conditional volatilities\n",
    "plt.figure(figsize=(14, 8))\n",
    "\n",
    "for ticker in ccc_vol_df.columns:\n",
    "    plt.plot(ccc_vol_df.index, ccc_vol_df[ticker], label=ticker)\n",
    "\n",
    "plt.title('CCC Conditional Volatilities')\n",
    "plt.ylabel('Volatility (%)')\n",
    "plt.xlabel('Date')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare volatility estimates from different models\n",
    "plt.figure(figsize=(14, 8))\n",
    "\n",
    "# Select a specific asset for comparison\n",
    "comparison_asset = 'SPY'\n",
    "\n",
    "# Plot volatilities from different models\n",
    "plt.plot(vol_df.index, vol_df[comparison_asset], label='DCC-GARCH', linewidth=2)\n",
    "plt.plot(ccc_vol_df.index, ccc_vol_df[comparison_asset], label='CCC', linewidth=2, alpha=0.7)\n",
    "if comparison_asset in bekk_vol_df.columns:\n",
    "    plt.plot(bekk_vol_df.index, bekk_vol_df[comparison_asset], label='BEKK', linewidth=2, alpha=0.7)\n",
    "\n",
    "plt.title(f'Comparison of Volatility Estimates: {comparison_asset}')\n",
    "plt.ylabel('Volatility (%)')\n",
    "plt.xlabel('Date')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare model fit using information criteria\n",
    "model_comparison = pd.DataFrame({\n",
    "    'Model': ['DCC-GARCH', 'BEKK', 'CCC'],\n",
    "    'Log-Likelihood': [\n",
    "        dcc_results.log_likelihood,\n",
    "        bekk_results.log_likelihood,\n",
    "        ccc_results.log_likelihood\n",
    "    ],\n",
    "    'AIC': [\n",
    "        dcc_results.aic,\n",
    "        bekk_results.aic,\n",
    "        ccc_results.aic\n",
    "    ],\n",
    "    'BIC': [\n",
    "        dcc_results.bic,\n",
    "        bekk_results.bic,\n",
    "        ccc_results.bic\n",
    "    ],\n",
    "    'Parameters': [\n",
    "        len(dcc_results.parameters),\n",
    "        len(bekk_results.parameters),\n",
    "        len(ccc_results.parameters)\n",
    "    ]\n",
    "})\n",
    "\n",
    "print(\"Model Comparison:\")\n",
    "model_comparison"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Other Multivariate Models (OGARCH, RARCH, etc.)\n",
    "\n",
    "Let's explore some other multivariate volatility models available in the MFE Toolbox."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create and estimate an OGARCH model\n",
    "# OGARCH uses principal component analysis to transform the data\n",
    "ogarch_model = OGARCH(returns_df.shape[1],  # Number of assets\n",
    "                     n_factors=3,          # Number of factors\n",
    "                     univariate_model=GARCH,  # Univariate model for each factor\n",
    "                     univariate_params={'p': 1, 'q': 1},  # GARCH(1,1) for each factor\n",
    "                     distribution=Normal())\n",
    "\n",
    "# Fit the model\n",
    "print(\"Estimating OGARCH model...\")\n",
    "ogarch_results = ogarch_model.fit(returns_array)\n",
    "print(\"OGARCH estimation complete.\")\n",
    "\n",
    "# Display estimation results\n",
    "print(\"\nOGARCH Estimation Results:\")\n",
    "print(f\"Log-Likelihood: {ogarch_results.log_likelihood:.4f}\")\n",
    "print(f\"AIC: {ogarch_results.aic:.4f}\")\n",
    "print(f\"BIC: {ogarch_results.bic:.4f}\")\n",
    "print(f\"Number of Parameters: {len(ogarch_results.parameters)}\")\n",
    "\n",
    "# Display factor loadings\n",
    "print(\"\nFactor Loadings:\")\n",
    "factor_loadings = ogarch_results.factor_loadings\n",
    "factor_loadings_df = pd.DataFrame(factor_loadings, index=returns_df.columns, \n",
    "                                 columns=[f'Factor {i+1}' for i in range(factor_loadings.shape[1])])\n",
    "print(factor_loadings_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract conditional covariances from OGARCH model\n",
    "ogarch_covariances = ogarch_results.conditional_covariance\n",
    "\n",
    "# Convert to conditional correlations for comparison\n",
    "ogarch_correlations = []\n",
    "for cov_matrix in ogarch_covariances:\n",
    "    corr_matrix = cov2corr(cov_matrix)\n",
    "    ogarch_correlations.append(corr_matrix)\n",
    "\n",
    "# Create a dictionary to store time series of pairwise correlations\n",
    "ogarch_pairwise_correlations = {}\n",
    "for i, asset1 in enumerate(returns_df.columns):\n",
    "    for j, asset2 in enumerate(returns_df.columns):\n",
    "        if i < j:  # Only store unique pairs\n",
    "            pair = (asset1, asset2)\n",
    "            ogarch_pairwise_correlations[pair] = [corr_matrix[i, j] for corr_matrix in ogarch_correlations]\n",
    "\n",
    "# Convert to DataFrame for easier plotting\n",
    "ogarch_corr_df = pd.DataFrame(ogarch_pairwise_correlations, index=returns_df.index)\n",
    "\n",
    "# Plot OGARCH conditional correlations for selected pairs\n",
    "plt.figure(figsize=(14, 8))\n",
    "\n",
    "for pair in pairs:\n",
    "    if pair in ogarch_corr_df.columns:\n",
    "        plt.plot(ogarch_corr_df.index, ogarch_corr_df[pair], label=f'{pair[0]} vs {pair[1]}')\n",
    "    elif (pair[1], pair[0]) in ogarch_corr_df.columns:  # Check reverse order\n",
    "        plt.plot(ogarch_corr_df.index, ogarch_corr_df[(pair[1], pair[0])], label=f'{pair[0]} vs {pair[1]}')\n",
    "\n",
    "plt.title('OGARCH Conditional Correlations')\n",
    "plt.ylabel('Correlation')\n",
    "plt.xlabel('Date')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.ylim(-0.2, 1.0)  # Adjust as needed\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create and estimate a RiskMetrics model\n",
    "# RiskMetrics uses exponential smoothing for covariance estimation\n",
    "riskmetrics_model = RiskMetrics(returns_df.shape[1],  # Number of assets\n",
    "                              lambda_=0.94)  # Decay factor\n",
    "\n",
    "# Fit the model\n",
    "print(\"Estimating RiskMetrics model...\")\n",
    "riskmetrics_results = riskmetrics_model.fit(returns_array)\n",
    "print(\"RiskMetrics estimation complete.\")\n",
    "\n",
    "# Display estimation results\n",
    "print(\"\nRiskMetrics Estimation Results:\")\n",
    "print(f\"Log-Likelihood: {riskmetrics_results.log_likelihood:.4f}\")\n",
    "print(f\"AIC: {riskmetrics_results.aic:.4f}\")\n",
    "print(f\"BIC: {riskmetrics_results.bic:.4f}\")\n",
    "print(f\"Lambda: {riskmetrics_results.parameters[0]:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract conditional covariances from RiskMetrics model\n",
    "riskmetrics_covariances = riskmetrics_results.conditional_covariance\n",
    "\n",
    "# Convert to conditional correlations for comparison\n",
    "riskmetrics_correlations = []\n",
    "for cov_matrix in riskmetrics_covariances:\n",
    "    corr_matrix = cov2corr(cov_matrix)\n",
    "    riskmetrics_correlations.append(corr_matrix)\n",
    "\n",
    "# Create a dictionary to store time series of pairwise correlations\n",
    "riskmetrics_pairwise_correlations = {}\n",
    "for i, asset1 in enumerate(returns_df.columns):\n",
    "    for j, asset2 in enumerate(returns_df.columns):\n",
    "        if i < j:  # Only store unique pairs\n",
    "            pair = (asset1, asset2)\n",
    "            riskmetrics_pairwise_correlations[pair] = [corr_matrix[i, j] for corr_matrix in riskmetrics_correlations]\n",
    "\n",
    "# Convert to DataFrame for easier plotting\n",
    "riskmetrics_corr_df = pd.DataFrame(riskmetrics_pairwise_correlations, index=returns_df.index)\n",
    "\n",
    "# Compare correlation estimates from different models\n",
    "plt.figure(figsize=(14, 8))\n",
    "\n",
    "# Select a specific pair for comparison\n",
    "comparison_pair = ('SPY', 'XLF')  # S&P 500 vs Financials\n",
    "\n",
    "# Get correlations for this pair from different models\n",
    "models = {\n",
    "    'DCC-GARCH': corr_df,\n",
    "    'OGARCH': ogarch_corr_df,\n",
    "    'RiskMetrics': riskmetrics_corr_df\n",
    "}\n",
    "\n",
    "for model_name, model_corr_df in models.items():\n",
    "    if comparison_pair in model_corr_df.columns:\n",
    "        model_corr = model_corr_df[comparison_pair]\n",
    "    elif (comparison_pair[1], comparison_pair[0]) in model_corr_df.columns:  # Check reverse order\n",
    "        model_corr = model_corr_df[(comparison_pair[1], comparison_pair[0])]\n",
    "    else:\n",
    "        model_corr = None\n",
    "    \n",
    "    if model_corr is not None:\n",
    "        plt.plot(model_corr.index, model_corr, label=model_name, linewidth=2, alpha=0.7)\n",
    "\n",
    "# Add constant correlation from CCC model\n",
    "i = returns_df.columns.get_loc(comparison_pair[0])\n",
    "j = returns_df.columns.get_loc(comparison_pair[1])\n",
    "ccc_corr_value = constant_corr[i, j]\n",
    "plt.axhline(y=ccc_corr_value, color='purple', linestyle='--', \n",
    "            label=f'CCC: {ccc_corr_value:.4f}', linewidth=2)\n",
    "\n",
    "plt.title(f'Comparison of Correlation Estimates: {comparison_pair[0]} vs {comparison_pair[1]}')\n",
    "plt.ylabel('Correlation')\n",
    "plt.xlabel('Date')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Update model comparison with all models\n",
    "model_comparison = pd.DataFrame({\n",
    "    'Model': ['DCC-GARCH', 'BEKK', 'CCC', 'OGARCH', 'RiskMetrics'],\n",
    "    'Log-Likelihood': [\n",
    "        dcc_results.log_likelihood,\n",
    "        bekk_results.log_likelihood,\n",
    "        ccc_results.log_likelihood,\n",
    "        ogarch_results.log_likelihood,\n",
    "        riskmetrics_results.log_likelihood\n",
    "    ],\n",
    "    'AIC': [\n",
    "        dcc_results.aic,\n",
    "        bekk_results.aic,\n",
    "        ccc_results.aic,\n",
    "        ogarch_results.aic,\n",
    "        riskmetrics_results.aic\n",
    "    ],\n",
    "    'BIC': [\n",
    "        dcc_results.bic,\n",
    "        bekk_results.bic,\n",
    "        ccc_results.bic,\n",
    "        ogarch_results.bic,\n",
    "        riskmetrics_results.bic\n",
    "    ],\n",
    "    'Parameters': [\n",
    "        len(dcc_results.parameters),\n",
    "        len(bekk_results.parameters),\n",
    "        len(ccc_results.parameters),\n",
    "        len(ogarch_results.parameters),\n",
    "        len(riskmetrics_results.parameters)\n",
    "    ]\n",
    "})\n",
    "\n",
    "# Sort by AIC\n",
    "model_comparison = model_comparison.sort_values('AIC')\n",
    "\n",
    "print(\"Model Comparison:\")\n",
    "model_comparison"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Portfolio Volatility Estimation and Forecasting\n",
    "\n",
    "Let's use our multivariate models to estimate and forecast portfolio volatility."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define portfolio weights\n",
    "# We'll create an equal-weighted portfolio\n",
    "n_assets = returns_df.shape[1]\n",
    "equal_weights = np.ones(n_assets) / n_assets\n",
    "\n",
    "# Create a DataFrame with weights\n",
    "weights_df = pd.DataFrame({\n",
    "    'Asset': returns_df.columns,\n",
    "    'Weight': equal_weights\n",
    "})\n",
    "\n",
    "print(\"Portfolio Weights:\")\n",
    "weights_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate portfolio returns\n",
    "portfolio_returns = returns_df.dot(equal_weights)\n",
    "\n",
    "# Plot portfolio returns\n",
    "plt.figure(figsize=(14, 6))\n",
    "plt.plot(portfolio_returns.index, portfolio_returns, color='blue')\n",
    "plt.title('Equal-Weighted Portfolio Returns')\n",
    "plt.ylabel('Returns (%)')\n",
    "plt.xlabel('Date')\n",
    "plt.grid(True)\n",
    "plt.show()\n",
    "\n",
    "# Calculate rolling portfolio volatility\n",
    "rolling_portfolio_vol = portfolio_returns.rolling(window=window_size).std() * np.sqrt(252)  # Annualized\n",
    "\n",
    "# Plot rolling portfolio volatility\n",
    "plt.figure(figsize=(14, 6))\n",
    "plt.plot(rolling_portfolio_vol.index, rolling_portfolio_vol, color='red')\n",
    "plt.title(f'{window_size}-Day Rolling Portfolio Volatility (Annualized)')\n",
    "plt.ylabel('Volatility (%)')\n",
    "plt.xlabel('Date')\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate portfolio volatility using multivariate models\n",
    "# For a portfolio with weights w, the variance is w' * Sigma * w\n",
    "# where Sigma is the covariance matrix\n",
    "\n",
    "# Function to calculate portfolio variance from covariance matrix\n",
    "def portfolio_variance(cov_matrix, weights):\n",
    "    return weights.T @ cov_matrix @ weights\n",
    "\n",
    "# Calculate portfolio volatility from different models\n",
    "portfolio_vols = {}\n",
    "\n",
    "# DCC model\n",
    "dcc_portfolio_var = np.array([portfolio_variance(cov_matrix, equal_weights) \n",
    "                             for cov_matrix in dcc_results.conditional_covariance])\n",
    "portfolio_vols['DCC-GARCH'] = np.sqrt(dcc_portfolio_var) * np.sqrt(252)  # Annualized\n",
    "\n",
    "# CCC model\n",
    "ccc_portfolio_var = np.array([portfolio_variance(cov_matrix, equal_weights) \n",
    "                             for cov_matrix in ccc_results.conditional_covariance])\n",
    "portfolio_vols['CCC'] = np.sqrt(ccc_portfolio_var) * np.sqrt(252)  # Annualized\n",
    "\n",
    "# OGARCH model\n",
    "ogarch_portfolio_var = np.array([portfolio_variance(cov_matrix, equal_weights) \n",
    "                               for cov_matrix in ogarch_results.conditional_covariance])\n",
    "portfolio_vols['OGARCH'] = np.sqrt(ogarch_portfolio_var) * np.sqrt(252)  # Annualized\n",
    "\n",
    "# RiskMetrics model\n",
    "riskmetrics_portfolio_var = np.array([portfolio_variance(cov_matrix, equal_weights) \n",
    "                                    for cov_matrix in riskmetrics_results.conditional_covariance])\n",
    "portfolio_vols['RiskMetrics'] = np.sqrt(riskmetrics_portfolio_var) * np.sqrt(252)  # Annualized\n",
    "\n",
    "# Convert to DataFrame\n",
    "portfolio_vols_df = pd.DataFrame(portfolio_vols, index=returns_df.index)\n",
    "\n",
    "# Add rolling volatility for comparison\n",
    "portfolio_vols_df['Rolling'] = rolling_portfolio_vol\n",
    "\n",
    "# Plot portfolio volatility from different models\n",
    "plt.figure(figsize=(14, 8))\n",
    "\n",
    "for model in portfolio_vols_df.columns:\n",
    "    plt.plot(portfolio_vols_df.index, portfolio_vols_df[model], label=model, linewidth=2, alpha=0.7)\n",
    "\n",
    "plt.title('Portfolio Volatility Estimates from Different Models (Annualized)')\n",
    "plt.ylabel('Volatility (%)')\n",
    "plt.xlabel('Date')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Forecast portfolio volatility\n",
    "# We'll use the best-performing model based on AIC\n",
    "best_model_name = model_comparison.iloc[0]['Model']\n",
    "print(f\"Best model based on AIC: {best_model_name}\")\n",
    "\n",
    "# Select the corresponding model and results\n",
    "if best_model_name == 'DCC-GARCH':\n",
    "    best_model = dcc_model\n",
    "    best_results = dcc_results\n",
    "elif best_model_name == 'BEKK':\n",
    "    best_model = bekk_model\n",
    "    best_results = bekk_results\n",
    "elif best_model_name == 'CCC':\n",
    "    best_model = ccc_model\n",
    "    best_results = ccc_results\n",
    "elif best_model_name == 'OGARCH':\n",
    "    best_model = ogarch_model\n",
    "    best_results = ogarch_results\n",
    "else:  # 'RiskMetrics'\n",
    "    best_model = riskmetrics_model\n",
    "    best_results = riskmetrics_results\n",
    "\n",
    "# Set forecast horizon\n",
    "forecast_horizon = 30  # 30 days ahead\n",
    "\n",
    "# Generate forecasts\n",
    "covariance_forecasts = best_model.forecast(\n",
    "    returns_array,\n",
    "    best_results.parameters,\n",
    "    horizon=forecast_horizon,\n",
    "    n_simulations=1000  # Number of Monte Carlo simulations\n",
    ")\n",
    "\n",
    "# Calculate portfolio variance forecasts\n",
    "portfolio_var_forecasts = np.array([portfolio_variance(cov_matrix, equal_weights) \n",
    "                                  for cov_matrix in covariance_forecasts.mean])\n",
    "portfolio_vol_forecasts = np.sqrt(portfolio_var_forecasts) * np.sqrt(252)  # Annualized\n",
    "\n",
    "# Create forecast dates\n",
    "last_date = returns_df.index[-1]\n",
    "forecast_dates = pd.date_range(start=last_date + pd.Timedelta(days=1), periods=forecast_horizon, freq='B')\n",
    "\n",
    "# Create DataFrame for forecasts\n",
    "forecast_df = pd.DataFrame({\n",
    "    'Volatility': portfolio_vol_forecasts\n",
    "}, index=forecast_dates)\n",
    "\n",
    "# Display forecast summary\n",
    "print(\"\nPortfolio Volatility Forecast Summary:\")\n",
    "forecast_df.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot portfolio volatility forecasts\n",
    "plt.figure(figsize=(14, 8))\n",
    "\n",
    "# Plot historical volatility\n",
    "historical_vol = portfolio_vols_df[best_model_name]\n",
    "plt.plot(historical_vol.index[-60:], historical_vol[-60:], label='Historical Volatility', color='blue')\n",
    "\n",
    "# Plot forecasted volatility\n",
    "plt.plot(forecast_df.index, forecast_df['Volatility'], label='Forecasted Volatility', color='red')\n",
    "\n",
    "# Add vertical line to separate historical and forecasted periods\n",
    "plt.axvline(x=returns_df.index[-1], color='black', linestyle='--', label='Forecast Start')\n",
    "\n",
    "plt.title(f'Portfolio Volatility Forecast using {best_model_name}')\n",
    "plt.ylabel('Volatility (% Annualized)')\n",
    "plt.xlabel('Date')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate Value-at-Risk (VaR) for the portfolio\n",
    "# We'll use the forecasted volatility and assume normal distribution\n",
    "from scipy import stats\n",
    "\n",
    "# Define confidence levels\n",
    "confidence_levels = [0.95, 0.99]\n",
    "\n",
    "# Calculate VaR for each forecast horizon\n",
    "var_df = pd.DataFrame(index=forecast_df.index)\n",
    "\n",
    "for confidence in confidence_levels:\n",
    "    # Calculate VaR assuming normal distribution\n",
    "    # VaR = -μ - σ * z_α\n",
    "    z_score = stats.norm.ppf(1 - confidence)\n",
    "    daily_vol = forecast_df['Volatility'] / np.sqrt(252)  # Convert to daily\n",
    "    var = -0 - daily_vol * z_score  # Assuming zero mean\n",
    "    var_df[f'VaR {confidence*100:.0f}%'] = var\n",
    "\n",
    "# Display VaR forecasts\n",
    "print(\"Portfolio Value-at-Risk Forecasts:\")\n",
    "var_df.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot VaR forecasts\n",
    "plt.figure(figsize=(14, 8))\n",
    "\n",
    "for confidence in confidence_levels:\n",
    "    plt.plot(var_df.index, var_df[f'VaR {confidence*100:.0f}%'], \n",
    "             label=f'VaR {confidence*100:.0f}%', linewidth=2)\n",
    "\n",
    "plt.title('Portfolio Value-at-Risk Forecasts')\n",
    "plt.ylabel('VaR (%)')\n",
    "plt.xlabel('Date')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Asynchronous Processing for Computationally Intensive Models\n",
    "\n",
    "For computationally intensive tasks like estimating multiple multivariate models, we can use asynchronous processing to improve efficiency."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define an asynchronous function to estimate a multivariate model\n",
    "async def estimate_model_async(model_class, model_name: str, returns: np.ndarray, \n",
    "                              model_params: dict) -> Tuple[str, Any]:\n",
    "    \"\"\"\n",
    "    Asynchronously estimate a multivariate volatility model and return results.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    model_class : class\n",
    "        Multivariate model class (DCC, BEKK, etc.)\n",
    "    model_name : str\n",
    "        Name of the model for display purposes\n",
    "    returns : np.ndarray\n",
    "        Array of returns\n",
    "    model_params : dict\n",
    "        Dictionary of model parameters\n",
    "        \n",
    "    Returns\n",
    "    -------\n",
    "    Tuple[str, Any]\n",
    "        Tuple containing model name and model results\n",
    "    \"\"\"\n",
    "    print(f\"Starting estimation of {model_name}...\")\n",
    "    \n",
    "    try:\n",
    "        # Create model\n",
    "        model = model_class(**model_params)\n",
    "        \n",
    "        # Use fit_async if available, otherwise use regular fit\n",
    "        if hasattr(model, 'fit_async'):\n",
    "            results = await model.fit_async(returns)\n",
    "        else:\n",
    "            # Run in executor to avoid blocking\n",
    "            loop = asyncio.get_event_loop()\n",
    "            results = await loop.run_in_executor(None, lambda: model.fit(returns))\n",
    "        \n",
    "        print(f\"Completed estimation of {model_name}\")\n",
    "        return model_name, results\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(f\"Error estimating {model_name}: {e}\")\n",
    "        return model_name, None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define an asynchronous function to estimate multiple models concurrently\n",
    "async def estimate_multiple_models_async(returns: np.ndarray) -> Dict[str, Any]:\n",
    "    \"\"\"\n",
    "    Asynchronously estimate multiple multivariate volatility models.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    returns : np.ndarray\n",
    "        Array of returns\n",
    "        \n",
    "    Returns\n",
    "    -------\n",
    "    Dict[str, Any]\n",
    "        Dictionary mapping model names to results\n",
    "    \"\"\"\n",
    "    # Define models to estimate\n",
    "    n_assets = returns.shape[1]\n",
    "    \n",
    "    # Use a subset of assets for BEKK to reduce computational complexity\n",
    "    subset_indices = [0, 1, 2, 3]  # First 4 assets\n",
    "    subset_returns = returns[:, subset_indices]\n",
    "    \n",
    "    models_to_estimate = [\n",
    "        # DCC model\n",
    "        (DCC, \"DCC-GARCH Normal\", returns, {\n",
    "            'n_assets': n_assets,\n",
    "            'p': 1, 'q': 1,\n",
    "            'univariate_model': GARCH,\n",
    "            'univariate_params': {'p': 1, 'q': 1},\n",
    "            'distribution': Normal()\n",
    "        }),\n",
    "        \n",
    "        # DCC model with Student's t distribution\n",
    "        (DCC, \"DCC-GARCH Student's t\", returns, {\n",
    "            'n_assets': n_assets,\n",
    "            'p': 1, 'q': 1,\n",
    "            'univariate_model': GARCH,\n",
    "            'univariate_params': {'p': 1, 'q': 1},\n",
    "            'distribution': StudentT()\n",
    "        }),\n",
    "        \n",
    "        # CCC model\n",
    "        (CCC, \"CCC Normal\", returns, {\n",
    "            'n_assets': n_assets,\n",
    "            'univariate_model': GARCH,\n",
    "            'univariate_params': {'p': 1, 'q': 1},\n",
    "            'distribution': Normal()\n",
    "        }),\n",
    "        \n",
    "        # BEKK model (using subset)\n",
    "        (BEKK, \"BEKK Normal\", subset_returns, {\n",
    "            'n_assets': len(subset_indices),\n",
    "            'p': 1, 'q': 1,\n",
    "            'distribution': Normal()\n",
    "        }),\n",
    "        \n",
    "        # OGARCH model\n",
    "        (OGARCH, \"OGARCH Normal\", returns, {\n",
    "            'n_assets': n_assets,\n",
    "            'n_factors': 3,\n",
    "            'univariate_model': GARCH,\n",
    "            'univariate_params': {'p': 1, 'q': 1},\n",
    "            'distribution': Normal()\n",
    "        }),\n",
    "        \n",
    "        # RiskMetrics model\n",
    "        (RiskMetrics, \"RiskMetrics\", returns, {\n",
    "            'n_assets': n_assets,\n",
    "            'lambda_': 0.94\n",
    "        })\n",
    "    ]\n",
    "    \n",
    "    # Create tasks for each model\n",
    "    tasks = [\n",
    "        estimate_model_async(model_class, model_name, model_returns, model_params)\n",
    "        for model_class, model_name, model_returns, model_params in models_to_estimate\n",
    "    ]\n",
    "    \n",
    "    # Run tasks concurrently\n",
    "    results = await asyncio.gather(*tasks)\n",
    "    \n",
    "    # Process results\n",
    "    model_results = {}\n",
    "    for model_name, result in results:\n",
    "        if result is not None:\n",
    "            model_results[model_name] = result\n",
    "    \n",
    "    return model_results\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run asynchronous estimation\n",
    "async def main():\n",
    "    print(\"Starting asynchronous model estimation...\")\n",
    "    start_time = datetime.datetime.now()\n",
    "    \n",
    "    # Estimate models\n",
    "    model_results = await estimate_multiple_models_async(returns_array)\n",
    "    \n",
    "    # Calculate elapsed time\n",
    "    end_time = datetime.datetime.now()\n",
    "    elapsed_time = (end_time - start_time).total_seconds()\n",
    "    \n",
    "    print(f\"\nEstimated {len(model_results)} models in {elapsed_time:.2f} seconds\")\n",
    "    \n",
    "    # Create comparison DataFrame\n",
    "    comparison = []\n",
    "    for model_name, result in model_results.items():\n",
    "        comparison.append({\n",
    "            'Model': model_name,\n",
    "            'Log-Likelihood': result.log_likelihood,\n",
    "            'AIC': result.aic,\n",
    "            'BIC': result.bic,\n",
    "            'Parameters': len(result.parameters)\n",
    "        })\n",
    "    \n",
    "    comparison_df = pd.DataFrame(comparison)\n",
    "    comparison_df = comparison_df.sort_values('AIC')\n",
    "    \n",
    "    print(\"\nModel Comparison:\")\n",
    "    print(comparison_df)\n",
    "    \n",
    "    # Return results for further analysis\n",
    "    return model_results, comparison_df\n",
    "\n",
    "# Run the async function\n",
    "async_results, async_comparison = await main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate portfolio volatility from asynchronously estimated models\n",
    "async_portfolio_vols = {}\n",
    "\n",
    "for model_name, result in async_results.items():\n",
    "    # Skip BEKK model since it was estimated on a subset\n",
    "    if 'BEKK' in model_name:\n",
    "        continue\n",
    "        \n",
    "    # Calculate portfolio variance\n",
    "    portfolio_var = np.array([portfolio_variance(cov_matrix, equal_weights) \n",
    "                             for cov_matrix in result.conditional_covariance])\n",
    "    async_portfolio_vols[model_name] = np.sqrt(portfolio_var) * np.sqrt(252)  # Annualized\n",
    "\n",
    "# Convert to DataFrame\n",
    "async_portfolio_vols_df = pd.DataFrame(async_portfolio_vols, index=returns_df.index)\n",
    "\n",
    "# Add rolling volatility for comparison\n",
    "async_portfolio_vols_df['Rolling'] = rolling_portfolio_vol\n",
    "\n",
    "# Plot portfolio volatility from different models\n",
    "plt.figure(figsize=(14, 8))\n",
    "\n",
    "for model in async_portfolio_vols_df.columns:\n",
    "    plt.plot(async_portfolio_vols_df.index, async_portfolio_vols_df[model], label=model, linewidth=2, alpha=0.7)\n",
    "\n",
    "plt.title('Portfolio Volatility Estimates from Different Models (Annualized)')\n",
    "plt.ylabel('Volatility (%)')\n",
    "plt.xlabel('Date')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Conclusion\n",
    "\n",
    "In this notebook, we've demonstrated the use of multivariate volatility models in the MFE Toolbox. We've covered:\n",
    "\n",
    "1. Data preparation and exploration for multiple assets\n",
    "2. Correlation analysis and visualization\n",
    "3. Dynamic Conditional Correlation (DCC) model estimation\n",
    "4. BEKK model for covariance dynamics\n",
    "5. Constant Conditional Correlation (CCC) model\n",
    "6. Other multivariate models (OGARCH, RiskMetrics)\n",
    "7. Portfolio volatility estimation and forecasting\n",
    "8. Asynchronous processing for computationally intensive models\n",
    "\n",
    "The MFE Toolbox provides a comprehensive set of multivariate volatility models for analyzing multiple asset returns, implemented using modern Python practices with NumPy, Pandas, and Numba acceleration. The class-based design with type hints and asynchronous processing capabilities makes it both powerful and user-friendly for complex multivariate analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display version information\n",
    "print(f\"MFE Toolbox version: {mfe.__version__}\")\n",
    "print(f\"NumPy version: {np.__version__}\")\n",
    "print(f\"Pandas version: {pd.__version__}\")\n",
    "print(f\"Matplotlib version: {plt.matplotlib.__version__}\")\n",
    "print(f\"Seaborn version: {sns.__version__}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
