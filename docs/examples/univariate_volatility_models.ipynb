{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Univariate Volatility Models\n",
    "\n",
    "This notebook demonstrates the use of univariate volatility models in the MFE Toolbox. We'll cover:\n",
    "\n",
    "1. Introduction to volatility modeling\n",
    "2. Data preparation and exploration\n",
    "3. GARCH model estimation and diagnostics\n",
    "4. Alternative models (EGARCH, TARCH, etc.)\n",
    "5. Forecasting volatility\n",
    "6. Simulation techniques\n",
    "7. Model comparison and selection\n",
    "8. Asynchronous processing for long-running estimations\n",
    "\n",
    "The MFE Toolbox provides a comprehensive set of volatility models for financial time series analysis, implemented using modern Python practices with NumPy, Pandas, and Numba acceleration."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Setup and Imports\n",
    "\n",
    "First, let's import the necessary modules and set up our environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standard imports\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from typing import Dict, List, Tuple, Optional, Union, Any\n",
    "import asyncio\n",
    "import datetime\n",
    "import warnings\n",
    "\n",
    "# MFE Toolbox imports\n",
    "import mfe\n",
    "from mfe.models.univariate import GARCH, EGARCH, TARCH, AGARCH, APARCH, FIGARCH, HEAVY, IGARCH\n",
    "from mfe.models.distributions import Normal, StudentT, GED, SkewedT\n",
    "from mfe.utils.data_transformations import returns_from_prices\n",
    "\n",
    "# Configure plotting\n",
    "plt.style.use('seaborn-v0_8-whitegrid')\n",
    "sns.set_context(\"notebook\", font_scale=1.2)\n",
    "plt.rcParams['figure.figsize'] = (12, 6)\n",
    "plt.rcParams['figure.dpi'] = 100\n",
    "\n",
    "# Display version information\n",
    "print(f\"MFE Toolbox version: {mfe.__version__}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Data Preparation and Exploration\n",
    "\n",
    "We'll use financial market data to demonstrate volatility modeling. Let's load and prepare some stock market data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load sample data (S&P 500 index)\n",
    "# In a real application, you might use yfinance, pandas-datareader, or your own data source\n",
    "# For this example, we'll create a function to download data using pandas-datareader\n",
    "\n",
    "def load_market_data(ticker: str = \"^GSPC\", \n",
    "                     start_date: str = \"2010-01-01\", \n",
    "                     end_date: Optional[str] = None) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Load market data for the specified ticker.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    ticker : str\n",
    "        Ticker symbol to load\n",
    "    start_date : str\n",
    "        Start date in YYYY-MM-DD format\n",
    "    end_date : str, optional\n",
    "        End date in YYYY-MM-DD format, defaults to current date\n",
    "        \n",
    "    Returns\n",
    "    -------\n",
    "    pd.DataFrame\n",
    "        DataFrame containing market data\n",
    "    \"\"\"\n",
    "    try:\n",
    "        import pandas_datareader as pdr\n",
    "        \n",
    "        # Set end date to today if not provided\n",
    "        if end_date is None:\n",
    "            end_date = datetime.datetime.now().strftime(\"%Y-%m-%d\")\n",
    "            \n",
    "        # Download data\n",
    "        data = pdr.get_data_yahoo(ticker, start=start_date, end=end_date)\n",
    "        return data\n",
    "    except ImportError:\n",
    "        print(\"pandas-datareader not installed. Please install with: pip install pandas-datareader\")\n",
    "        # Create synthetic data as fallback\n",
    "        return create_synthetic_market_data(start_date, end_date)\n",
    "\n",
    "def create_synthetic_market_data(start_date: str = \"2010-01-01\", \n",
    "                                end_date: Optional[str] = None) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Create synthetic market data for demonstration purposes.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    start_date : str\n",
    "        Start date in YYYY-MM-DD format\n",
    "    end_date : str, optional\n",
    "        End date in YYYY-MM-DD format, defaults to current date\n",
    "        \n",
    "    Returns\n",
    "    -------\n",
    "    pd.DataFrame\n",
    "        DataFrame containing synthetic market data\n",
    "    \"\"\"\n",
    "    # Parse dates\n",
    "    start = pd.to_datetime(start_date)\n",
    "    if end_date is None:\n",
    "        end = pd.to_datetime(datetime.datetime.now().strftime(\"%Y-%m-%d\"))\n",
    "    else:\n",
    "        end = pd.to_datetime(end_date)\n",
    "    \n",
    "    # Create date range (business days only)\n",
    "    dates = pd.date_range(start=start, end=end, freq='B')\n",
    "    \n",
    "    # Set random seed for reproducibility\n",
    "    np.random.seed(42)\n",
    "    \n",
    "    # Create price series with realistic properties\n",
    "    # - Upward trend\n",
    "    # - Volatility clustering\n",
    "    # - Occasional jumps\n",
    "    n = len(dates)\n",
    "    \n",
    "    # Create returns with volatility clustering\n",
    "    # Simulate GARCH(1,1) process\n",
    "    omega, alpha, beta = 0.00001, 0.1, 0.85\n",
    "    sigma2 = np.zeros(n)\n",
    "    sigma2[0] = omega / (1 - alpha - beta)  # Unconditional variance\n",
    "    \n",
    "    # Generate innovations\n",
    "    z = np.random.normal(0, 1, n)\n",
    "    \n",
    "    # Generate volatility process\n",
    "    for t in range(1, n):\n",
    "        sigma2[t] = omega + alpha * (z[t-1]**2 * sigma2[t-1]) + beta * sigma2[t-1]\n",
    "    \n",
    "    # Generate returns\n",
    "    returns = 0.0005 + np.sqrt(sigma2) * z\n",
    "    \n",
    "    # Add occasional jumps\n",
    "    jump_idx = np.random.choice(range(n), size=int(n*0.01), replace=False)\n",
    "    jump_signs = np.random.choice([-1, 1], size=len(jump_idx))\n",
    "    returns[jump_idx] += jump_signs * np.random.uniform(0.02, 0.05, size=len(jump_idx))\n",
    "    \n",
    "    # Convert returns to prices\n",
    "    prices = 1000 * np.exp(np.cumsum(returns))\n",
    "    \n",
    "    # Create DataFrame\n",
    "    data = pd.DataFrame({\n",
    "        'Open': prices,\n",
    "        'High': prices * np.random.uniform(1.0, 1.02, size=n),\n",
    "        'Low': prices * np.random.uniform(0.98, 1.0, size=n),\n",
    "        'Close': prices,\n",
    "        'Adj Close': prices,\n",
    "        'Volume': np.random.randint(1000000, 10000000, size=n)\n",
    "    }, index=dates)\n",
    "    \n",
    "    # Ensure High >= Open/Close and Low <= Open/Close\n",
    "    data['High'] = data[['Open', 'Close', 'High']].max(axis=1)\n",
    "    data['Low'] = data[['Open', 'Close', 'Low']].min(axis=1)\n",
    "    \n",
    "    return data\n",
    "\n",
    "# Load data\n",
    "try:\n",
    "    market_data = load_market_data(\"^GSPC\", \"2010-01-01\")\n",
    "    print(f\"Loaded {len(market_data)} days of market data\")\n",
    "except Exception as e:\n",
    "    print(f\"Error loading data: {e}\")\n",
    "    print(\"Using synthetic data instead\")\n",
    "    market_data = create_synthetic_market_data(\"2010-01-01\")\n",
    "    print(f\"Created {len(market_data)} days of synthetic data\")\n",
    "\n",
    "# Display the first few rows\n",
    "market_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate returns\n",
    "# We'll use log returns for volatility modeling\n",
    "returns = returns_from_prices(market_data['Adj Close'], log=True) * 100  # Convert to percentage\n",
    "\n",
    "# Create a DataFrame for the returns\n",
    "returns_df = pd.DataFrame({\n",
    "    'Returns': returns\n",
    "}, index=market_data.index[1:])  # Adjust index for returns calculation\n",
    "\n",
    "# Display summary statistics\n",
    "returns_summary = returns_df.describe()\n",
    "print(\"Returns Summary Statistics:\")\n",
    "returns_summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the price series and returns\n",
    "fig, axes = plt.subplots(2, 1, figsize=(14, 10), sharex=True)\n",
    "\n",
    "# Plot price series\n",
    "axes[0].plot(market_data.index, market_data['Adj Close'], color='blue')\n",
    "axes[0].set_title('Price Series')\n",
    "axes[0].set_ylabel('Price')\n",
    "axes[0].grid(True)\n",
    "\n",
    "# Plot returns\n",
    "axes[1].plot(returns_df.index, returns_df['Returns'], color='red', alpha=0.7)\n",
    "axes[1].set_title('Daily Returns (%)')\n",
    "axes[1].set_ylabel('Returns (%)')\n",
    "axes[1].set_xlabel('Date')\n",
    "axes[1].grid(True)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Examine the distribution of returns\n",
    "plt.figure(figsize=(14, 6))\n",
    "\n",
    "# Plot histogram with kernel density estimate\n",
    "sns.histplot(returns_df['Returns'], kde=True, stat='density', bins=50)\n",
    "\n",
    "# Add normal distribution for comparison\n",
    "x = np.linspace(returns_df['Returns'].min(), returns_df['Returns'].max(), 100)\n",
    "mean = returns_df['Returns'].mean()\n",
    "std = returns_df['Returns'].std()\n",
    "normal_pdf = (1 / (std * np.sqrt(2 * np.pi))) * np.exp(-(x - mean)**2 / (2 * std**2))\n",
    "plt.plot(x, normal_pdf, 'r-', label='Normal Distribution')\n",
    "\n",
    "plt.title('Distribution of Returns')\n",
    "plt.xlabel('Returns (%)')\n",
    "plt.ylabel('Density')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()\n",
    "\n",
    "# Calculate and display skewness and kurtosis\n",
    "skewness = returns_df['Returns'].skew()\n",
    "kurtosis = returns_df['Returns'].kurtosis()  # Excess kurtosis\n",
    "\n",
    "print(f\"Skewness: {skewness:.4f}\")\n",
    "print(f\"Excess Kurtosis: {kurtosis:.4f}\")\n",
    "\n",
    "# Check for autocorrelation in returns and squared returns\n",
    "from statsmodels.graphics.tsaplots import plot_acf\n",
    "\n",
    "fig, axes = plt.subplots(2, 1, figsize=(14, 10))\n",
    "\n",
    "# ACF of returns\n",
    "plot_acf(returns_df['Returns'].dropna(), lags=30, ax=axes[0], title='ACF of Returns')\n",
    "axes[0].set_xlabel('Lag')\n",
    "axes[0].set_ylabel('Autocorrelation')\n",
    "\n",
    "# ACF of squared returns (proxy for volatility)\n",
    "plot_acf(returns_df['Returns'].dropna()**2, lags=30, ax=axes[1], title='ACF of Squared Returns')\n",
    "axes[1].set_xlabel('Lag')\n",
    "axes[1].set_ylabel('Autocorrelation')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. GARCH Model Estimation and Diagnostics\n",
    "\n",
    "Now that we've explored the data, let's estimate a GARCH(1,1) model with normal innovations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare data for GARCH estimation\n",
    "# Convert returns to NumPy array\n",
    "returns_array = returns_df['Returns'].values\n",
    "\n",
    "# Create and estimate a GARCH(1,1) model with normal innovations\n",
    "garch_model = GARCH(p=1, q=1, distribution=Normal())\n",
    "\n",
    "# Fit the model\n",
    "garch_results = garch_model.fit(returns_array)\n",
    "\n",
    "# Display estimation results\n",
    "print(\"GARCH(1,1) Estimation Results:\")\n",
    "print(f\"Log-Likelihood: {garch_results.log_likelihood:.4f}\")\n",
    "print(f\"AIC: {garch_results.aic:.4f}\")\n",
    "print(f\"BIC: {garch_results.bic:.4f}\")\n",
    "print(\"\nParameter Estimates:\")\n",
    "for name, value, std_err, t_stat, p_value in zip(\n",
    "    garch_results.parameter_names,\n",
    "    garch_results.parameters,\n",
    "    garch_results.std_errors,\n",
    "    garch_results.t_stats,\n",
    "    garch_results.p_values\n",
    "):\n",
    "    print(f\"{name}: {value:.6f} (SE: {std_err:.6f}, t: {t_stat:.4f}, p: {p_value:.4f})\")\n",
    "\n",
    "# Check persistence\n",
    "alpha = garch_results.parameters[1]  # ARCH parameter\n",
    "beta = garch_results.parameters[2]   # GARCH parameter\n",
    "persistence = alpha + beta\n",
    "print(f\"\nPersistence (α + β): {persistence:.4f}\")\n",
    "print(f\"Half-life: {np.log(0.5) / np.log(persistence):.2f} days\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract conditional volatility\n",
    "conditional_variance = garch_results.conditional_variance\n",
    "conditional_volatility = np.sqrt(conditional_variance)\n",
    "\n",
    "# Create a DataFrame with returns and conditional volatility\n",
    "volatility_df = pd.DataFrame({\n",
    "    'Returns': returns_df['Returns'],\n",
    "    'Conditional Volatility': conditional_volatility\n",
    "}, index=returns_df.index)\n",
    "\n",
    "# Plot returns and conditional volatility\n",
    "fig, axes = plt.subplots(2, 1, figsize=(14, 10), sharex=True)\n",
    "\n",
    "# Plot returns\n",
    "axes[0].plot(volatility_df.index, volatility_df['Returns'], color='blue', alpha=0.7)\n",
    "axes[0].set_title('Returns (%)')\n",
    "axes[0].set_ylabel('Returns (%)')\n",
    "axes[0].grid(True)\n",
    "\n",
    "# Plot conditional volatility\n",
    "axes[1].plot(volatility_df.index, volatility_df['Conditional Volatility'], color='red')\n",
    "axes[1].set_title('GARCH(1,1) Conditional Volatility (%)')\n",
    "axes[1].set_ylabel('Volatility (%)')\n",
    "axes[1].set_xlabel('Date')\n",
    "axes[1].grid(True)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze standardized residuals\n",
    "std_residuals = garch_results.standardized_residuals\n",
    "\n",
    "# Create a DataFrame for standardized residuals\n",
    "std_residuals_df = pd.DataFrame({\n",
    "    'Standardized Residuals': std_residuals\n",
    "}, index=returns_df.index)\n",
    "\n",
    "# Plot standardized residuals\n",
    "plt.figure(figsize=(14, 6))\n",
    "plt.plot(std_residuals_df.index, std_residuals_df['Standardized Residuals'], color='blue', alpha=0.7)\n",
    "plt.title('Standardized Residuals')\n",
    "plt.ylabel('Standardized Residuals')\n",
    "plt.xlabel('Date')\n",
    "plt.grid(True)\n",
    "plt.show()\n",
    "\n",
    "# Check distribution of standardized residuals\n",
    "plt.figure(figsize=(14, 6))\n",
    "\n",
    "# Plot histogram with kernel density estimate\n",
    "sns.histplot(std_residuals_df['Standardized Residuals'], kde=True, stat='density', bins=50)\n",
    "\n",
    "# Add normal distribution for comparison\n",
    "x = np.linspace(std_residuals_df['Standardized Residuals'].min(), \n",
    "                std_residuals_df['Standardized Residuals'].max(), 100)\n",
    "normal_pdf = (1 / np.sqrt(2 * np.pi)) * np.exp(-x**2 / 2)\n",
    "plt.plot(x, normal_pdf, 'r-', label='Standard Normal')\n",
    "\n",
    "plt.title('Distribution of Standardized Residuals')\n",
    "plt.xlabel('Standardized Residuals')\n",
    "plt.ylabel('Density')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()\n",
    "\n",
    "# Calculate and display skewness and kurtosis of standardized residuals\n",
    "skewness = std_residuals_df['Standardized Residuals'].skew()\n",
    "kurtosis = std_residuals_df['Standardized Residuals'].kurtosis()  # Excess kurtosis\n",
    "\n",
    "print(f\"Skewness of Standardized Residuals: {skewness:.4f}\")\n",
    "print(f\"Excess Kurtosis of Standardized Residuals: {kurtosis:.4f}\")\n",
    "\n",
    "# Check for autocorrelation in standardized residuals and squared standardized residuals\n",
    "fig, axes = plt.subplots(2, 1, figsize=(14, 10))\n",
    "\n",
    "# ACF of standardized residuals\n",
    "plot_acf(std_residuals_df['Standardized Residuals'].dropna(), \n",
    "         lags=30, ax=axes[0], title='ACF of Standardized Residuals')\n",
    "axes[0].set_xlabel('Lag')\n",
    "axes[0].set_ylabel('Autocorrelation')\n",
    "\n",
    "# ACF of squared standardized residuals\n",
    "plot_acf(std_residuals_df['Standardized Residuals'].dropna()**2, \n",
    "         lags=30, ax=axes[1], title='ACF of Squared Standardized Residuals')\n",
    "axes[1].set_xlabel('Lag')\n",
    "axes[1].set_ylabel('Autocorrelation')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perform Ljung-Box test on standardized residuals and squared standardized residuals\n",
    "from statsmodels.stats.diagnostic import acorr_ljungbox\n",
    "\n",
    "# Ljung-Box test on standardized residuals (test for serial correlation)\n",
    "lb_test = acorr_ljungbox(std_residuals_df['Standardized Residuals'].dropna(), lags=[10, 15, 20])\n",
    "print(\"Ljung-Box Test on Standardized Residuals:\")\n",
    "for i, lag in enumerate([10, 15, 20]):\n",
    "    print(f\"Lag {lag}: Q-stat = {lb_test.iloc[i, 0]:.4f}, p-value = {lb_test.iloc[i, 1]:.4f}\")\n",
    "\n",
    "# Ljung-Box test on squared standardized residuals (test for ARCH effects)\n",
    "lb_test_squared = acorr_ljungbox(std_residuals_df['Standardized Residuals'].dropna()**2, lags=[10, 15, 20])\n",
    "print(\"\nLjung-Box Test on Squared Standardized Residuals:\")\n",
    "for i, lag in enumerate([10, 15, 20]):\n",
    "    print(f\"Lag {lag}: Q-stat = {lb_test_squared.iloc[i, 0]:.4f}, p-value = {lb_test_squared.iloc[i, 1]:.4f}\")\n",
    "\n",
    "# Jarque-Bera test for normality\n",
    "from scipy import stats\n",
    "jb_stat, jb_pvalue = stats.jarque_bera(std_residuals_df['Standardized Residuals'].dropna())\n",
    "print(f\"\nJarque-Bera Test for Normality:\")\n",
    "print(f\"JB statistic = {jb_stat:.4f}, p-value = {jb_pvalue:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Alternative Models (EGARCH, TARCH, etc.)\n",
    "\n",
    "Let's estimate alternative volatility models and compare their performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a function to estimate and summarize different volatility models\n",
    "def estimate_volatility_model(model_class, model_name: str, returns: np.ndarray, \n",
    "                             p: int = 1, q: int = 1, distribution=None) -> Tuple[Any, pd.DataFrame]:\n",
    "    \"\"\"\n",
    "    Estimate a volatility model and return results summary.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    model_class : class\n",
    "        Volatility model class (GARCH, EGARCH, etc.)\n",
    "    model_name : str\n",
    "        Name of the model for display purposes\n",
    "    returns : np.ndarray\n",
    "        Array of returns\n",
    "    p : int, optional\n",
    "        ARCH order, default is 1\n",
    "    q : int, optional\n",
    "        GARCH order, default is 1\n",
    "    distribution : Distribution, optional\n",
    "        Error distribution, default is Normal()\n",
    "        \n",
    "    Returns\n",
    "    -------\n",
    "    Tuple[Any, pd.DataFrame]\n",
    "        Tuple containing model results and summary DataFrame\n",
    "    \"\"\"\n",
    "    # Set default distribution if not provided\n",
    "    if distribution is None:\n",
    "        distribution = Normal()\n",
    "    \n",
    "    # Create and estimate model\n",
    "    model = model_class(p=p, q=q, distribution=distribution)\n",
    "    results = model.fit(returns)\n",
    "    \n",
    "    # Create summary DataFrame\n",
    "    summary = pd.DataFrame({\n",
    "        'Parameter': results.parameter_names,\n",
    "        'Estimate': results.parameters,\n",
    "        'Std. Error': results.std_errors,\n",
    "        't-statistic': results.t_stats,\n",
    "        'p-value': results.p_values\n",
    "    })\n",
    "    \n",
    "    # Print summary\n",
    "    print(f\"{model_name} Estimation Results:\")\n",
    "    print(f\"Log-Likelihood: {results.log_likelihood:.4f}\")\n",
    "    print(f\"AIC: {results.aic:.4f}\")\n",
    "    print(f\"BIC: {results.bic:.4f}\")\n",
    "    print(\"\nParameter Estimates:\")\n",
    "    print(summary.to_string(index=False))\n",
    "    print(\"\n\" + \"-\"*80)\n",
    "    \n",
    "    return results, summary\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Estimate different volatility models\n",
    "# We'll compare GARCH, EGARCH, and TARCH models with normal innovations\n",
    "\n",
    "# GARCH(1,1) with normal innovations\n",
    "garch_results, garch_summary = estimate_volatility_model(\n",
    "    GARCH, \"GARCH(1,1)\", returns_array, p=1, q=1, distribution=Normal()\n",
    ")\n",
    "\n",
    "# EGARCH(1,1) with normal innovations\n",
    "egarch_results, egarch_summary = estimate_volatility_model(\n",
    "    EGARCH, \"EGARCH(1,1)\", returns_array, p=1, q=1, distribution=Normal()\n",
    ")\n",
    "\n",
    "# TARCH(1,1) with normal innovations\n",
    "tarch_results, tarch_summary = estimate_volatility_model(\n",
    "    TARCH, \"TARCH(1,1)\", returns_array, p=1, q=1, distribution=Normal()\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare model fit\n",
    "model_comparison = pd.DataFrame({\n",
    "    'Model': ['GARCH(1,1)', 'EGARCH(1,1)', 'TARCH(1,1)'],\n",
    "    'Log-Likelihood': [\n",
    "        garch_results.log_likelihood,\n",
    "        egarch_results.log_likelihood,\n",
    "        tarch_results.log_likelihood\n",
    "    ],\n",
    "    'AIC': [\n",
    "        garch_results.aic,\n",
    "        egarch_results.aic,\n",
    "        tarch_results.aic\n",
    "    ],\n",
    "    'BIC': [\n",
    "        garch_results.bic,\n",
    "        egarch_results.bic,\n",
    "        tarch_results.bic\n",
    "    ]\n",
    "})\n",
    "\n",
    "print(\"Model Comparison:\")\n",
    "model_comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare conditional volatility from different models\n",
    "volatility_comparison = pd.DataFrame({\n",
    "    'Returns': returns_df['Returns'],\n",
    "    'GARCH(1,1)': np.sqrt(garch_results.conditional_variance),\n",
    "    'EGARCH(1,1)': np.sqrt(egarch_results.conditional_variance),\n",
    "    'TARCH(1,1)': np.sqrt(tarch_results.conditional_variance)\n",
    "}, index=returns_df.index)\n",
    "\n",
    "# Plot volatility comparison\n",
    "plt.figure(figsize=(14, 8))\n",
    "plt.plot(volatility_comparison.index, volatility_comparison['GARCH(1,1)'], label='GARCH(1,1)', linewidth=1.5)\n",
    "plt.plot(volatility_comparison.index, volatility_comparison['EGARCH(1,1)'], label='EGARCH(1,1)', linewidth=1.5)\n",
    "plt.plot(volatility_comparison.index, volatility_comparison['TARCH(1,1)'], label='TARCH(1,1)', linewidth=1.5)\n",
    "plt.title('Conditional Volatility Comparison')\n",
    "plt.ylabel('Volatility (%)')\n",
    "plt.xlabel('Date')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()\n",
    "\n",
    "# Plot volatility differences\n",
    "plt.figure(figsize=(14, 8))\n",
    "plt.plot(volatility_comparison.index, \n",
    "         volatility_comparison['EGARCH(1,1)'] - volatility_comparison['GARCH(1,1)'], \n",
    "         label='EGARCH - GARCH', linewidth=1.5)\n",
    "plt.plot(volatility_comparison.index, \n",
    "         volatility_comparison['TARCH(1,1)'] - volatility_comparison['GARCH(1,1)'], \n",
    "         label='TARCH - GARCH', linewidth=1.5)\n",
    "plt.title('Volatility Differences Relative to GARCH(1,1)')\n",
    "plt.ylabel('Volatility Difference (%)')\n",
    "plt.xlabel('Date')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.axhline(y=0, color='black', linestyle='--')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Estimate GARCH models with different error distributions\n",
    "# We'll compare Normal, Student's t, and GED distributions\n",
    "\n",
    "# GARCH(1,1) with Student's t innovations\n",
    "garch_t_results, garch_t_summary = estimate_volatility_model(\n",
    "    GARCH, \"GARCH(1,1) with Student's t\", returns_array, p=1, q=1, \n",
    "    distribution=StudentT(nu=5)  # Starting value for degrees of freedom\n",
    ")\n",
    "\n",
    "# GARCH(1,1) with GED innovations\n",
    "garch_ged_results, garch_ged_summary = estimate_volatility_model(\n",
    "    GARCH, \"GARCH(1,1) with GED\", returns_array, p=1, q=1, \n",
    "    distribution=GED(nu=1.5)  # Starting value for shape parameter\n",
    ")\n",
    "\n",
    "# GARCH(1,1) with Skewed t innovations\n",
    "garch_skewt_results, garch_skewt_summary = estimate_volatility_model(\n",
    "    GARCH, \"GARCH(1,1) with Skewed t\", returns_array, p=1, q=1, \n",
    "    distribution=SkewedT(nu=5, lambda_=0)  # Starting values\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare model fit with different distributions\n",
    "distribution_comparison = pd.DataFrame({\n",
    "    'Model': ['GARCH(1,1) Normal', 'GARCH(1,1) Student\'s t', 'GARCH(1,1) GED', 'GARCH(1,1) Skewed t'],\n",
    "    'Log-Likelihood': [\n",
    "        garch_results.log_likelihood,\n",
    "        garch_t_results.log_likelihood,\n",
    "        garch_ged_results.log_likelihood,\n",
    "        garch_skewt_results.log_likelihood\n",
    "    ],\n",
    "    'AIC': [\n",
    "        garch_results.aic,\n",
    "        garch_t_results.aic,\n",
    "        garch_ged_results.aic,\n",
    "        garch_skewt_results.aic\n",
    "    ],\n",
    "    'BIC': [\n",
    "        garch_results.bic,\n",
    "        garch_t_results.bic,\n",
    "        garch_ged_results.bic,\n",
    "        garch_skewt_results.bic\n",
    "    ]\n",
    "})\n",
    "\n",
    "print(\"Model Comparison with Different Distributions:\")\n",
    "distribution_comparison"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Forecasting Volatility\n",
    "\n",
    "Now let's use our estimated models to forecast future volatility."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate volatility forecasts\n",
    "# We'll use the best-performing model based on AIC/BIC\n",
    "\n",
    "# Determine the best model\n",
    "best_model_idx = distribution_comparison['AIC'].idxmin()\n",
    "best_model_name = distribution_comparison.loc[best_model_idx, 'Model']\n",
    "print(f\"Best model based on AIC: {best_model_name}\")\n",
    "\n",
    "# Select the corresponding model results\n",
    "if best_model_name == 'GARCH(1,1) Normal':\n",
    "    best_model_results = garch_results\n",
    "    best_model = GARCH(p=1, q=1, distribution=Normal())\n",
    "elif best_model_name == \"GARCH(1,1) Student's t\":\n",
    "    best_model_results = garch_t_results\n",
    "    best_model = GARCH(p=1, q=1, distribution=StudentT())\n",
    "elif best_model_name == 'GARCH(1,1) GED':\n",
    "    best_model_results = garch_ged_results\n",
    "    best_model = GARCH(p=1, q=1, distribution=GED())\n",
    "else:  # 'GARCH(1,1) Skewed t'\n",
    "    best_model_results = garch_skewt_results\n",
    "    best_model = GARCH(p=1, q=1, distribution=SkewedT())\n",
    "\n",
    "# Set forecast horizon\n",
    "forecast_horizon = 30  # 30 days ahead\n",
    "\n",
    "# Generate forecasts\n",
    "volatility_forecasts = best_model.forecast(\n",
    "    returns_array,\n",
    "    best_model_results.parameters,\n",
    "    horizon=forecast_horizon,\n",
    "    n_simulations=10000  # Number of Monte Carlo simulations\n",
    ")\n",
    "\n",
    "# Extract forecast results\n",
    "forecast_mean = volatility_forecasts.mean\n",
    "forecast_std = volatility_forecasts.std\n",
    "forecast_quantiles = volatility_forecasts.quantiles\n",
    "\n",
    "# Create forecast dates\n",
    "last_date = returns_df.index[-1]\n",
    "forecast_dates = pd.date_range(start=last_date + pd.Timedelta(days=1), periods=forecast_horizon, freq='B')\n",
    "\n",
    "# Create DataFrame for forecasts\n",
    "forecast_df = pd.DataFrame({\n",
    "    'Mean': np.sqrt(forecast_mean),  # Convert variance to volatility\n",
    "    'Lower_95': np.sqrt(forecast_quantiles[0.025]),  # 2.5% quantile\n",
    "    'Upper_95': np.sqrt(forecast_quantiles[0.975])   # 97.5% quantile\n",
    "}, index=forecast_dates)\n",
    "\n",
    "# Display forecast summary\n",
    "print(\"\nVolatility Forecast Summary:\")\n",
    "forecast_df.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot volatility forecasts\n",
    "plt.figure(figsize=(14, 8))\n",
    "\n",
    "# Plot historical volatility\n",
    "historical_vol = np.sqrt(best_model_results.conditional_variance)\n",
    "plt.plot(returns_df.index[-60:], historical_vol[-60:], label='Historical Volatility', color='blue')\n",
    "\n",
    "# Plot forecasted volatility\n",
    "plt.plot(forecast_df.index, forecast_df['Mean'], label='Forecasted Volatility', color='red')\n",
    "\n",
    "# Plot confidence intervals\n",
    "plt.fill_between(forecast_df.index, \n",
    "                 forecast_df['Lower_95'], \n",
    "                 forecast_df['Upper_95'], \n",
    "                 color='red', alpha=0.2, label='95% Confidence Interval')\n",
    "\n",
    "# Add vertical line to separate historical and forecasted periods\n",
    "plt.axvline(x=returns_df.index[-1], color='black', linestyle='--', label='Forecast Start')\n",
    "\n",
    "plt.title(f'Volatility Forecast using {best_model_name}')\n",
    "plt.ylabel('Volatility (%)')\n",
    "plt.xlabel('Date')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate term structure of volatility\n",
    "# This shows how volatility is expected to evolve over different horizons\n",
    "\n",
    "plt.figure(figsize=(14, 8))\n",
    "plt.plot(range(1, forecast_horizon + 1), forecast_df['Mean'], marker='o', linestyle='-', color='blue')\n",
    "plt.fill_between(range(1, forecast_horizon + 1), \n",
    "                 forecast_df['Lower_95'], \n",
    "                 forecast_df['Upper_95'], \n",
    "                 color='blue', alpha=0.2)\n",
    "\n",
    "# Add horizontal line for unconditional volatility\n",
    "omega = best_model_results.parameters[0]  # Constant term\n",
    "alpha = best_model_results.parameters[1]  # ARCH term\n",
    "beta = best_model_results.parameters[2]   # GARCH term\n",
    "unconditional_variance = omega / (1 - alpha - beta)\n",
    "unconditional_volatility = np.sqrt(unconditional_variance)\n",
    "plt.axhline(y=unconditional_volatility, color='red', linestyle='--', \n",
    "            label=f'Unconditional Volatility: {unconditional_volatility:.4f}%')\n",
    "\n",
    "plt.title('Term Structure of Volatility')\n",
    "plt.xlabel('Forecast Horizon (Days)')\n",
    "plt.ylabel('Volatility (%)')\n",
    "plt.grid(True)\n",
    "plt.legend()\n",
    "plt.xticks(range(0, forecast_horizon + 1, 5))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Simulation Techniques\n",
    "\n",
    "Let's demonstrate how to simulate returns from our estimated volatility models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simulate returns from the best model\n",
    "# We'll simulate multiple paths to understand the distribution of potential outcomes\n",
    "\n",
    "# Set simulation parameters\n",
    "n_simulations = 5  # Number of paths to plot (we'll simulate more but only plot a few)\n",
    "simulation_length = 252  # One year of trading days\n",
    "total_simulations = 1000  # Total number of simulations for statistics\n",
    "\n",
    "# Simulate returns\n",
    "simulated_returns = best_model.simulate(\n",
    "    best_model_results.parameters,\n",
    "    simulation_length,\n",
    "    n_simulations=total_simulations,\n",
    "    initial_value=returns_array[-1],  # Start from the last observed return\n",
    "    initial_variance=best_model_results.conditional_variance[-1]  # Start from the last conditional variance\n",
    ")\n",
    "\n",
    "# Extract simulated returns and conditional variances\n",
    "sim_returns = simulated_returns.returns\n",
    "sim_variances = simulated_returns.variances\n",
    "\n",
    "# Create simulation dates\n",
    "last_date = returns_df.index[-1]\n",
    "sim_dates = pd.date_range(start=last_date + pd.Timedelta(days=1), periods=simulation_length, freq='B')\n",
    "\n",
    "# Plot simulated returns\n",
    "plt.figure(figsize=(14, 8))\n",
    "\n",
    "# Plot a few simulated paths\n",
    "for i in range(n_simulations):\n",
    "    plt.plot(sim_dates, sim_returns[:, i], alpha=0.7, label=f'Simulation {i+1}')\n",
    "\n",
    "plt.title('Simulated Returns Paths')\n",
    "plt.ylabel('Returns (%)')\n",
    "plt.xlabel('Date')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()\n",
    "\n",
    "# Plot simulated volatility\n",
    "plt.figure(figsize=(14, 8))\n",
    "\n",
    "# Plot a few simulated volatility paths\n",
    "for i in range(n_simulations):\n",
    "    plt.plot(sim_dates, np.sqrt(sim_variances[:, i]), alpha=0.7, label=f'Simulation {i+1}')\n",
    "\n",
    "# Add horizontal line for unconditional volatility\n",
    "plt.axhline(y=unconditional_volatility, color='red', linestyle='--', \n",
    "            label=f'Unconditional Volatility: {unconditional_volatility:.4f}%')\n",
    "\n",
    "plt.title('Simulated Volatility Paths')\n",
    "plt.ylabel('Volatility (%)')\n",
    "plt.xlabel('Date')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate price paths from simulated returns\n",
    "# We'll start from the last observed price\n",
    "last_price = market_data['Adj Close'].iloc[-1]\n",
    "\n",
    "# Convert percentage returns to decimal\n",
    "sim_returns_decimal = sim_returns / 100\n",
    "\n",
    "# Calculate price paths\n",
    "sim_prices = np.zeros((simulation_length, total_simulations))\n",
    "for i in range(total_simulations):\n",
    "    sim_prices[:, i] = last_price * np.cumprod(1 + sim_returns_decimal[:, i])\n",
    "\n",
    "# Plot simulated price paths\n",
    "plt.figure(figsize=(14, 8))\n",
    "\n",
    "# Plot historical prices (last 60 days)\n",
    "historical_dates = market_data.index[-60:]\n",
    "historical_prices = market_data['Adj Close'].iloc[-60:]\n",
    "plt.plot(historical_dates, historical_prices, color='blue', linewidth=2, label='Historical')\n",
    "\n",
    "# Plot a few simulated paths\n",
    "for i in range(n_simulations):\n",
    "    plt.plot(sim_dates, sim_prices[:, i], alpha=0.7, label=f'Simulation {i+1}')\n",
    "\n",
    "# Add vertical line to separate historical and simulated periods\n",
    "plt.axvline(x=market_data.index[-1], color='black', linestyle='--', label='Simulation Start')\n",
    "\n",
    "plt.title('Simulated Price Paths')\n",
    "plt.ylabel('Price')\n",
    "plt.xlabel('Date')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate statistics from simulations\n",
    "# We'll look at the distribution of final prices and maximum drawdowns\n",
    "\n",
    "# Final prices\n",
    "final_prices = sim_prices[-1, :]\n",
    "\n",
    "# Calculate returns from initial price\n",
    "total_returns = (final_prices / last_price - 1) * 100\n",
    "\n",
    "# Calculate drawdowns for each path\n",
    "max_drawdowns = np.zeros(total_simulations)\n",
    "for i in range(total_simulations):\n",
    "    # Calculate running maximum\n",
    "    running_max = np.maximum.accumulate(sim_prices[:, i])\n",
    "    # Calculate drawdowns\n",
    "    drawdowns = (sim_prices[:, i] / running_max - 1) * 100\n",
    "    # Find maximum drawdown\n",
    "    max_drawdowns[i] = np.min(drawdowns)\n",
    "\n",
    "# Create a DataFrame with simulation statistics\n",
    "sim_stats = pd.DataFrame({\n",
    "    'Final Price': final_prices,\n",
    "    'Total Return (%)': total_returns,\n",
    "    'Max Drawdown (%)': max_drawdowns\n",
    "})\n",
    "\n",
    "# Display summary statistics\n",
    "print(\"Simulation Statistics:\")\n",
    "sim_stats.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot distribution of final prices\n",
    "plt.figure(figsize=(14, 6))\n",
    "sns.histplot(sim_stats['Final Price'], kde=True, bins=50)\n",
    "plt.axvline(x=last_price, color='red', linestyle='--', \n",
    "            label=f'Initial Price: {last_price:.2f}')\n",
    "plt.title('Distribution of Final Prices')\n",
    "plt.xlabel('Price')\n",
    "plt.ylabel('Frequency')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()\n",
    "\n",
    "# Plot distribution of total returns\n",
    "plt.figure(figsize=(14, 6))\n",
    "sns.histplot(sim_stats['Total Return (%)'], kde=True, bins=50)\n",
    "plt.axvline(x=0, color='red', linestyle='--', label='Zero Return')\n",
    "plt.title('Distribution of Total Returns')\n",
    "plt.xlabel('Return (%)')\n",
    "plt.ylabel('Frequency')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()\n",
    "\n",
    "# Plot distribution of maximum drawdowns\n",
    "plt.figure(figsize=(14, 6))\n",
    "sns.histplot(sim_stats['Max Drawdown (%)'], kde=True, bins=50)\n",
    "plt.title('Distribution of Maximum Drawdowns')\n",
    "plt.xlabel('Drawdown (%)')\n",
    "plt.ylabel('Frequency')\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate Value-at-Risk (VaR) and Expected Shortfall (ES) from simulations\n",
    "# We'll calculate these risk measures for different horizons\n",
    "\n",
    "# Define horizons (in trading days)\n",
    "horizons = [5, 10, 21, 63, 126, 252]  # 1 week, 2 weeks, 1 month, 3 months, 6 months, 1 year\n",
    "\n",
    "# Calculate cumulative returns for each horizon\n",
    "horizon_returns = {}\n",
    "for horizon in horizons:\n",
    "    if horizon <= simulation_length:\n",
    "        # Calculate price at horizon\n",
    "        horizon_prices = sim_prices[horizon-1, :]\n",
    "        # Calculate return from initial price\n",
    "        horizon_returns[horizon] = (horizon_prices / last_price - 1) * 100\n",
    "\n",
    "# Calculate VaR and ES for each horizon\n",
    "confidence_levels = [0.95, 0.99]\n",
    "risk_measures = pd.DataFrame(index=horizons)\n",
    "\n",
    "for confidence in confidence_levels:\n",
    "    var_column = f'VaR {confidence*100:.0f}%'\n",
    "    es_column = f'ES {confidence*100:.0f}%'\n",
    "    \n",
    "    for horizon in horizons:\n",
    "        if horizon in horizon_returns:\n",
    "            # Calculate VaR (negative of the return at the specified percentile)\n",
    "            var = -np.percentile(horizon_returns[horizon], 100 * (1 - confidence))\n",
    "            risk_measures.loc[horizon, var_column] = var\n",
    "            \n",
    "            # Calculate ES (negative of the mean of returns beyond VaR)\n",
    "            threshold = -var\n",
    "            beyond_var = horizon_returns[horizon][horizon_returns[horizon] <= threshold]\n",
    "            es = -beyond_var.mean() if len(beyond_var) > 0 else var\n",
    "            risk_measures.loc[horizon, es_column] = es\n",
    "\n",
    "# Add column for horizon names\n",
    "horizon_names = {\n",
    "    5: '1 Week',\n",
    "    10: '2 Weeks',\n",
    "    21: '1 Month',\n",
    "    63: '3 Months',\n",
    "    126: '6 Months',\n",
    "    252: '1 Year'\n",
    "}\n",
    "risk_measures['Horizon'] = [horizon_names.get(h, f'{h} Days') for h in horizons]\n",
    "\n",
    "# Reorder columns\n",
    "cols = ['Horizon'] + [col for col in risk_measures.columns if col != 'Horizon']\n",
    "risk_measures = risk_measures[cols]\n",
    "\n",
    "# Display risk measures\n",
    "print(\"Risk Measures from Simulations:\")\n",
    "risk_measures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot VaR and ES across horizons\n",
    "plt.figure(figsize=(14, 8))\n",
    "\n",
    "# Plot VaR\n",
    "plt.plot(horizons, risk_measures['VaR 95%'], marker='o', label='VaR 95%')\n",
    "plt.plot(horizons, risk_measures['VaR 99%'], marker='s', label='VaR 99%')\n",
    "\n",
    "# Plot ES\n",
    "plt.plot(horizons, risk_measures['ES 95%'], marker='^', linestyle='--', label='ES 95%')\n",
    "plt.plot(horizons, risk_measures['ES 99%'], marker='d', linestyle='--', label='ES 99%')\n",
    "\n",
    "plt.title('Value-at-Risk and Expected Shortfall Across Horizons')\n",
    "plt.xlabel('Horizon (Trading Days)')\n",
    "plt.ylabel('Risk Measure (%)')\n",
    "plt.grid(True)\n",
    "plt.legend()\n",
    "plt.xticks(horizons, [horizon_names.get(h, f'{h} Days') for h in horizons], rotation=45)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Model Comparison and Selection\n",
    "\n",
    "Let's compare different model specifications and select the best model based on information criteria and out-of-sample performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a function to estimate models with different specifications\n",
    "def estimate_model_grid(returns: np.ndarray, model_class, distribution_class, \n",
    "                        p_values: List[int], q_values: List[int]) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Estimate models with different specifications and return comparison DataFrame.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    returns : np.ndarray\n",
    "        Array of returns\n",
    "    model_class : class\n",
    "        Volatility model class (GARCH, EGARCH, etc.)\n",
    "    distribution_class : class\n",
    "        Distribution class (Normal, StudentT, etc.)\n",
    "    p_values : List[int]\n",
    "        List of ARCH orders to try\n",
    "    q_values : List[int]\n",
    "        List of GARCH orders to try\n",
    "        \n",
    "    Returns\n",
    "    -------\n",
    "    pd.DataFrame\n",
    "        DataFrame with model comparison results\n",
    "    \"\"\"\n",
    "    results = []\n",
    "    \n",
    "    for p in p_values:\n",
    "        for q in q_values:\n",
    "            model_name = f\"{model_class.__name__}({p},{q}) with {distribution_class.__name__}\"\n",
    "            print(f\"Estimating {model_name}...\")\n",
    "            \n",
    "            try:\n",
    "                # Create and estimate model\n",
    "                model = model_class(p=p, q=q, distribution=distribution_class())\n",
    "                model_results = model.fit(returns)\n",
    "                \n",
    "                # Store results\n",
    "                results.append({\n",
    "                    'Model': model_name,\n",
    "                    'p': p,\n",
    "                    'q': q,\n",
    "                    'Distribution': distribution_class.__name__,\n",
    "                    'Log-Likelihood': model_results.log_likelihood,\n",
    "                    'AIC': model_results.aic,\n",
    "                    'BIC': model_results.bic,\n",
    "                    'Parameters': len(model_results.parameters),\n",
    "                    'Convergence': model_results.convergence_status\n",
    "                })\n",
    "            except Exception as e:\n",
    "                print(f\"Error estimating {model_name}: {e}\")\n",
    "                results.append({\n",
    "                    'Model': model_name,\n",
    "                    'p': p,\n",
    "                    'q': q,\n",
    "                    'Distribution': distribution_class.__name__,\n",
    "                    'Log-Likelihood': np.nan,\n",
    "                    'AIC': np.nan,\n",
    "                    'BIC': np.nan,\n",
    "                    'Parameters': np.nan,\n",
    "                    'Convergence': 'Failed'\n",
    "                })\n",
    "    \n",
    "    # Create DataFrame\n",
    "    results_df = pd.DataFrame(results)\n",
    "    \n",
    "    # Sort by AIC\n",
    "    results_df = results_df.sort_values('AIC')\n",
    "    \n",
    "    return results_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Estimate GARCH models with different specifications\n",
    "# We'll try different orders (p,q) and distributions\n",
    "\n",
    "# Define model specifications\n",
    "p_values = [1, 2]  # ARCH orders\n",
    "q_values = [1, 2]  # GARCH orders\n",
    "\n",
    "# Estimate GARCH models with normal distribution\n",
    "garch_normal_results = estimate_model_grid(\n",
    "    returns_array, GARCH, Normal, p_values, q_values\n",
    ")\n",
    "\n",
    "# Estimate GARCH models with Student's t distribution\n",
    "garch_t_results = estimate_model_grid(\n",
    "    returns_array, GARCH, StudentT, p_values, q_values\n",
    ")\n",
    "\n",
    "# Combine results\n",
    "all_garch_results = pd.concat([garch_normal_results, garch_t_results])\n",
    "all_garch_results = all_garch_results.sort_values('AIC')\n",
    "\n",
    "# Display results\n",
    "print(\"GARCH Model Comparison:\")\n",
    "all_garch_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Estimate EGARCH models with different specifications\n",
    "# EGARCH models can capture asymmetric effects\n",
    "\n",
    "# Estimate EGARCH models with normal distribution\n",
    "egarch_normal_results = estimate_model_grid(\n",
    "    returns_array, EGARCH, Normal, p_values, q_values\n",
    ")\n",
    "\n",
    "# Estimate EGARCH models with Student's t distribution\n",
    "egarch_t_results = estimate_model_grid(\n",
    "    returns_array, EGARCH, StudentT, p_values, q_values\n",
    ")\n",
    "\n",
    "# Combine results\n",
    "all_egarch_results = pd.concat([egarch_normal_results, egarch_t_results])\n",
    "all_egarch_results = all_egarch_results.sort_values('AIC')\n",
    "\n",
    "# Display results\n",
    "print(\"EGARCH Model Comparison:\")\n",
    "all_egarch_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combine all results and find the best model\n",
    "all_models = pd.concat([all_garch_results, all_egarch_results])\n",
    "all_models = all_models.sort_values('AIC')\n",
    "\n",
    "# Display top 5 models\n",
    "print(\"Top 5 Models by AIC:\")\n",
    "all_models.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot AIC and BIC for different models\n",
    "plt.figure(figsize=(14, 8))\n",
    "\n",
    "# Create model labels\n",
    "model_labels = all_models['Model'].values\n",
    "\n",
    "# Plot AIC\n",
    "plt.subplot(2, 1, 1)\n",
    "plt.bar(range(len(all_models)), all_models['AIC'], color='blue', alpha=0.7)\n",
    "plt.title('AIC for Different Model Specifications')\n",
    "plt.ylabel('AIC')\n",
    "plt.xticks(range(len(all_models)), model_labels, rotation=90)\n",
    "plt.grid(True, axis='y')\n",
    "\n",
    "# Plot BIC\n",
    "plt.subplot(2, 1, 2)\n",
    "plt.bar(range(len(all_models)), all_models['BIC'], color='green', alpha=0.7)\n",
    "plt.title('BIC for Different Model Specifications')\n",
    "plt.ylabel('BIC')\n",
    "plt.xticks(range(len(all_models)), model_labels, rotation=90)\n",
    "plt.grid(True, axis='y')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Asynchronous Processing for Long-Running Estimations\n",
    "\n",
    "For computationally intensive tasks like estimating multiple models or running large simulations, we can use asynchronous processing to improve efficiency."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define an asynchronous function to estimate a model\n",
    "async def estimate_model_async(model_class, model_name: str, returns: np.ndarray, \n",
    "                              p: int = 1, q: int = 1, distribution=None) -> Tuple[str, Any, pd.DataFrame]:\n",
    "    \"\"\"\n",
    "    Asynchronously estimate a volatility model and return results.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    model_class : class\n",
    "        Volatility model class (GARCH, EGARCH, etc.)\n",
    "    model_name : str\n",
    "        Name of the model for display purposes\n",
    "    returns : np.ndarray\n",
    "        Array of returns\n",
    "    p : int, optional\n",
    "        ARCH order, default is 1\n",
    "    q : int, optional\n",
    "        GARCH order, default is 1\n",
    "    distribution : Distribution, optional\n",
    "        Error distribution, default is Normal()\n",
    "        \n",
    "    Returns\n",
    "    -------\n",
    "    Tuple[str, Any, pd.DataFrame]\n",
    "        Tuple containing model name, model results, and summary DataFrame\n",
    "    \"\"\"\n",
    "    print(f\"Starting estimation of {model_name}...\")\n",
    "    \n",
    "    # Set default distribution if not provided\n",
    "    if distribution is None:\n",
    "        distribution = Normal()\n",
    "    \n",
    "    try:\n",
    "        # Create and estimate model\n",
    "        model = model_class(p=p, q=q, distribution=distribution)\n",
    "        \n",
    "        # Use fit_async if available, otherwise use regular fit\n",
    "        if hasattr(model, 'fit_async'):\n",
    "            results = await model.fit_async(returns)\n",
    "        else:\n",
    "            # Run in executor to avoid blocking\n",
    "            loop = asyncio.get_event_loop()\n",
    "            results = await loop.run_in_executor(None, lambda: model.fit(returns))\n",
    "        \n",
    "        # Create summary DataFrame\n",
    "        summary = pd.DataFrame({\n",
    "            'Parameter': results.parameter_names,\n",
    "            'Estimate': results.parameters,\n",
    "            'Std. Error': results.std_errors,\n",
    "            't-statistic': results.t_stats,\n",
    "            'p-value': results.p_values\n",
    "        })\n",
    "        \n",
    "        print(f\"Completed estimation of {model_name}\")\n",
    "        return model_name, results, summary\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(f\"Error estimating {model_name}: {e}\")\n",
    "        return model_name, None, None\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define an asynchronous function to estimate multiple models concurrently\n",
    "async def estimate_multiple_models_async(returns: np.ndarray) -> Dict[str, Any]:\n",
    "    \"\"\"\n",
    "    Asynchronously estimate multiple volatility models.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    returns : np.ndarray\n",
    "        Array of returns\n",
    "        \n",
    "    Returns\n",
    "    -------\n",
    "    Dict[str, Any]\n",
    "        Dictionary mapping model names to results\n",
    "    \"\"\"\n",
    "    # Define models to estimate\n",
    "    models_to_estimate = [\n",
    "        (GARCH, \"GARCH(1,1) Normal\", 1, 1, Normal()),\n",
    "        (GARCH, \"GARCH(1,1) Student's t\", 1, 1, StudentT()),\n",
    "        (GARCH, \"GARCH(2,1) Normal\", 2, 1, Normal()),\n",
    "        (GARCH, \"GARCH(1,2) Normal\", 1, 2, Normal()),\n",
    "        (EGARCH, \"EGARCH(1,1) Normal\", 1, 1, Normal()),\n",
    "        (EGARCH, \"EGARCH(1,1) Student's t\", 1, 1, StudentT()),\n",
    "        (TARCH, \"TARCH(1,1) Normal\", 1, 1, Normal()),\n",
    "        (TARCH, \"TARCH(1,1) Student's t\", 1, 1, StudentT())\n",
    "    ]\n",
    "    \n",
    "    # Create tasks for each model\n",
    "    tasks = [\n",
    "        estimate_model_async(model_class, model_name, returns, p, q, \n",
    "                           distribution)\n",
    "        for model_class, model_name, p, q, distribution in models_to_estimate\n",
    "    ]\n",
    "    \n",
    "    # Run tasks concurrently\n",
    "    results = await asyncio.gather(*tasks)\n",
    "    \n",
    "    # Process results\n",
    "    model_results = {}\n",
    "    for model_name, result, summary in results:\n",
    "        if result is not None:\n",
    "            model_results[model_name] = {\n",
    "                'result': result,\n",
    "                'summary': summary\n",
    "            }\n",
    "    \n",
    "    return model_results\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run asynchronous estimation\n",
    "async def main():\n",
    "    print(\"Starting asynchronous model estimation...\")\n",
    "    start_time = datetime.datetime.now()\n",
    "    \n",
    "    # Estimate models\n",
    "    model_results = await estimate_multiple_models_async(returns_array)\n",
    "    \n",
    "    # Calculate elapsed time\n",
    "    end_time = datetime.datetime.now()\n",
    "    elapsed_time = (end_time - start_time).total_seconds()\n",
    "    \n",
    "    print(f\"\nEstimated {len(model_results)} models in {elapsed_time:.2f} seconds\")\n",
    "    \n",
    "    # Create comparison DataFrame\n",
    "    comparison = []\n",
    "    for model_name, data in model_results.items():\n",
    "        result = data['result']\n",
    "        comparison.append({\n",
    "            'Model': model_name,\n",
    "            'Log-Likelihood': result.log_likelihood,\n",
    "            'AIC': result.aic,\n",
    "            'BIC': result.bic,\n",
    "            'Parameters': len(result.parameters)\n",
    "        })\n",
    "    \n",
    "    comparison_df = pd.DataFrame(comparison)\n",
    "    comparison_df = comparison_df.sort_values('AIC')\n",
    "    \n",
    "    print(\"\nModel Comparison:\")\n",
    "    print(comparison_df)\n",
    "    \n",
    "    # Return results for further analysis\n",
    "    return model_results, comparison_df\n",
    "\n",
    "# Run the async function\n",
    "async_results, async_comparison = await main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot comparison of conditional volatility from different models\n",
    "plt.figure(figsize=(14, 8))\n",
    "\n",
    "# Get the best model based on AIC\n",
    "best_model_name = async_comparison.iloc[0]['Model']\n",
    "best_model_data = async_results[best_model_name]\n",
    "best_model_result = best_model_data['result']\n",
    "\n",
    "# Plot volatility from different models\n",
    "for model_name, data in async_results.items():\n",
    "    result = data['result']\n",
    "    volatility = np.sqrt(result.conditional_variance)\n",
    "    \n",
    "    # Use thicker line for the best model\n",
    "    if model_name == best_model_name:\n",
    "        plt.plot(returns_df.index, volatility, linewidth=2.5, label=f\"{model_name} (Best)\")\n",
    "    else:\n",
    "        plt.plot(returns_df.index, volatility, linewidth=1, alpha=0.7, label=model_name)\n",
    "\n",
    "plt.title('Conditional Volatility Comparison')\n",
    "plt.ylabel('Volatility (%)')\n",
    "plt.xlabel('Date')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Conclusion\n",
    "\n",
    "In this notebook, we've demonstrated the use of univariate volatility models in the MFE Toolbox. We've covered:\n",
    "\n",
    "1. Data preparation and exploration\n",
    "2. GARCH model estimation and diagnostics\n",
    "3. Alternative models (EGARCH, TARCH) and distributions (Normal, Student's t, GED, Skewed t)\n",
    "4. Volatility forecasting\n",
    "5. Simulation techniques\n",
    "6. Model comparison and selection\n",
    "7. Asynchronous processing for efficient estimation\n",
    "\n",
    "The MFE Toolbox provides a comprehensive set of tools for volatility modeling, implemented using modern Python practices with NumPy, Pandas, and Numba acceleration. The class-based design with type hints and asynchronous processing capabilities makes it both powerful and user-friendly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display version information\n",
    "print(f\"MFE Toolbox version: {mfe.__version__}\")\n",
    "print(f\"NumPy version: {np.__version__}\")\n",
    "print(f\"Pandas version: {pd.__version__}\")\n",
    "print(f\"Matplotlib version: {plt.matplotlib.__version__}\")\n",
    "print(f\"Seaborn version: {sns.__version__}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
